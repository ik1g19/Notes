{"path":"Git Ignore/Heavy Stuff/Lecture Slides/Intelligent-Systems-2020-compressed.pdf","text":"COMP 2208 : Intelligent Systems Richard Watson 3 parts n An introduction to classical artificial intelligence, including topics such as the history and philosophy of the discipline, search and planning (Richard Watson). n An introduction to recent advances in artificial intelligence, e.g., neural networks, Bayesian methods, decision making, etc. (Srinandan Dasmahapatra). n Mathematical techniques, e.g., functions, vectors and matrices, multi-variate calculus, etc. (Danesh Tarapore). Structure n 15 credits (~150 hours of your time) n 3x ‘Lectures’ per week n Pre-recorded lectures n Plus 30 mins each per week in-person (within these Part II mega sessions) n Plus 60 min interactive online. n in-person and online shared among the three lecturers n Assessment n 20% maths worksheets n 80% exam n Readings n Keep an eye on web page and emails! Resources n Artificial Intelligence: A modern approach n Russell and Norvig n Web n Lots of good resources, but be a bit careful n Library n Books, Journals, Conference Proceedings Artificial Intelligence n Previous experience? structure n See notes page Web pages n https://secure.ecs.soton.ac.uk/notes/co mp2208/ n https://secure.ecs.soton.ac.uk/module/ 2021/COMP2208/31030/ - aims and objectives n Lectures n Additional Readings Throughout the course n Think about the topics – bigger picture n Not just learning techniques n Need to understand their application n AI is a good topic to ‘read around’ n Try to be ‘scientific’ in your approach Parts 2 and 3 n An introduction to recent advances in artificial intelligence, e.g., neural networks, Bayesian methods, decision making, etc. (Srinandan Dasmahapatra). n Mathematical techniques, e.g., functions, vectors and matrices, multi- variate calculus, etc. (Danesh Tarapore). The search/classical AI part n what is AI n search - blind n search - heuristic n history and philosophy n search - local n constraint satisfaction problems n adversarial games: minimax n alpha-beta pruning n planning… Readings for discussion 1. Picard: Data 2. Turing: Turing test 3. Searle: Chinese room 4. Brooks: Elephants… n Be ready to answer questions in class lecture of week 5 Your first ‘reading’ n STTNG \"The measure of a man\" (especially minutes 25 onwards) n a relevant bit. n Consider: what evidence would Riker need to prove his case? AI 1967, Marvin Minsky: \"Within a generation ... the problem of creating 'artificial intelligence' will substantially be solved So – how far away are we? Questions? Reading: Chapter 3 Everyone in this room can easily code an optimal chess-playing program } Who agrees? } Lots of problems involve search ◦ Game trees, path finding, etc. } Usually a trade-off between ◦ Speed ◦ Memory ◦ ‘Optimality’ } On holiday in Romania; currently in Arad. } Flight leaves tomorrow from Bucharest } Formulate goal: ◦ be in Bucharest } Formulate problem: ◦ states: various cities ◦ actions: drive between cities } Find solution: ◦ sequence of cities, e.g., Arad, Sibiu, Fagaras, Bucharest } Deterministic, fully observable à single-state problem ◦ Agent knows exactly which state it will be in; solution is a sequence } Non-observable à sensorless problem ◦ Agent may have no idea where it is; solution is a sequence } Nondeterministic and/or partially observable à contingency problem ◦ percepts provide new information about current state ◦ often interleave} search, execution } Unknown state space à exploration problem Single-state } start in #5. } Solution? [Right, Suck] Sensorless } start in {1,2,3,4,5,6,7,8} e.g., Right goes to {2,4,6,8} } Solution? [Right,Suck,Left,Suck] Contingency } Nondeterministic: Suck may dirty a clean carpet } Partially observable: location, dirt at current location. } Percept: [L, Clean], i.e., start in #5 or #7 Solution? [Right, if dirt then Suck] A problem is defined by four items: 1. initial state e.g., \"at Arad\" 2. actions or successor function S(x) = set of action–state pairs ◦ e.g., S(Arad) = {<Arad à Zerind, Zerind>, … } 3. goal test, can be ◦ explicit, e.g., x = \"at Bucharest\" ◦ implicit, e.g., Checkmate(x) 4. path cost (additive) ◦ e.g., sum of distances, number of actions executed, etc. ◦ c(x,a,y) is the step cost, assumed to be ≥ 0 } A solution is a sequence of actions leading from the initial state to a goal state } Real world is absurdly complex à state space must be abstracted for problem solving } (Abstract) state = set of real states } (Abstract) action = complex combination of real actions ◦ e.g., \"Arad à Zerind\" represents a complex set of possible routes, detours, rest stops, etc. } (Abstract) solution = ◦ set of real paths that are solutions in the real world } Each abstract action should be \"easier\" than the original problem } states? } actions? } goal test? } path cost? } states? integer dirt and robot location } actions? Left, Right, Suck } goal test? no dirt at all locations } path cost? 1 per action } states? } actions? } goal test? } path cost? } states? configuration of tiles } actions? move blank left, right, up, down } goal test? = goal state (given) } path cost? 1 per move [Note: optimal solution of n-Puzzle family is NP- hard] } Basic idea: ◦ offline, simulated exploration of state space by generating successors of already-explored states (a.k.a.~expanding states) } A state is a (representation of) a physical configuration } A node is a data structure constituting part of a search tree includes state, parent node, action, path cost g(x), depth } States and nodes are not the same thing! } The Expand function creates new nodes, filling in the various fields and using the SuccessorFn of the problem to create the corresponding states. } A search strategy is defined by picking the order of node expansion } Strategies are evaluated along the following dimensions: ◦ completeness: does it always find a solution if one exists? ◦ time complexity: number of nodes generated ◦ space complexity: maximum number of nodes in memory ◦ optimality: does it always find a least-cost solution? } Time and space complexity are measured in terms of ◦ b: maximum branching factor of the search tree ◦ d: depth of the least-cost solution ◦ m: maximum depth of the state space (may be ∞) } Uninformed search strategies ◦ use only the information available in the problem definition } Breadth-first search } Depth-first search } Depth-limited search } Iterative deepening search } Expand shallowest unexpanded node } Implementation: ◦ fringe is a FIFO queue, i.e., new successors go at end } Expand shallowest unexpanded node } Implementation: ◦ fringe is a FIFO queue, i.e., new successors go at end } Expand shallowest unexpanded node } Implementation: ◦ fringe is a FIFO queue, i.e., new successors go at end } Expand shallowest unexpanded node } Implementation: ◦ fringe is a FIFO queue, i.e., new successors go at end Complete? Time? Space? Optimal? Complete? Yes (if b is finite) Time? 1+b+b2+b3+… +bd + b(bd-1) = O(bd+1) Space? O(bd+1) (keeps every node in memory) Optimal? Yes (if cost = 1 per step) Space is the bigger problem (more than time) } Expand deepest unexpanded node } Implementation: ◦ fringe = LIFO queue, i.e., put successors at front } Expand deepest unexpanded node } Implementation: ◦ fringe = LIFO queue, i.e., put successors at front } Expand deepest unexpanded node } Implementation: ◦ fringe = LIFO queue, i.e., put successors at front } Expand deepest unexpanded node } Implementation: ◦ fringe = LIFO queue, i.e., put successors at front } Expand deepest unexpanded node } Implementation: ◦ fringe = LIFO queue, i.e., put successors at front } Expand deepest unexpanded node } Implementation: ◦ fringe = LIFO queue, i.e., put successors at front } Expand deepest unexpanded node } Implementation: ◦ fringe = LIFO queue, i.e., put successors at front } Expand deepest unexpanded node } Implementation: ◦ fringe = LIFO queue, i.e., put successors at front } Expand deepest unexpanded node } Implementation: ◦ fringe = LIFO queue, i.e., put successors at front } Expand deepest unexpanded node } Implementation: ◦ fringe = LIFO queue, i.e., put successors at front } Expand deepest unexpanded node } Implementation: ◦ fringe = LIFO queue, i.e., put successors at front } Expand deepest unexpanded node } Implementation: ◦ fringe = LIFO queue, i.e., put successors at front Complete? No: fails in infinite-depth spaces, spaces with loops ◦ Modify to avoid repeated states along path àcomplete in finite space Time? O(bm): terrible if m is much larger than d ◦ but if solutions are dense, may be much faster than breadth-first } Space? O(bm), i.e., linear space! } Optimal? No = depth-first search with depth limit n, i.e., nodes at depth n have no successors } Number of nodes generated in a depth- limited search to depth d with branching factor b: NDLS = b0 + b1 + b2 + … + bd-2 + bd-1 + bd } Number of nodes generated in an iterative deepening search to depth d with branching factor b: NIDS = (d+1)b0 + d b1 + (d-1)b2 + … + 3bd-2 +2bd-1 + 1bd } For b = 10, d = 5, } NBFS = 1+ 10 + 100 + 1,000 + 10,000 + 100,000 = 111,111 } NIDS = 6 + 50 + 400 + 3,000 + 20,000 + 100,000 = 123,456 } IDS is not much slower } And only needs to store O(bd) nodes at any one time Complete? Yes Time? (d+1)b0 +db1 + (d-1)b2 +…+ bd = O(bd) Space? O(bd) Optimal? Yes, if step cost = 1 } Failure to detect repeated states can turn a linear problem into an exponential one! } Do two searches ◦ One starts from the initial state ◦ One starts from the goal state } Motivation: ◦ bd/2 + bd/2 is much less than bd } At each iteration of each search: ◦ Check if a node is in the fringe/open of the other before expansion ◦ If yes, a solution has been found } Assumes Pred(x) is easy to compute ◦ Easy if Pred(x) = Succ(x) } If many goal states: ◦ Add a dummy node which has the true goal states as predecessors } Easy: 8-puzzle, Romania } Hard: Checkmate in chess } Problem formulation usually requires abstracting away real-world details to define a state space that can feasibly be explored } Variety of uninformed search strategies } Iterative deepening search uses only linear space and not much more time than other uninformed algorithms It pays to guess… Reading: Chapter 4 } We have seen many examples of tree-search ◦ BFS, DFS, IDS } Key idea is order of node expansion } In this lecture: ◦ Greedy best-first search, A* ◦ Best first is not really the best! } Idea: use an evaluation function f(n) for each node ◦ estimate of \"desirability\" àExpand most desirable unexpanded node } Implementation: Order the nodes in fringe in decreasing order of desirability } Special cases: ◦ greedy best-first search ◦ A* search } Evaluation function f(n) = h(n) (heuristic) } = estimate of cost from n to goal } e.g., hSLD(n) = straight-line distance from n to Bucharest } Greedy best-first search expands the node that appears to be closest to goal } Complete? No – can get stuck in loops like depth first if heuristic is bad } Time? O(bm), but a good heuristic can give dramatic improvement } Space? O(bm) -- keeps all nodes in memory } Optimal? No } Idea: avoid expanding paths that are already expensive } Evaluation function f(n) = g(n) + h(n) } g(n) = cost so far to reach n } h(n) = estimated cost from n to goal } f(n) = estimated total cost of path through n to goal } A heuristic h(n) is admissible if for every node n, h(n) ≤ h*(n), where h*(n) is the true cost to reach the goal state from n. } An admissible heuristic never overestimates the cost to reach the goal, i.e., it is optimistic } Example: hSLD(n) (never overestimates the actual road distance) } Theorem: If h(n) is admissible, A* using TREE-SEARCH is optimal } Suppose some suboptimal goal state G2 has been generated and is in the fringe. Let n be an unexpanded node in the fringe such that n is on a shortest path to an optimal goal G with true cost C*. f(G2) = g(G2)+h(G2) (from h(x)≤g(x) and G2 is a goal state) f(G2)=g(G2) (from suboptimality of G2) f(G2)>C* f(n) = g(n)+h(n) (from h(x)≤g(x) and g(G) =C*) f(n) ≤ C* \\ f(n) ≤ f(G2), and A* will never select G2 for expansion } That proof doesn’t hold for GRAPH-SEARCH } A* might discard the optimal path to a repeated state if it is not the first one generated } If the cost for any action was a constant, this wouldn’t be a problem } OR, if we can guarantee that the first path found to a repeated node was the best path to that node } We can guarantee this if we impose an extra requirement on the heuristic } A heuristic is consistent (or monotonic) if for every node n, every successor n' of n generated by any action a h(n) ≤ c(n,a,n') + h(n') } If h is consistent, we have f(n') = g(n') + h(n') = g(n) + c(n,a,n') + h(n') ≥ g(n) + h(n) = f(n) } i.e., f(n) is non-decreasing along any path. } Theorem: If h(n) is consistent, A* using GRAPH-SEARCH is optimal } Given f(n) is non-decreasing along any path } The sequence of nodes expanded by A* is in nondecreasing order of f(n) } Hence the first goal to be selected for expansion must be an optimal solution (since all the latter nodes are at least as expensive). } A* expands nodes in order of increasing f value } Gradually adds \"f-contours\" of nodes } Contour i has all nodes with f=fi, where fi < fi+1 } If C* is the cost of the optimal path } A* expands all nodes s.t. f(n)<C* } A* will possibly expand nodes on the right of the ‘goal contour’ (f(n)=C*), before finding the goal node } A* never expands nodes with f(n)>C* } Does efficient pruning ◦ Bits of the tree for which f(n) > f(G) are not expanded } Is optimally efficient ◦ No other algorithm is guaranteed to expand fewer nodes than A* (unless there many tie-breaking nodes) } Complete? Yes (unless there are infinitely many nodes with f ≤ f(G) ) } Time? Exponential (in length of optimal solution) } Space? Keeps all (expanded) nodes in memory } Optimal? Yes E.g., for the 8-puzzle: } h1(n) = number of misplaced tiles } h2(n) = total Manhattan distance (i.e., no. of squares from desired location of each tile) } h1(S) = ? } h2(S) = ? E.g., for the 8-puzzle: } h1(n) = number of misplaced tiles } h2(n) = total Manhattan distance (i.e., no. of squares from desired location of each tile) } h1(S) = ? 8 } h2(S) = ? 3+1+2+2+2+3+3+2 = 18 } If h2(n) ≥ h1(n) for all n (both admissible) } then h2 dominates h1 } h2 is better for search } Typical search costs (average number of nodes expanded = effective branching factor): } d=12 IDS = 3,644,035 nodes A*(h1) = 227 nodes A*(h2) = 73 nodes } d=24 IDS = too many nodes A*(h1) = 39,135 nodes A*(h2) = 1,641 nodes } Often impractical to find optimal solution ◦ Variants of A* that find sub-optimal solutions quickly ◦ ‘More accurate’ heuristics that are not strictly admissable } How do you create a heuristic? } A problem with fewer restrictions on the actions is called a relaxed problem } The cost of an optimal solution to a relaxed problem is an admissible heuristic for the original problem } If the rules of the 8-puzzle are relaxed so that a tile can move anywhere, then h1(n) gives the shortest solution } If the rules are relaxed so that a tile can move to any adjacent square, then h2(n) gives the shortest solution } Good heuristics help! ◦ Bad heuristics also help (but not much) } A* really is optimal in many ways (but optimality is often impractical) } Always remember: ◦ (naive) Search space is usually much, much, much, much, bigger than it first looks History of Artificial Intelligence 2 Outline • Pre-History • Early-AI • Milestones and Movements • Today? John McCarthy (1927- 2011) 3 Pre-History: Tributaries of AI • Philosophy Logic, methods of reasoning, phil. of mind, learning, language, rationality • Mathematics Formal notation & proof, algorithms, computation, undecidability, intractability • Neuroscience Physical substrate for mental activity • Psychology Perception, motor control, experimentation • Engineering Building (fast) computing devices • Control Theory Feedback, homeostasis, stability, optimality • Linguistics Knowledge representation, grammar • Economics Utility, decision theory 4 Pre-History: Examples Automata, e.g, Vaucanson’s Duck Babbage’s Difference Engine: • Ada Lovelace speculated on its potential for AI 19th century economists: • Jevons built an automated logic engine – the “logic piano” • Marshall described an algorithm capable of evolving new logics 5 Turing and Bletchley Park • During WWII, Alan Turing worked on code-breaking at Bletchley Park. • Used heuristic search to translate Nazi messages in real time • With others, e.g., Jack Good and Don Michie, he speculated on machine intelligence, learning… • Much of this remained secret until after the war. • The military has retained a strong interest in AI ever since… 6 After WWII • 1943: McCulloch & Pitts model of artificial boolean neurons. • First steps toward connectionist computation and learning • 1951: Marvin Minsky builds the first neural network computer • 1950: Turing’s “Computing Machinery and Intelligence” is published. – First complete vision of AI. Marvin Minsky (1927- ) Warren McCulloch (1899-1969 7 The Birth of AI (1956) The Dartmouth Workshop brings together 10 top minds on automata theory, neural nets and the study of intelligence. • Conjecture: “every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it” • Ray Solomonoff, Oliver Selfridge, Trenchard More, Arthur Samuel, John McCarthy, Marvin Minsky, etc. • Allen Newell and Herb Simon’s Logic Theorist • For the next 20 years the field was dominated by these participants. 8 Great Expectations (1952-1969) • Newell & Simon imitated human problem-solving – General Problem Solver. • Arthur Samuel (1952-) – Had success with checkers. • John McCarthy (1958-) invented Lisp (2nd high-level lang.) – Advice Taker • Marvin Minsky introduced “microworlds” – “Society of Mind” 9 AI Winter • Collapse in AI research (1966 - 1973) – Progress was slower than expected. • Unrealistic predictions. – Some systems lacked scalability. • Combinatorial explosion in search. – Fundamental limitations on techniques and representations. • Minsky and Papert (1969) Perceptrons. Seymour Papert (1928- ) 10 AI Revival (1969-1970s) • Exploiting encoded domain knowledge – DENDRAL (Buchanan et al. 1969) • First successful knowledge-intensive system (organic chemistry/mass spec data). – MYCIN diagnosed blood infections (Feigenbaum et al.) • Introduction of uncertainty in reasoning. • Increase in knowledge representation research. – Logic, frames, scripts, semantic nets, etc., … 11 Cognitive AI (1980- ) • Marr’s (1980) posthumous Vision advances vision, neuroscience and cog. sci. after he dies young • AI engages with cognitive philosophy: – Dennett’s (1981) Brainstorms and (1985) Mind’s I (with Hofstadter) – Fodor’s (1983) Modularity of Mind – Chuchland’s (1984) Matter & Consciousness – Clark’s (1989) Microcognition – Port & van Gelder’s (1995) It’s About Time Jerry Fodor (1935- ) Andy Clark 12 (the first) Connectionist Revival (1986- ) • Parallel distributed processing (Rumelhart & McClelland ‘86) • Multi-level perceptrons and backpropagation learning • Language, reasoning, perception, control + a little mystery • Robust behaviour, graceful degradation • No representations? Sub-symbolic AI… • 90s: Elman pioneers layered recurrent nets • 90s: Fully recurrent networks and robot control (e.g., Beer) • Ultimately… “neural” networks as data-mining, statistics… 13 Nouvelle AI (1988- ) • Rodney Brooks and other roboticists challenge the formalist, “representational” orthodoxy – Elephant don’t play chess, Brooks – Why not the whole iguana?, Dennett – Nevermind the blocksworld, Cliff • Situated, Embedded, Embodied cognition • Inspired by simple insects, rather than chess and logic • Anti-representationalist, anti-reasoning, anti-generality • Evolutionary robotics, artificial life, “the new cybernetics” 14 What is Intelligence? 15 Intelligent Agents (1995- ) • Combined whole organism perspective with a rational utility-maximising framework borrowed from economics. • A response to nouvelle AI? • An empty label? • A hybrid? A bolt-hole for formalists? A revolution? – “How does an agent act/behave embedded in real environments with continuous sensory inputs” 16 Data, Data, Everwhere (2000- ) • Massive amounts of raw power and raw data fuel advances in machine learning: – Eigenfaces – Corpus linguistics – Kernel methods – Computational learning theories • Offline vs. Online AI? • IBM Watson ‘deep AI’ – the second connectionist revival • Deep machine learning – Google deep mind: deep ML + lots of data • Alpha GO, monte carlo tree search • Deep learning – Multi-layer perceptrons – Deep belief networks 1719 Background Reading… AIAMA2e Chapter 1 + cited references Brooks, R. A. (1991). Intelligence without reason. MIT AI Memo #1293.* Dupuy, J.-P. (2000). The mechanization of mind. Princeton University Press. Husbands, P., Holland, O., & Wheeler, M. (eds.) (2008). The mechanical mind in history. MIT Press. Luger, G. F.(ed). (1995). Computation and intelligence: Collected readings. AAAI/MIT Press. 20 Online Resources… • Wikipedia Entries – History of AI – Nouvelle AI – Machine Learning – AI Winter Solving optimisation problems } We have been looking at searches for optimal path } What if only the goal state mattered? ◦ Path is irrelevant ◦ Goal state itself is the solution } State space = set of \"complete\" configurations } Find configuration satisfying constraints, e.g., n-queens } In such cases, we can use local search algorithms } keep a single \"current\" state, try to improve it } Put n queens on an n × n board with no two queens on the same row, column, or diagonal } \"Like climbing Everest in thick fog with amnesia\" } Problem: depending on initial state, can get stuck in local maxima} h = number of pairs of queens that are attacking each other, either directly or indirectly } h = 17 for the above state } A local minimum with h = 1 } What would the numbered board look like? } For a random initialisation, hill climbing gets stuck 86% of the time } Idea: escape local maxima by allowing some \"bad\" moves but gradually decrease their likelihood } One can prove: ◦ If T decreases slowly enough, then simulated annealing search will find a global optimum with probability approaching 1 ◦ In asymptotic limit (‘asymtopia’): time required is exponential in the problem size for all non-zero temperatures } Widely used in VLSI layout, airline scheduling, training neural networks, everywhere! } Keep track of k states rather than just one } Start with k randomly generated states } At each iteration, all the successors of all k states are generated } If any one is a goal state, stop; else select the k best successors from the complete list and repeat. } Exploits parallelism ◦ If one node produces many good successors and one produces many bad ones, the search ‘jumps’ } Is there a disadvantage with this? ◦ K-beam can become narrowly focussed in a small region of the search space. } A successor state is generated by combining two parent states } Start with k randomly generated states (population) } A state is represented as a string over a finite alphabet (often a string of 0s and 1s) } Evaluation function (fitness function). Higher values for better states. } Produce the next generation of states by selection, crossover, and mutation } Fitness function: number of non-attacking pairs of queens (min = 0, max = 8 × 7/2 = 28) } 24/(24+23+20+11) = 31% } 23/(24+23+20+11) = 29% etc } Up until now: ◦ Agent knows all about the space, can reason about it before doing anything } Online setting: ◦ Agent only knows path cost after it has committed to an action ◦ Therefore no access to successors of a state except by actually trying all possible actions } Actions are deterministic } Agent can recognise the state it’s in } What about irreversible actions? ◦ Dead ends, etc. ◦ Assume state space is safely explorable } Choose actions at random ◦ Give preference to those that haven’t been tried ◦ Will eventually find a goal state (safely explorable) – or complete its exploration ◦ State space needs to be finite ◦ Can be exponentially slow! } Local search => only goal is relevant } Hill climbing } Simulated Annealing } Local Beam Search } Genetic Algorithms } Online search } Reading: ◦ Chapter 4 : Second half Reading: Chapter 5 } Standard search problem: } ◦ state is a \"black box“ – any data structure that supports successor function, heuristic function, and goal test ◦ } CSP: } ◦ state is defined by variables Xi with values from domain Di ◦ ◦ goal test is a set of constraints specifying allowable combinations of values for subsets of variables ◦ E.g. graph colouring, N-queens } Simple example of a formal representation language } Allows useful general-purpose algorithms with more power than standard search algorithms } 2 } Variables WA, NT, Q, NSW, V, SA, T } Domains Di = {red,green,blue} } Constraints: adjacent regions must have different colors } } e.g., WA ≠ NT, or (WA,NT) in {(red,green),(red,blue),(green,red), (green,blue),(blue,red),(blue,green)} } 3 } Solutions are complete and consistent assignments, e.g., WA = red, NT = green,Q = red,NSW = green,V = red,SA = blue,T = green } 4 } Binary CSP: each constraint relates two variables } } Constraint graph: nodes are variables, arcs are constraints } Colouring has many uses: including bandwidth assignment in mobile-phone masts 5 } Discrete variables } ◦ finite domains:  n variables, domain size d à O(dn) complete assignments  e.g., Boolean CSPs, incl.~Boolean satisfiability (NP-complete) ◦ infinite domains:  integers, strings, etc.  e.g., job scheduling, variables are start/end days for each job  need a constraint language, e.g., StartJob1 + 5 ≤ StartJob3 } Continuous variables } ◦ e.g., start/end times for Hubble Space Telescope observations ◦ linear constraints solvable in polynomial time by linear programming 6 } Unary constraints involve a single variable, ◦ e.g., SA ≠ green ◦ } Binary constraints involve pairs of variables, ◦ e.g., SA ≠ WA ◦ } Higher-order constraints involve 3 or more variables, ◦ e.g., cryptarithmetic column constraints ◦ 7 } Assignment problems ◦ e.g., who teaches what class } Timetabling problems } e.g., which class is offered when and where? } Transportation scheduling } Factory scheduling } Note that many real-world problems involve real- valued variables 8 Let's start with the straightforward approach, then fix it States are defined by the values assigned so far } Initial state: the empty assignment { } } Successor function: assign a value to an unassigned variable that does not conflict with current assignment à fail if no legal assignments } Goal test: the current assignment is complete 1. This is the same for all CSPs 2. Every solution appears at depth n with n variables à use depth-first search 3. Path is irrelevant, so can also use complete-state formulation 4. b = (n - l )d at depth l, hence #leaves= n!dn, even though there are only dn assignments 9 } Variable assignments are commutative}, i.e., [ WA = red then NT = green ] same as [ NT = green then WA = red ] } Only need to consider assignments to a single variable at each node àb = d and there are dn leaves } Depth-first search for CSPs with single-variable assignments is called backtracking search } } Backtracking search is the basic uninformed algorithm for CSPs } } Can solve n-queens for n ≈ 25 1011 } Conceptually VERY similar } But in DFS we assumed that a node was expanded by applying all possible actions to add nodes to the fringe (then later, if nec, these nodes will be on the fringe ready to be expanded further) } Backtracking search just applies one action to produce one successor node, and continues on down (then later, if nec, on returning to this node it will check whether there are any other successors that can be generated) COMP6031 121314 ….? 151617 Is there a good reason to pick NT next rather than, say, NSW? 18 } General-purpose methods can give huge gains in speed: } ◦ Which variable should be assigned next? ◦ ◦ In what order should its values be tried? ◦ ◦ Can we detect inevitable failure early? ◦ 19 } Most constrained variable: choose the variable with the fewest legal values } a.k.a. minimum remaining values (MRV) heuristic } Because these are the variables that are most likely to prune the search tree (e.g. consider a variable with no legal values remaining) 20 } Most constrained is more important heuristic, but most constraining is a useful tie-breaker among most constrained variables } Most constraining variable = the variable with the most constraints on remaining variables ◦ Because the variable involved in the most constraints -> is again, most likely to cause a failure early/prune the search tree ◦ 21 } Given a variable, choose the least constraining value: ◦ the one that rules out the fewest values in the neighbouring variables – to leave other variables as open as possible ◦ ◦ } Combining these heuristics makes 1000 queens feasible } } 22 } Idea: ◦ Keep track of remaining legal values for unassigned variables ◦ Terminate search when any variable has no legal values ◦ 23 } Idea: ◦ Keep track of remaining legal values for unassigned variables ◦ Terminate search when any variable has no legal values ◦ 24 } Idea: ◦ Keep track of remaining legal values for unassigned variables ◦ Terminate search when any variable has no legal values ◦ 25 } Idea: ◦ Keep track of remaining legal values for unassigned variables ◦ Terminate search when any variable has no legal values ◦ 26 Detects failure because no options remain for SA } Simplest form of propagation makes each arc consistent } X àY is consistent iff for every value x of X there is some allowed y } If X loses a value, neighbors of X need to be rechecked } 30 } Hill-climbing, simulated annealing typically work with \"complete\" states, i.e., all variables assigned } } To apply to CSPs: ◦ allow states with constraint violations ◦ operators reassign variable values ◦ } Variable selection: randomly select any conflicted variable } } Value selection by min-conflicts heuristic: ◦ choose value that violates the fewest constraints ◦ i.e., hill-climb with h(n) = total number of violated constraints 33 } States: 4 queens in 4 columns (44 = 256 states) } } Actions: move queen in column } } Goal test: no attacks } } Evaluation: h(n) = number of attacks } 34 } While we are not at a solution: ◦ Choose a random conflicted variable ◦ Pick a value for this that has minimum conflicts } Works surprisingly well (on N-queens)! ◦ For n-queens, almost independent of problem size - due to dense solution space ◦ Also works on scheduling problems  Note: No need to restart and re-run backtracking… COMP6031 35 } CSPs are a special kind of problem: ◦ states defined by values of a fixed set of variables ◦ goal test defined by constraints on variable values ◦ } Backtracking = depth-first search with one variable assigned per node } Variable ordering and value selection heuristics help significantly } } Forward checking prevents assignments that guarantee later failure } } Constraint propagation (e.g., arc consistency) does additional work to constrain values and detect inconsistencies } } Iterative min-conflicts is usually effective in practice } 36 Adversarial Game Playing Reading: Chapter 6 } States, actions, goals } Search techniques ◦ Blind (depth-first, breadth-first, IDS) ◦ Heuristic (best-first, A*) ◦ Local (hill-climbing, etc) } Search examples so far have concentrated on unopposed search problems (e.g. 8-square problem, eight queens, etc) } Game playing is a traditional focus for Artificial Intelligence ◦ Checkers (Samuels, 1959) ◦ Chess (Turing, 1957; Greenblatt, 1966) ◦ … } Must take account of the opponent } Start with a single pile of seven matches } Each player takes it in turn to take a pile of matches and split into two differently-sized piles of matches } The last player who is able to make a move is the winner } Very small game ◦ Few legal states ◦ Few legal moves from each state ◦ Game doesn’t last long (few turns) } Player who plays second has an (unfair) advantage } Possible to construct a tree representing all the states of the game 7 6-1 5-2 5-1-1 4-3 4-2-1 4-1-1-1 2-1-1-1-1-1 3-1-1-1-1 3-2-2 3-2-1-1 3-3-1 2-2-2-1 2-2-1-1-1 7 6-1 5-2 5-1-1 4-3 4-2-1 4-1-1-1 2-1-1-1-1-1 3-1-1-1-1 3-2-2 3-2-1-1 3-3-1 2-2-2-1 2-2-1-1-1 YOU ME YOU YOU ME ME } Initial state: initial board state } Goal state: winning positions } Actions: one for each legal move } Expand function: generate legal moves } Evaluation function: assigns score to each board state } Game tree: all possible games } View of search in adversarial games } MAX is traditionally the computer player, and picks the moves whose heuristic value is the best (maximum) } MIN is the opponent, and picks moves that minimise MAX’s advantage 7 6-1 5-2 5-1-1 4-3 4-2-1 4-1-1-1 2-1-1-1-1-1 3-1-1-1-1 3-2-2 3-2-1-1 3-3-1 2-2-2-1 2-2-1-1-1 MAX MAX MAX MIN MIN MIN } Search to a given ply (depth-limited search) } Evaluate heuristic for leaf nodes } Internal nodes propagate heuristic values towards tree root ◦ MAX nodes take the maximum of their child values (we make the best move we can) ◦ MIN nodes take the minimum of their child values (we assume our adversary makes the move that is worst for us) } Pick action with best guaranteed score function MAX-VALUE(state, depth) if (depth ==0) then return EVAL(state) v = -¥ for each s in SUCCESSORS(state) do v = MAX(v, MIN-VALUE(s, depth-1)) end return v function MIN-VALUE(state, depth) if (depth ==0) then return EVAL(state) v = ¥ for each s in SUCCESSORS(state) do v = MIN(v, MAX-VALUE(s, depth-1)) end return v 4 3 2 8 36 1 MAX MIN MAX 4 3 2 8 36 1 3 2 1 MAX MIN MAX 4 3 2 8 36 1 3 2 1 3MAX MIN MAX 4 3 2 8 36 1 3 2 1 3MAX MIN MAX } We use an evaluation function that is: ◦ +1 for a winning move for MAX ◦ -1 for a winning move for MIN 7 6-1 5-2 5-1-1 4-3 4-2-1 4-1-1-1 2-1-1-1-1-1 3-1-1-1-1 3-2-2 3-2-1-1 3-3-1 2-2-2-1 2-2-1-1-1 MIN MAX MIN MIN MAX MAX -1 -1 -1 -1 +1 +1 +1 +1 +1 +1 +1 +1 -1 -1 } Complete? Yes (if tree is finite) } Optimal? Yes (against an optimal opponent) } Time complexity? O(bm) } Space complexity? O(bm) (depth-first exploration) } For chess, b ≈ 35, m ≈100 for \"reasonable\" games à exact solution completely infeasible } Typically a linear function in which coefficients are used to weight game features } Unlikely to be a perfect, computable evaluation function for most games } Games with uncertainty (backgammon, etc) add notions of expectation For noughts and crosses: S = #(potential nought lines) -#(potential cross lines) For chess: } Any ideas? } Material is a good starting point ◦ e.g. S = (1 * numPawns) + (3 * numBishops) + … ◦ How do you decide on the weights? } Anything else? For chess: S = c1 * material c2 * pawn structure c3 * king safety c4 * mobility … Each of these can be another linear function } Cannot exhaustively search most game trees } Significant events may exist just beyond that part of the tree we have searched } The further we look ahead, the better our evaluation of a position } If we’re searching the game tree to a depth of n ply, what happens if our opponent is looking n+1 moves ahead? } Bad evaluation function can lead to wild swings in behaviour ◦ Consider just material in chess (queen capture) } Look for stable (quiescent) positions } Can expand nodes in nonquiescent positions to a deeper ply, until a relatively stable situation is reached } Consider this } A ply of 14 is needed to see the white pawn become a queen } Single extension is when one move is clearly better. (b=1!) 4 5 6 7 8 9 10 11 12 13 Max ply depth USCF Rating 1200 2000 Expert 2900 World champ MacHack (1966) Belle (1977) Deep Blue(1997) } The branching factor of a game is the number of actions which can be chosen } Nim has a low branching factor, but many games do not ◦ Chess (c. 36) ◦ Go (c. 200) } With game length (tree depth), affects complexity of decision making } Evaluation functions are core to adversarial games ◦ Usually a linear weighted function of easily computed game attributes } Minimax is the de-facto algorithm ◦ Ply depth very important ◦ Not efficient! Introduction to Learning in Artificial Intelligence Srinandan Dasmahapatra Sense-recognition-response cycle hard to encode into logical rules; learn patterns from data Recognising signals Speech recognition Labelling images Translating text Generating signals Speech synthesis, music generation Images (fakes) Text translation Natural language generation 1943 Beginnings: plasticity of neurons 1949 Neurons that fire together wire together 1958 From perceptrons to deep learning From Rosenblatt’s 1958 paper Contemporary convolutional neural network Perceptron learning rule: guided by mistakes Iteration step t (initial): Green line: decision boundary, characterised by perpendicular arrow (vector !) Iteration step t: Red arrow: vector \" on wrong side, dotted red arrow is −\". Change: ! → ! − &\", dotted black arrow New weight, step t+1: updated decision boundary, with dotted perpendicular arrow from previous frame Feature space ID X1 X2 Class 1 -2.7 1.8 -1 2 -1.4 0.5 -1 3 -0.5 -1.6 1 4 0.3 0.6 -1 5 0.4 2.0 -1 6 0.8 -1.6 1 7 2.5 -1.4 1 8 2.7 0.2 1 0 0-2 2 2 -2 Decision boundary: eqn of (hyper)-plane ! !\" = $\"1 + $#'# + $$'$ = 0 0 0-2 2 2 -2 Green line: !! = −1.8 + (!\" \"# \"#\"$ )!$ Or, −5!$ + 4!! + 7.2 = 0 Red line: !! = 1.0 + ( #\" \"# $\"(\"!) )!$ Or, −2!$ + !! − 1.0 = 0 % = 1 (! (\" ) = *# *! *\" ) = 7.2 −5 4.0. 0 ) = −1.0 −2.0 1.0 !'\" partitions plane into positive or negative regions for any \" Identify the errors to correct for decision boundary ID X1 X2 C 1 -2.7 1.8 -1 2 -1.4 0.5 -1 3 -0.5 -1.6 1 4 0.3 0.6 -1 5 0.4 2.0 -1 6 0.8 -1.6 1 7 2.5 -1.4 1 8 2.7 0.2 1 0 0-2 2 2 -2 ( Green line: \"( = −0.3 + ( )*.+)* *)()() )\". Or, 0.3\". + 2\"( + 0.6 = 0 ! = 0.6 0.3 2.0 , !'\" = 0 For every point in plane, ,- = ./01(!'\") is a prediction. If prediction for point 1 is ,-/, and training label is -/. ( ,-/)×( -/) = +1 if correct, −1 if incorrect. Perceptron algorithm: Update step ID X1 X2 C 1 -2.7 1.8 -1 2 -1.4 0.5 -1 3 -0.5 -1.6 1 4 0.3 0.6 -1 5 0.4 2.0 -1 6 0.8 -1.6 1 7 2.5 -1.4 1 8 2.7 0.2 1 0 0-2 2 2 -2 ID:8 on wrong side of decision boundary (green line). Use its location (') to alter weight vector for new decision boundary: (((\") ↦ ((\"$%) = ((\") ± , ') ( −2 ( Old ! New boundary Green line (old boundary): 0.3'% + 2'& + 0.6 = 0 <latexit sha1_base64=\"dEbxC95e7aE0NnCpfp4qR95ais4=\">AAADkXicnVJdixMxFE1n/Fir7nbdR1+CxaULdsi0HdsilaIvgi8r2N2FppRMJm3DZj5IMmoJ+T3+H9/8N2Zqhe4Xwl4YOJyTe87NncSF4Eoj9Lvm+Q8ePnq896T+9Nnz/YPG4YszlZeSsgnNRS4vYqKY4BmbaK4FuygkI2ks2Hl8+bHSz78xqXiefdXrgs1Sssz4glOiHTU/rP3EMVvyzBApydoaIW3d4DgXSUr0yny3dmTwJsYsJWOZxYItdOtqE7UoeIsxCroYdwKEWZZsJSz5cqVP7Bv8Du7Y/rB2bnAFZWqYlLl0Obc7h5VlvzLv3PQ9hhhfG7eNmSb3i/rPJdotp53Ae4w5Or4r0GW1URBhHAbDm331HWreaKIg7Ebdfh+ioBdFHRQ5MBwOet0BDAO0qSbY1um88QsnOS1TlmkqiFLTEBV65tw0p4LZOi4VKwi9JEs2dTAjKVMzs/nVFr52TAIXuXRfpuGG3e0wJFVqncbuZLVadV2ryNu0aakXg5nhWVFqltG/QYtSQJ3D6nnChEtGtVg7QKjkblZIV0QSqt0jrrsl/LspvBucdYIwCtCXXnP8YbuOPfASvAItEII+GINP4BRMAPX2vZ438t77R/7QH/vbs15t23MErpT/+Q/b3irs</latexit> w = 0 @ 0.6 0.3 2.0 1 A,xerror = 0 @ 1 2.7 0.2 1 A w \u0000 ⌘xerror = 0 @ 0.6 0.3 2.0 1 A \u0000 (0.3) 0 @ 1 2.7 0.2 1 A = 0 @ 0.3 \u00000.5 1.9 1 A Perceptron learning algorithm for binary(±1) classification • Target (output) labels . ∈ {+1, −1}. • Features !$, !!, … , !; vector components of 4 ∈ 5; • Weight features by 6$, 6!, … , 6; to produce 7<4, 7 ∈ 5; • If 7<4 > −6=, predict 9. = +1, else predict 9. = −1. • For every misclassified example 4> (i.e., 9.> .> = −1): • > ← > + @-/A/ where @ is called the learning rate A hatchet job Not linearly separable 1969 Minsky and Papert use the word perceptron to denote a restricted subset of the general class of Perceptrons. They show that these simple machines are limited in their capabilities. This approach is reminiscent of the möhel who throws the baby into the furnace, hands the father the foreskin and says, “Here it is; but it will never amount to much.” H D Block, Information and Control 17, 501-522 (1970) From perceptrons to deep learning From Rosenblatt’s 1958 paper Contemporary convolutional neural network 1 1 -1 -1 1 -1 -1 Hidden Layer 1 Perspective: reasoning as multivariate state change • GOFAI: logical/rule-based symbolic representations of state • Now: internal representation: states ∈ 5?, big :. • Hinton: “reasoning is just sequences of such state vectors” G Hinton Older metaphors for the brain: hydraulics, mechanical clockwork, electrical circuits, bit registers Varieties of Learning tasks in Artificial Intelligence: Classification beyond perceptrons Srinandan (“Sri”) Dasmahapatra Machine Learning: the basic structure • A repeatable task that can be evaluated • Data driven model facilitates task • Alter model to perform better on task !! Task Evaluation Model !! Task Evaluation Supervised Learning: models of association in labelled data: Classification / Regression \": = {(!!, (!)}, + = 1, … , . Model!! !\"! = /(!!) 0 = ∑ !\"# $ 2(!\"!, (!) • Given data \" construct model # such that the distance between model output and real “output” is small • If $! is discrete (e.g., +1/-1, cat/dog/bird), # is a classification model; if $! continuous, # is a regression model • Mismatch between model prediction and given labels quantified as loss function • Learning = model construction by minimising loss L \"#! Unsupervised Learning: data unlabelled: Discover patterns, capture essence • Group data into similar subsets (cluster label cn) • Inputs xn mapped to output cn • Representation learning — express data in transformed manner: #! captures “essence” • Input $! mapped to itself via #! intermediate code (think compression) 0 = ∑ !\"# $ 2(!$!, !!) 3! = /%(!!), !$! = /&(3!) encode decode !! 3! \"!! \": = {!!}, + = 1, … , . Clustering!! 4! 0 = ∑ !\"# $ 2(%!, !!) How does the perceptron fit into this general story? What is the loss function there? • Perceptron algorithm: updates driven by misclassification • No errors – no more iterations • Do we always converge? o Only if data linearly separable (recall XOR) o Not clear if convergence is reached • Even if data linearly separable, there could be o Multiple solutions for !, decision boundary hyperplane • What about the Loss function: 1. Number of misclassifications: discrete jumps, hard to train 2. Perceptron criterion: smooth, differentiable Perceptron as example of supervised learning: classifying to labels +1/-1 \": = {(!!#, !!', … , !!(, (!)}, + = 1, … , . Model # !\"! = 567+( 89 ):!) & ' = ∑ !\"# $ 2( !\"!, (!) • Given data $ construct model % such that the distance between model output and real “output” is small • If #! is discrete (e.g., +1/-1, cat/dog/bird), % is a classification model • Mismatch between model prediction and given labels quantified as loss function • Learning = model construction by minimising loss L \"#! !!\" !!# ⋮ !!$ Minimise ' ( for choice of ( Perceptron loss function reduced by gradient descent • Number of misclassifications: o recall, if prediction \"#! and label #! agree ( \"#!)(#!) = 1, else ( \"#!)(#!) = -1. o ∑! (1 − \"#!#!)/2 = ∑! (1 − 0123(4%!!)#!)/2 counts misclassifications o Discrete function of ( : therefore changes to loss in jumps, hard to train • Perceptron criterion (drop the ()*+ ): o ' 4 = (\" # ) ∑! (1 − #!4%!!) o Derivative exists o & &' ' 4 = − \" # ∑! !!#! o Change of 4: proportional to & &' ' 4 -- gradient descent Perceptron can linearly separate outputs of logical OR and AND 0 1 1 0 1 1 0 OR AND 0 1 !\" !# 4( 4\" 4# 0 !\"! = 567+( 89 ):!) But what can we do about XOR and other functions that need non-linear boundaries? 0 1 1 XOR 0 1 !\" !# 4( 4\" 4# XOR: (i) map each of 4 inputs ! by 2 linear maps (dot with 2 vectors \") (ii) non-linear activation 0 1 1 XOR 7 = 1 !\" !# ( = 4( 4\" 4# !\" = 1 0 0 , 7) = 1 0 1 , 7* = 1 1 0 , 7+ = 1 1 1 , (, = 0 1 1 , () = −1 1 1 7%(\" 7%(# (i) Two sets of dot products presented graphically XOR: (i) map each of 4 inputs ! by 2 linear maps (dot with 2 vectors \") (ii) non-linear activation 0 1 1 XOR 7 = 1 !\" !# ( = 4( 4\" 4# 7%(\" 7%(# 9-(7%(\") 9-(7%(#) (ii) (ii) :- ! = ; !, => ! > 0 0, @ABCDE=FC . / . = . / . = 1!(.) . Internal representation (hidden layer, H) capable of linear separation of output of XOR 0 1 1 G, = 9-(7%(\") G) = 9-(7%(#) !\" !# 1 %\" %# ℎ\" ℎ\" 1 <latexit sha1_base64=\"7hiMvqGbHSrrEBCo0aIo4xzFIn0=\">AAAChXicdVHLbtswEKTUR1L35TTHXogaDdxDBMq2audQNGgvPaZAHRswDYOiVjYRihJIKoAh6E/yVbn1b0LZCpA+MgSBwezukLsbF1IYS8hvz3/y9Nnzg8MXnZevXr952z16d2nyUnOY8lzmeh4zA1IomFphJcwLDSyLJcziq+9NfHYN2ohc/bLbApYZWyuRCs6sk1bdm4rGuUwyZjfVvK6/UAmp7dMY1kJVTGu2rSvuUIcnu0MpOSH3JGwoBZW0mVSL9cZ+ohQ/cJ097lq3Tqd763+MVt0eCcJhNByPMQlGUTQgkSNnZ5PRcILDgOzQQy0uVt1bmuS8zEBZLpkxi5AUdulMreAS6g4tDRSMX7E1LBxVLAOzrHZTrPFHpyQ4zbW7yuKd+rCiYpkx2yx2mU1j5u9YI/4vtihtOllWQhWlBcX3D6WlxDbHzUpwIjRwK7eOMK6F+yvmG6YZt25xHTeE+07x4+RyEIRRQH6Oeuff2nEcovfoA+qjEI3ROfqBLtAUcc/3+l7oDfwD/9Qf+Z/3qb7X1hyjP+B/vQOJ98GU</latexit> X = 0 @ 1111 0011 0101 1 A W = ✓ 01 1 \u0000111 ◆ <latexit sha1_base64=\"wEA19FyqM9Vtg8BUNYNvbWp6inE=\">AAAChXichVFNj9MwEHXCxy7lq8CRi0XFqghtZLcN7R4QK7hwXCS6rVRHleNMUmsdJ7IdpCrKP+FXcePf4HYLAsSKN7L09Gbe2J5JayWtI+R7EN66fefu0fG93v0HDx897j95emmrxgiYi0pVZplyC0pqmDvpFCxrA7xMFSzSqw+7/OILGCsr/dlta0hKXmiZS8Gdl9b9r8zKouTr16uWpZXKSu427WLZdcnbXxmmIHdDlkIhdcuN4duuFR4dOaE+Royd0hPigzLQ2aGCGVls3Cvf5r9ucpN53R+QiI7j8XSKSTSJ4xGJPTk7m03GM0wjsscAHXCx7n9jWSWaErQTilu7oqR2iW/qpFDQ9Vhjoebiihew8lTzEmzS7qfY4ZdeyXBeGX+0w3v1d0fLS2u3Zeord1Oyf+d24r9yq8bls6SVum4caHF9Ud4o7Cq8WwnOpAHh1NYTLoz0b8Viww0Xzi+u54fw86f4ZnI5imgckU+Twfn7wziO0XP0Ag0RRVN0jj6iCzRHIgiDYUCDUXgUnoaT8M11aRgcPM/QHwjf/QABjcGm</latexit> \u0000+[WX]= \u0000+[✓ 01 1 2 \u00001001 ◆]= ✓ 0112 0001 ◆ <latexit sha1_base64=\"uvkMvP0otbqzqIQttlOfks9H7S0=\">AAACXHicdVHBahsxENVu09ZxmtZtIZdeRE3BgXbR2l7sHAKhueSYQpwELNdotbO2iFbaStoWs+xP9pZLf6WVHRea0AwIPd6bGc08paUU1hFyG4RPdp4+e97abe+92H/5qvP6zaXVleEw4Vpqc50yC1IomDjhJFyXBliRSrhKb07X+tV3MFZodeFWJcwKtlAiF5w5T807tq5pqmVWMLesfzRNc0wl5K5HU1gIVTNj2KqpeRNT+qmPKahsy1EjFkt3+BE/bPCVOl3iM3yMe4R+q1iG43sXOZx3uiSKB8lgNMIkGiZJnyQeHB2Nh4MxjiOyiS7axvm885NmmlcFKMcls3Yak9LN/CBOcAlNm1YWSsZv2AKmHipWgJ3VG3Ma/MEzGc618Uc5vGH/rahZYe2qSH3megf7UFuT/9OmlcvHs1qosnKg+N1DeSWx03jtNM6EAe7kygPGjfCzYr5khnHn/6PtTfi7KX4cXPajOInIl2H35PPWjhZ6h96jHorRCJ2gM3SOJoijW/Q7aAW7wa9wJ9wL9+9Sw2Bb8xbdi/DgD/smtlE=</latexit> w = ✓ 1 \u00002 ◆ ,w>H = (0 1 1 0) <latexit sha1_base64=\"8yzDdF+gEnM570ljAKsx/TqOk64=\">AAAB9XicdVDLSgMxFM34rPVVdekmWARBGGbaDm0XQtFNlxXsA6ZjyaRpG5pkhiSjlKH/4caFIm79F3f+jelDUNEDFw7n3Mu994Qxo0o7zoe1srq2vrGZ2cpu7+zu7ecODlsqSiQmTRyxSHZCpAijgjQ11Yx0YkkQDxlph+Ormd++I1LRSNzoSUwCjoaCDihG2ki39YuuokOOeud+uxP0cnnHdotesVyGjl3yvILjGVKtVkrFCnRtZ448WKLRy713+xFOOBEaM6SU7zqxDlIkNcWMTLPdRJEY4TEaEt9QgThRQTq/egpPjdKHg0iaEhrO1e8TKeJKTXhoOjnSI/Xbm4l/eX6iB5UgpSJONBF4sWiQMKgjOIsA9qkkWLOJIQhLam6FeIQkwtoElTUhfH0K/yetgu16tnNdytcul3FkwDE4AWfABWVQA3XQAE2AgQQP4Ak8W/fWo/VivS5aV6zlzBH4AevtExkQkkU=</latexit> H = \u0000+[WX] Internal representation and dimensionality • “Instinctively birds that fly swim”* • “Instinctively” unambiguously modifies “swim” not “fly” • Linear (one dimensional) distance inadequate to capture this *Why Only Us, by Robert Berwick and Noam Chomsky Word order and ambiguity • “Instinctively birds that fly swim”* • “Instinctively” unambiguously modifies “swim” not “fly” • However, “Birds that fly instinctively swim” is ambiguous – • “instinctively” modifies either “swim” or “fly” Ambiguity often a source of humour (multiple representational structures ) • “in my pyjamas” is a participial phrase (PP): • (i) adverbial or (ii) adjectival S NP VP I shot an elephant in my pyjamas NP PP N NPP V det S NP VP I shot an elephant in my pyjamas NP NP V NPPdet N Multiple parse trees underly linear sequence PP Modern neural network architectures seek to capture domain specific representations implicitly !! !\" ⋮ !# Emergent linguistic structure in artificial neural networks trained by self-supervision Christopher D. Manning et al. PNAS doi:10.1073/pnas.1907367117 ©2020 by National Academy of Sciences Introduce nonlinearity in transformations :- ! = ; !, => ! > 0 0, @ABCDE=FC . / . = . / . = 1!(.) . ReLU : ! = 1 (1 + I56 ) . / . = . . / . = 1!(.)Sigmoid <latexit sha1_base64=\"1LGlARCeyjQ/E1JOIvg7Ocpg+yk=\">AAACJnicdZDLSgMxFIYzXmu9VV26CRahRSgZW2k3haIblxWsLXSGkkkzbTBzITkjlKFP48ZXceOiIuLORzG9KCp6IPDx/+dwcn4vlkIDIW/W0vLK6tp6ZiO7ubW9s5vb27/RUaIYb7FIRqrjUc2lCHkLBEjeiRWngSd527u9mPrtO660iMJrGMXcDeggFL5gFIzUy9WdIYV0NK77BUeLQUB7J912wWH9CPR34Qs7btGd20W32MvlSYmQMqmWsQGb2JXqDGo1u4ZtY00rjxbV7OUmTj9iScBDYJJq3bVJDG5KFQgm+TjrJJrHlN3SAe8aDGnAtZvOzhzjY6P0sR8p80LAM/X7REoDrUeBZzoDCkP925uKf3ndBPyam4owToCHbL7ITySGCE8zw32hOAM5MkCZEuavmA2pogxMslkTwuel+H+4OS3ZZyVyVck3zhdxZNAhOkIFZKMqaqBL1EQtxNA9ekQT9Gw9WE/Wi/U6b12yFjMH6EdZ7x+oI6VB</latexit>ˆy = f (\u0000+[W (·· · \u0000+[W (\u0000+[WX])] ·· · )]) Increased representation capacity with hidden layers, but is the model trainable? • Can you find the suitable weights W? • Modify W based on mistakes – loss function • Loss function chosen to be differentiable • Backpropagation Linear regression: learning continuous labels Srinandan (“Sri”) Dasmahapatra Machine Learning: the basic structure • A repeatable task that can be evaluated • Data driven model facilitates task • Alter model to perform better on task !! Task Evaluation Model !! Task Evaluation Supervised Learning: data with continuous labels Regression • Given data ! construct model \"(⋅; &) such that the “distance” between model output and real “output” is small • Learning = model construction by minimising loss \"($) = ∑ !\"# $ (( )*!, *!) • * = ,(⋅; $) continuous (e.g., y=23.4), !(⋅; %) is a regression model /: = {(!!, *!)}, 3 = 1, … , 6 Model !(⋅; %)!! !\"! = ,(!!; $) (( )*!, *!) How far is prediction \"#! from actual data '!? )*! Reduce mismatch between model prediction and data Squared residual loss function • Regression models minimise residuals — deviations of model predictions from outputs in training data • Contribution to loss function: ,!(-) = (*! − \"(0!; -))\" = 1! \"(-). • Average loss: \"($) = # $ ∑ !\"# $ 7!($) #\"! = ,(!!) x y (!: = ('! − !\"!) Minimise this by tuning $ <latexit sha1_base64=\"0EV0kjCkGqw1M5IerYKGGotzJD8=\">AAACQXicbVDNS8MwHE3n9/yaevQSHMKGOFpRFEQQvXhUcG6w1pJmqQtLk5Kkain917z4H3jz7sWDIl69mM2CH/NB4OW93+OXvCBmVGnbfrRKY+MTk1PTM+XZufmFxcrS8oUSicSkiQUTsh0gRRjlpKmpZqQdS4KigJFW0D8e+K1rIhUV/FynMfEidMVpSDHSRvIr7dTn8AC6PaSzNDd8A7rCBAKJMMlqA3fz263nl5n0eW4SYe3W5/tuIFg3u8nrJmeM2vfdr1Tthj0EHCVOQaqgwKlfeXC7AicR4RozpFTHsWPtZUhqihnJy26iSIxwH12RjqEcRUR52bCBHK4bpQtDIc3hGg7Vn4kMRUqlUWAmI6R76q83EP/zOokO97yM8jjRhOOvRWHCoBZwUCfsUkmwZqkhCEtq3gpxD5nytCm9bEpw/n55lFxsNZydhn22XT08KuqYBqtgDdSAA3bBITgBp6AJMLgDT+AFvFr31rP1Zr1/jZasIrMCfsH6+ATrga9I</latexit> yn =ˆyn + rn z }| { (yn \u0000 ˆyn)= f (xn; w)+ rn(w) residuals Choose weight for minimum loss Example: find slope of straight line • Fit y=w x to data, slope w of the line is the weight/parameter to be learnt • Loss L = sum of squares of residuals (in red), for 3 possible choices of slopes — {\"$, \"%, \"&}: the 3 residuals for input xn is shown • Choose the slope that gives the smallest value from {\"(8#), \"(8%), \"(8&)} xn yn $!(&) = )! % = (&*! − \"!)% \"(8) = 1 6 ∑ !\"# $ 7!(8) w Loss function needs a distance: introducing the norm Treat all data points as collective unit Loss = mean [(length)2 of N-dim vector of residuals Other distance measures possible, such as mean[abs values] Update weights to reduce loss: gradient descent Differentiable loss function: ,!(&) = 1! \" = (&0! − *!)\" '(\"#$) = '(\") + * +,('(\")) +' , * < 0 •Slope can take any real value w∈ ℝ •Iterate w(t) ↦w(t+1) , t = 0, 1, …, •Update weights w(t) to w(t+1) so that L(w(t+1)) < L(w(t)) •Change weights in the direction opposite to the slope of the loss function (we want to reduce the loss, hence descent) 2 = arctan +, +' ! '()) linear approximation of loss function Learning rate 3 Linear Regression: solving for zero gradient of loss • ,(') = (1/:) (';$ − #$)& + (';& − #&)& + ⋯ + (';' − #')& • Loss function is quadratic in w • 4(&) = 5&\" + 7& + 8 • Follow gradients until minimum reached • Solution for weights: set gradient = 0 Exercise: differential calculus • .(&) = (1/1) (&*$ − \"$)% + (&*% − \"%)% + ⋯ + (&*> − \">)% • Exercise: In .(&) = 4&% + 5& + 6, show • 4 = (1/1)[*$ % + *% % + ⋯ + *> % ] • 5 = (−2/1)[*$\"$ + *%\"% + ⋯ + *>\">] • 0 = ?@(A) ?A |ABA∗ ⟹ &∗ = − D %E = F\"G F\"F Closed form solution exists, special case 90ox y !9\" Many parameters: reduce loss by gradient descent: optimisation by vector calculus ('H()I = +( +\"I !(\"#$) = !(\") − $%&& Evaluate partial derivatives (gradient) to choose direction of weight updates Fit straight line to (\"!, $!), & = 1, … , * <latexit sha1_base64=\"tX6YtGMdBmXQW7TGzhlWaSpuioY=\">AAACNXicdZDNSgMxFIUz/lv/Rl26CRahopSkqG0RQXTjwoWCVaEtQybN2GAmMyQZaxn6Um58D1e6cKGIW1/BjK1gRS8kHM6Xm+QePxZcG4SenJHRsfGJyanp3Mzs3PyCu7h0rqNEUVajkYjUpU80E1yymuFGsMtYMRL6gl3414cZv7hhSvNInpluzJohuZI84JQYa3nucaNNTNrteRLuwaBw68ndRiJbTGUXpp3eurU7HoIbdsfQ4k04xC0uWL6Z4XXPzaMiQghjDDOByzvIimq1UsIViDNkKw8GdeK5D41WRJOQSUMF0bqOUWyaKVGGU8F6uUaiWUzoNblidSslCZlupl9T9+CadVowiJRd0sAv92dHSkKtu6FvT4bEtPVvlpl/sXpigkoz5TJODJO0/1CQCGgimEUIW1wxakTXCkIVt3+FtE0UocYGnbMhfE8K/xfnpSLeLqLTrfz+wSCOKbACVkEBYFAG++AInIAaoOAOPIIX8OrcO8/Om/PePzriDHqWwVA5H59pnKlX</latexit>ˆyn = f (xn; w)= w0 + w1xn,w =(w0,w1) <latexit sha1_base64=\"PrqU9xgzq7StWJ24KeegwfDqhyk=\">AAACZXicdVFBb9MwGHXCgNIx6MbEZQc+USF1QqvsamPdYdIEFw4IbdK6TWqK5bjOas1xItuhRCF/ktuuu/A3cLpOArR9kqXn99732X6OcyWtw/g6CB+tPH7ytPWsvfp87cXLzvrGmc0Kw8WIZyozFzGzQkktRk46JS5yI1gaK3EeX31q9PPvwliZ6VNX5mKSskstE8mZ8xTt/PzSiwo9FaYZUM3rbTiEKDGMV6SuvtaRLVKqwVD9bXCv0iup3olmzFVlTfX2Q645xfAe5pTAD7/dgXLhpZ0u7mOMCSHQALL/AXtwcDAckCGQRvLVRcs6pp1f0TTjRSq044pZOyY4d5OKGSe5EnU7KqzIGb9il2LsoWapsJNqkVIN7zwzhSQzfmkHC/bvjoql1pZp7J0pczP7v9aQ92njwiXDSSV1Xjih+e1BSaHAZdBEDlNpBHeq9IBxI/1dgc+Yz8j5j2n7EO5eCg+Ds0Gf7PXxyW736OMyjhbaQm9RDxG0j47QZ3SMRoijm6AVrAcbwe9wLdwMX99aw2DZ8wr9U+GbPyuEteM=</latexit> L(w)= 1 N X n r2 n = 1 N X n (yn \u0000 ˆyn)2 = 1 N X n (w0 + w1xn \u0000 yn)2 <latexit sha1_base64=\"DCxz09TKMQlzJVj4A+moxqSvMFo=\">AAACU3icdVHPS8MwGE3r/DV/VT16CQ5BL6OdittBEL14EFFwU1hLSbN0i0vTkqTKKP0fRfDgP+LFg6bbFDf1g4THe9/3krwECaNS2farYc6UZufmFxbLS8srq2vW+kZLxqnApIljFou7AEnCKCdNRRUjd4kgKAoYuQ36Z4V++0CEpDG/UYOEeBHqchpSjJSmfOveDQXCmZsgoShi+TeCjz7NL3bdlHeIKNyzx3wPHsNRv5Nnl7kr08jnsCb0NmkDNTVlBaFvVexqrVG39xvwN3Cq9rAqYFxXvvXsdmKcRoQrzJCUbcdOlJcVppiRvOymkiQI91GXtDXkKCLSy4aZ5HBHMx0YxkIvruCQ/TmRoUjKQRTozgipnpzWCvIvrZ2qsO5llCepIhyPDgpTBlUMi4BhhwqCFRtogLCg+q4Q95BOR+lvKOsQvl4K/wetWtU5rNrXB5WT03EcC2ALbINd4IAjcALOwRVoAgyewBv4MIDxYrybplkatZrGeGYTTJS58gkTvLXG</latexit> @ @wi L(w)= 1 N X n 2rn @rn @wi Exercise: evaluate <latexit sha1_base64=\"aitUagsFIGNUQJpBUxR4USi4DjU=\">AAACO3ichVC7TsMwFHV4lvIKMLJYVEhMlcNDtFsFC2NB9CE1VeS4TmvVcSLbAaoo/8XCT7CxsDCAECs77gPxFkeydHzOvde+x485UxqhO2tqemZ2bj63kF9cWl5ZtdfW6ypKJKE1EvFINn2sKGeC1jTTnDZjSXHoc9rw+8dDv3FBpWKRONeDmLZD3BUsYARrI3n2mRtITFI3xlIzzKH0RPZxu/RQBqEb+tFVCrHowAz+U+9k0LMLqIjKJbRXhj+JU0QjFMAEVc++dTsRSUIqNOFYqZaDYt1OhzMJp1neTRSNMenjLm0ZKnBIVTsd7Z7BbaN0YBBJc4SGI/VzR4pDpQahbypDrHvquzcUf/NaiQ5K7ZSJONFUkPFDQcKhjuAwSNhhkhLNB4ZgIpn5KyQ9bMLRJu68CeF9U/g3qe8WnYMiOt0vVI4mceTAJtgCO8ABh6ACTkAV1AAB1+AePIIn68Z6sJ6tl3HplDXp2QBfYL2+AeZ9rzY=</latexit>@rn @w0 and @rn @w1 Best fit regression line • Set partial derivatives of L with respect to weights to 0 • Vanishing gradient implies weight updates stop • Show\t 1. ∑!B$ > )! = 0 2. ∑!B$ > *!)! = 0 • Solve for &J, &$: • \"\" = # $ %&' +()$ %*') # $ %&' + # $ %&' • &J = < \" > − &$< * > G = K# K$ ⋮ K% , < * > = $ > ∑!B$ > *! H = M# M$ ⋮ M% , < \" > = $ > ∑!B$ > \"! Best fit regression line via gradient descent From Watt, et al, Machine Learning Refined https://github.com/jermwatt/machine_learning_refined Learning or memorising? Different training sets for same model has different residuals, hence different model parameters with different predictions: Can we Generalise? 1 Learning or memorising? Evaluating ML models: Cross-validation • Evaluate model performance on test data (generalisation) • Different training sets can lead to different model parameters with different predictions, hence different residuals • Characterise model on distribution of residuals trained on different subsets of available training data • Cross validation Coloured boxes: possible training sets Overfitting: good on training set, poor on test set Polynomials of high degrees overfit From C Bishop, PRML # ̂ = %\" + %#' # ̂ = %\" + %#' + %$'$ + %%'% # ̂ = %\" + %#' + ⋯ + %&'& + %''' )*(0; -) = &) + &*0 + &\"0\" + ⋯ + &+0+ M complexityerror Overfitting: good on training set, poor on test set Polynomials of high degrees overﬁt From WaC, et al, Machine Learning Reﬁned hCps://github.com/jermwaC/machine_learning_reﬁned )*(0; -) = &) + &*0 + &\"0\" + ⋯ + &+0+Dealing with variability requires probabilistic modelling • That will be covered next time • Here, we introduced supervised learning of continuous target values • Deviation of prediction from given label = residual • Loss function – mean (square of residuals) • Partial derivatives of loss gives direction of movement in w-space • Learn to generalise, not memorise – identify patterns that work across datasets • Can we capture and model variability, to make rational decisions based on learning? Probabilities: Mostly review of COMP1215 Noisy channel model: Task: Decode(X|Y) Input(X), Channel(Y|X), Output(Y) Input (image) zero one two three four five six seven eight nine Output (text/symbol) Channel Noisy Channel Model Application Input Output Channel Prior Machine translation L1 word sequence L2 word sequence Translation model Probability of L1 sequence Speech recognition Word sequence Acoustic utterance Acoustic model Probability of word sequences Optical Character Recognition Intended document Visual appearance on page Typographic model Probability of document text Document Classification Class/Topic label Actual document Document from topic model Probability of topic Part of Speech tagging POS tag sequence Text sentence Words from POS model Probability of POS tag sequence Probabilistic Framework S zero one two three four five six seven eight nine ! \" = $! ) = ! \" = $!) !(\" = $!) !( ) Contextual information three one four one five nine two six not eight ! = 3.1415926 … . . Learning the pattern? ! = 3.1415926 …Probabilistic Framework n P(X=x) n P(Y=y) n P(Y=y | X =y); prob Y=y given X has value x. n Decoding: Among all values P(X=x | Y=y) for different values x, find the one (say x*) for which ★ P(X=x* |Y=y) ≥ P(X=x |Y=y) for all other x ★ x* = argmax P(X=x | Y=y) Input(X), Channel(Y|X), Output(Y) Task: Decode(X|Y) Probability Theory Basics -- fundamental axioms n P:{events} ---> [0,1] or P:{proposition} ---> [0,1] n P(A) = 1 if and only if A is certain n If two events or propositions are mutually exclusive (incompatible, disjoint) then P(A or B) = P(A) + P(B) n For independent events or propositions, P(A and B) = P(A) P(B) n Represent event space as ovals (Venn diagrams): B BA contrast with A Events, Variables and Values n Uppercase random variable !, lowercase value ! = #! n Propositional or Boolean/categorical random variables: ‣ Covid = yes/no ‣ Forecast = sun/rain/cloud/snow n Real valued: ‣ Reproduction number, R From COMP1215 Probability Basics: Joint, Conditional and Marginal Probability From COMP1215From COMP1215From COMP1215 Probability Basics: Conditional Probability P(A|B, A)=1 P(A|B, not A)=0“Lack of modularity” From COMP1215 Given the table of the joint distribution, construct the tables for P(X|Y) and P(Y|X) From COMP1215From COMP1215 Given the table of the joint distribution, construct the tables for P(X|Y) and P(Y|X) From COMP1215 Observe the values in the conditional and the jointFrom COMP1215From COMP1215From COMP1215 f=“free money”; s=“spam”From COMP1215From COMP1215From COMP1215From COMP1215From COMP1215From COMP1215 Now that we’ve reviewed all of this material, what next? n Patterns of probabilistic reasoning: n I’m not home, neighbour John calls to say my alarm is ringing, but neighbour Mary doesn't call. Sometimes it's set off by minor earthquakes. Is there a burglar? n During the dry season a sprinkler is sometimes used. I slip and fall on the pavement; is it a dry season? !! !\" !# !$!% burglary earthquake alarm Mary callsJohn calls Reinforcement Learning Srinandan Dasmahapatra Different guidance for learning: action dependent evaluation vs action-independent instruction • Difference between supervised learning – instruct correct answer/label … • … and reinforcement learning – evaluate action undertaken Evaluation: making rational decisions based on utility – desirability of outcomes • Choose action ! ∈ #, % → ! %′: transit from % to %’. (%, %\" ∈ +) • -(%′|%, !): probability of being in %’ upon taking action ! from % • -[result ! = %′]=∑#∈% - %\" %, ! -(%) • Utility function 3 % , 3: + → ℝ, desirability of state % • Expected utility: 63(!) = ∑#!∈% - [result ! = %′]3(%\") • Rational agent chooses action !∗that maximises expected utility: !∗ = arg max !∈' 63(!) • Specifies the what, not the how K-armed bandit (\", ℛ) Action, A Prob(A=ai) [Reward | A=ai] P(A)*[R|A] A=a1 P(a1)=? r1 P(a1)*r1 A=a2 P(a2)=? r2 P(a2)*r2 A=a3 P(a3)=? r3 P(a3)*r3 A=a4 P(a4)=? r4 P(a4)*r4 Four-armed bandit Best action: argmax(\" < = = !)] Expected utility = ∑)*+ , - = = !) < = = !) ] But if rewards unpredictable, what action to choose? Need model of reward to plan action Exploitation or exploration • Action at time t, =- • Reward at time t, <- =- = !] • >∗ ! = 6 <- A. = a] value of action • If >∗ ! known, choose argmax >∗ ! • Estimate @-(!) of value of action • >∗ ! − @- ! small • If ! = argmax( @-(!) chosen, greedy choice :exploitation • If not, exploration: seek better model of non-greedy action’s rewards Reward, [Rt = r | At = a ]Prob (Rt= r)Exploitation or exploration: uncertainty reduction • Non-greedy choice =- = !, uncertain reward <- =- = !] • Could be better than greedy choice !/# = argmax( @-(!) • Need more values of <- =- = !] to get better estimate • Hence, take non-greedy choices • Cannot take two actions at once Action value method • Notation: ! \" = 1 if predicate \" is true, 0 otherwise • Number of times action \" taken before time %, '!\"# $%# ![)! = \"] • Sum of rewards for actions \" before time %, '!\"# $%# +!![)! = \"] • ,$ \" = & !\"# $%# '!([*!\"+] & !\"# $%# ([*!\"+] = & !\"# $%# '!([*!\"+] -$(+) • For -$(\") → ∞, ,$ \" → 2∗(\") = 3 +$ A1 = a] value of action • Greedy action: )$ = arg max + ,$(\") • Exploits current knowledge to maximise immediate reward • Alternative: with probability : explore non-greedy actions Quiz • For each time t = 1, 2, … 6, calculate Qt(1), Qt(2), Qt(3) • For the non-greedy sequence of actions shown 2 4.0 At R 1 4.0 1 1.5 3 2.0 2 2.0 1 0.5 time Action Q1(Action) Q2(Action) Q3(Action) Q4(Action) Q5(Action) Q6(Action) 1 0 4.00 4.00 2.75 2 0 0 4.00 4.00 3 0 0 0 0 Your turn: Complete the table Quiz • For each time t = 1, 2, … 6, calculate Qt(1), Qt(2), Qt(3) • For the non-greedy sequence of actions shown 2 4.0 At R 1 4.0 1 1.5 3 2.0 2 2.0 1 0.5 time Action Q1(Action) Q2(Action) Q3(Action) Q4(Action) Q5(Action) Q6(Action) 1 0 4.00 4.00 2.75 2.75 2.75 2 0 0 4.00 4.00 4.00 3.00 3 0 0 0 0 2.00 2.00 • Notice: greedy action biased by early choices • Good to explore in the beginning • Choose parameter B for exploration -- epsilon-first / epsilon-greedy 2 4.0 1 4.0 1 1.5 3 2.0 2 2.0 1 0.5 Action Q1(Action) Q2(Action) Q3(Action) Q4(Action) Q5(Action) Q6(Action) 1 0 4.00 4.00 2.75 2.75 2.75 2 0 0 4.00 4.00 4.00 3.00 3 0 0 0 0 2.00 2.00 Multi-armed bandits: balancing exploitation (optimality) against exploration (!-dependent) • 0 ≤ E ≤ F, F is number of times an arm (of bandit) is pulled • No/little knowledge of rewards for early times • Epsilon first: Explore for 0 ≤ E ≤ BF, exploit for BF ≤ E ≤ F, B < 1. • Epsilon greedy: At each choice, choose optimal arm with probability 1 − B , choose one of remaining (I − 1) arms with probability B • Goal: optimal strategy -- maximise long term rewards: üFor E → ∞, @- ! → >∗(!) = 6 <- A. = a] value of action • Early convergence Simulation study – 1* -5 5 0.1 0.2 0.3 0.4 • Extend the 4-armed bandit scenario to 10-armed case • 10 distributions of rewards • Choose different !-dependent strategies for choosing actions • Plot accumulated rewards * Sutton, Barto: Reinforcement learning4-armed bandit Reward distributions Reward distributions for simulation study *Performance of &-greedy strategy: B = 0, 0.01, 0.1 * Sutton, Barto: Reinforcement learning • Greedy actions stuck in sub-optimal choices • Value of exploration increases in uncertain environments (greater variance) Quiz: 4-armed bandit; &-greedy, '\"#$ ( = 0 • For non-zero B, action can be selected at random. • On which time steps did this definitely occur? • On which time steps could this possibly have occurred? T=1 T=2 T=3 T=4 T=5 action 1 2 2 2 3 reward 1 1 2 2 0 * Sutton, Barto: Reinforcement learning Learning '\"((): learning by averaging rewards; recursively compute average • ,$ \" = & !\"# $%# '!([*!\"+] & !\"# $%# ([*!\"+] = & !\"# $%# '!([*!\"+] -$(+) running average of rewards for action \" • Before time t, for some action assume n-1 rewards +#, … , +2%# received • ,2 = Average of +#, … , +2%# = '#3⋯3'&%# 2%# • After next reward ,23# =Average of +#, … , +2%#, +2 • ,23# = '#3⋯3'&%#3'& 2 = '#3⋯3'&%# 2 + '& 2 = 2%# 2 ,2 + '& 2 = 1 − # 2 ,2 + '& 2 = ,2 + # 2 (+2 − ,2) • New estimate ⟵ Old estimate + Step-size(Target – Old estimate) Bandit algorithm • Initialise: @ ! ← 0, L ! ← 0, for action ! = 1, … , I • Repeat: = ← Narg max ! @ ! , with probability 1 − B random action with probability B < ← [!\\]^E = L = ← L = + 1 @ = ← @ = + 1 L(=) [< − @ = ] Four-armed bandit Summary • Multi-armed bandits – simplification -- action, reward -- no input, no sequential decision making • Exploitation-exploration tradeoff -- B-greedy action choice • Learn action values – running averages for learning Reinforcement Learning: Markov decision process Srinandan Dasmahapatra Behaviourist Psychology: conditioning to stimulus: Pavlov/Skinner Direct/action-mediated (!) association or conditioning between environmental stimulus (\") and reward (ℛ) Classical Operant Markov decision processes (MDP) • Like bandits, agent receives evaluative feedback • Unlike bandits, choose different actions in different situations/states • Sequential decision making – actions (!) influence immediate rewards and future states (\") • Delayed rewards and immediate rewards – trade-off • In bandits, estimate #∗(!), whereas in MDP estimate #∗(\", !) • State-dependence crucial for long-term credit • Markov assumption: at each step, the next state and the reward depends only on current state and action taken, no memory Agent-environment interaction • Discrete time steps ! = 0,1,2, … • At time !, agent receives information about state (! of environment • Chooses action )! ∈ +(-) based on state • Receives reward /!\"# ∈ / ⊂ ℝ as consequence of action • Lands in new state (!\"# ∈ 2 • Trajectory: ($, )$, /#, (#, )#, /%, (%, )%, /&, (&, … • In finite MDP, sets of states, actions, rewards (2, +, ℛ) are finite • Dynamics of MDP defined by 4 -', 5 -, 6 = 7((! = -', /! = 5|(!(# = -, )!(# = 6) • Markovian dynamics: (!, /! depends only on (!(#, )!(#, no memory AgentState !! Reward \"! Action #! Environment MDP: flexible framework • Time steps: not just clock time: eg., stages of decisions / actions • Actions – low-level (turn wheel, increase voltage) or high-level (go shopping/do coursework), also mental (focus attention) • States – abstract (symbolic description) or concrete (sensor reading) • Environment considered external • Rewards experienced inside but emanate externally • Task to be undertaken by agent determines reward, hence external • Agent-environment boundary – limit of control, not of knowledge Recycling robot – collect empty cans • Reward, ℛ: mostly 0, small positive (can found), large negative (battery runs out when searching), rsearch > rwait rewards for finding can • State, ( ={high, low} • Action, * ={wait, search, recharge} Sutton, Barto, Reinforcement Learning. Transition graph: State node, Action node Policy: choice of action between states which receive rewards • Policy, step !, 9! 6 - = 7 )! = 6 (! = -) • Special case: deterministic policy – probability of action = 1 • Reinforcement learning – change policy as result of experience, maximise reward in the long run • Future rewards: :! = /!\"# + /!\"% + ⋯ + /) • Discounted future rewards: :! = /!\"# + =/!\"% + =%/!\"& + ⋯ , (0 ≤ = < 1) • Reward values are stochastic, hence expected rewards determine actions taken (policy): V!(s) = &! '\" (, *# = +] Optimal value functions – V and Q • Policy + better than +\" if expected reward ,# \" ≥ ,# \"(\") for all \" ∈ ( • Denoted + ≥ +′ • There is at least one policy better than all others – optimal policy • Optimal state-value function, ,∗: ,∗ \" = max$ ,# \" , for all \" ∈ ( • Optimal action-value function, !∗ : !∗ \", $ = max \" !\"(\", $), for all \" ∈ ,, $ ∈ - Learn Policy: learn map from states to probability of action based on rewards • Reinforcement learning – change policy as result of experience, maximise reward in the long run • Learn optimal action policy +∗: ( → * that maximises value V# s , expected discounted future rewards: V$(s) = &$ '% (, *& = +] = &$ -%'( + /-%') + /)-%'* + ⋯ (, *& = +] • Optimal policy: +∗ = arg max # ,#(\") for all \" ∈ ( • Unlike supervised learning, to learn +: ( → * we do not have examples of type (\", !); instead, we have ( \", ! , 9) Recursion, relate V and Q • Discounted future rewards: :% = ;%&' + =;%&( + =(;%&) + ⋯ , (1 ≤ = < 1) Ø:% = ;%&' + =(;%&( + =;%&) + =(;%&* + ⋯ ) Ø:% = ;%&' + =:%&' • ,# \" = B# :% C+ = \" = B#[;%&' + =:%&'|C+ = \"], ,# \" is state-value function for policy + • For specific action !% taken at time G from specific state \"% : Q# \", ! = B# ;%&' + =:%&' C% = \", I% = !], Q# \", ! is action-value function for policy + Recursion relation for V. \" = $. %/01 + '(/01 )/ = \"], • Prob. reach (!\"# = -′ and get reward /!\"# = 5 from (! = - by )! = 6 is 7 (!\"# = -', /!\"# = 5 (! = -, )! = 6) = 4(-', 5|-, 6) • Probability to choose the specific action (policy) )! = 6 from (! = - ∶ 9 )! = 6 (! = - = 9(-|6) • V* - = ∑+ 9 6 - ∑,' ∑- 4(-', 5|-, 6) 5 + =D*[:!\"# (!\"# = -′]] • V* - = ∑+ 9 6 - ∑,' ∑- 4(-', 5|-, 6) [5 + =G* -' ] • Bellman equation for V: expresses value of state in terms of values of successor states • But, in RL we do not know 9(-|6) or 4(-', 5|-, 6) + p a r S’ S Estimating !H \" : TD($) Updating values for random % ∈ ' legal moves • The update step of value for a given action (given policy 9) is weighted average of reward + discounted value at next step • Weights are based on unknown 4(-', 5|-, 6) • Value updates without weights and learning rate I ∈ (0,1]: G - ← G - + I [/ -' + =G -' − G(-)] • Difference between current and future time – temporal difference (TD) • Samples expected values needed for G* and uses estimates G - , G(-') • Example maze, 6 states, f terminal, a starting • Rewards: 0 everywhere except at f, where reward is 100. • Discounted rewards (0 < = < 1): hence, find shortest path (limited battery) a b c d e f 0 0 0 0 0 100 Example updates , = 0.5, ' = 0.1 • Initialise: for all \" ∈ ( = !, J, K, L, M, N , ,(\") = 0 • 3-step sequence of (state,reward) • (1, 0) ⇒ ( (4, 0) ⇒ ) (5, 0) ⇒ * (6, 100) terminate • 8 1 ← 8 1 + : - 4 + / 8 4 − 8 1 = 0 • 8 4 ← 8 4 + : - 5 + / 8 5 − 8 4 = 0 • 8 5 ← 8 5 + : - 6 + / 8 6 − 8 5 = 0 + 0.5 100 + 0 − 0 = 50 a b c d e f 0 0 0 0 0 100 0 0 0 0 50 0 0 0 0 0 0 0 0 0 0 0 0 0 V Example updates , = 0.5, ' = 0.1 • Initialise: for all \" ∈ ( = !, J, K, L, M, N , ,(\") = 0 • 3-step sequence of (state,reward) • (1, 0) ⇒ ( (4, 0) ⇒ ) (>, 0) ⇒ * (6, 100) terminate • 8 1 ← 8 1 + : - 4 + / 8 4 − 8 1 = 0 • 8 4 ← 8 4 + : - > + / 8 > − 8 4 = 0 • 8 > ← 8 > + : - 6 + / 8 6 − 8 > = 0 + 0.5 100 + 0 − 0 = 50 a b c d e f 0 0 0 0 0 100 0 0 50 0 50 0 0 0 0 0 50 0 0 0 0 0 50 0 V Example updates , = 0.5, ' = 0.1 • Initialise: for all \" ∈ ( = !, J, K, L, M, N , ,(\") = 0 • 3-step sequence of (state, reward) • (1, 0) ⇒ ( (4, 0) ⇒ ) (>, 0) ⇒ * (6, 100) terminate • 8 1 ← 8 1 + : - 4 + / 8 4 − 8 1 = 0 • 8 4 ← 8 4 + : - > + / 8 > − 8 4 = 0 + 0.5 0 + 0.1 50 − 0 = 2.5 • 8 > ← 8 > + : - 6 + / 8 6 − 8 > = 50 + 0.5 100 + 0 − 50 = 75 a b c d e f 0 0 0 0 0 100 0 2.5 75 0 50 0 0 0 50 0 50 0 0 0 50 0 50 0 V Rationale for temporal difference learning • ,# \" = B# :% C+ = \" = B# ;%&' + =:%&' C+ = \" • Expectation is population average, approximate by samples • B P = ' , (Q' + Q( + ⋯ + Q,), sample future steps in B#[:%&'] • Update ,# every time a transition occurs (learn from experience) • Step \" -∼#(-|1) \"′ [Sample of , \" ] = ; \"\" + =,(\"\") • For learning rate R, , \" ← 1 − R , \" + R[; \"\" + =, \"\" ] So far, any legal moves chosen at random; instead, choose action giving maximal expected rewards • However, we assumed we know what legal means • State transitions based on actions unknown • Need to acquire that knowledge • Not just value at state G(-) (estimate of G* (-)) • Keep track of state-action pair L(-, 6) estimate of Q* -, 6 • Q* -, 6 = D* /!\"# + =:!\"# (! = -, )! = 6], • Choose action 6∗ that maximises estimate L(-, 6): If - ⇒ +∗ -', L -, 6 ≤ L -, 6∗ = max +\" Q(s, 6′) • L -, 6 ← L -, 6 + I [/ -' + = max+' L -', 6' − L -, 6 ] Q-learning algorithm for estimating 1 ≈ 1∗ • Learning rate (step size) I ∈ 0,1 , small S > 0 • Initialise L -, 6 , for all - ∈ 2, 6 ∈ +, except L(terminal, s) = 0 ∀- • Loop for each episode: Initialise S Loop for each step of episode: Ø Choose A from S using policy derived from Q (e.g., S – greedy) Ø Take action A, observe R, S’ Ø L (, ) ← L (, ) + I / (' + = V6W + L (', 6 − L (, ) ( ← (′ Until S is terminal Example of updating Q(s, a) 72 100 8163 90 100 8163 3 4! = 6\", 8! = “:;<ℎ>” ← (1 − C)3 4! = 6\", 8! = “:;<ℎ>” + C[F!(6#) + G HIJ\"$ 3(4!%& = 6#, 8!%& = I$)] 3 4! = 6\", 8! = “:;<ℎ>” ← 1 − C 72 + C[0 + G HIJ\"!'{“*+,!”,“/012”,“3456!”} 3(4!%& = 6#, 8!%& = I$)] a b c d e f 0 0 100 0 0 0 3 4! = 6\", 8! = “:;<ℎ>” ← 1 − C 72 + C[0 + G HIJ {63, 81, 100}] = (1 − C) 72 + C G 100 For learning rate : = 1, discount factor / = 0.9, updated B *% = +,, C% = “EFGℎI” = 90 Your turn: check for convergence , = 1, ' = 0.9 81 72 72 a b c d e f 0 0 100 0 0 0 81 81 72 ? 81 3 4! = 6+, 8! = “TUV>” ← (1 − C)3 4! = 6+, 8! = “TUV>” + C[F!(6/) + G HIJ\"$ 3(4!%& = 6/, 8!%& = I$)] Summary: rational decision making • Maximise utility/rewards – the objective (what, not how) • MDP – state, action, reward • Bandits – action and reward • Unknown terrain -- Exploration vs exploitation (U-greedy) • Reinforcement learning – learn policy (state, action) transition • Value function – expected future rewards from every state, given policy, estimate via TD(V) • State-action function W(\", !) – estimate guides policy Bayesian Networks COMP2208 Srinandan Dasmahapatra A B A B Network representation of likely influence • Probability that !! takes any of its values \"!,#, \"!,$, … given that !# = \"#, !$ = \"$, … , !% = \"% is summarised in a conditional probability table, one for every combination of \"#, \"$, … , \"%. If !& binary, 2% such tables. '! '\" '# '$ '% Patterns of probabilistic reasoning • I’m not home, neighbour John calls to say my alarm is ringing, but neighbour Mary doesn't call. Sometimes it's set off by minor earthquakes. Is there a burglar? • During the dry season a sprinkler is sometimes used. I slip and fall on the pavement; is it a dry season? • Metastatic cancer potential cause of brain tumour, cause of increased serum calcium. Either explains patient falling into coma. Brain tumour can cause severe headaches. '! '\" '$ '&'' burglary earthquake alarm Mary callsJohn calls Example: Alarm network • I’m not home, neighbour John calls to say my alarm is ringing, but neighbour Mary doesn't call. Sometimes it's set off by minor earthquakes. Is there a burglar? '! '\" '$ '&'' burglary earthquake alarm Mary callsJohn calls (( )((() , 0.001 () )(()) - 0.002 (( () (* ) (* ((, ()) , - . 0.98 , ¬ - . 0.95 ¬ , - . 0.15 ¬ , ¬ - . 0.001 (* (+ )((+|(*) . 1 0.9 ¬ . 1 0.1 (* (, )((,|(*) . 2 0.6 ¬ . 2 0.05 !(#! = %|#\" = ', ## = ), #$ = ¬ +) Joint probability: !(#! = %, #% = ¬-, #\" = ', ## = ), #$ = ¬ +) (( )((() , 0.001 () )(()) - 0.002 (( () (* ) (* ((, ()) , - . 0.98 , ¬ - . 0.95 ¬ , - . 0.15 ¬ , ¬ - . 0.001 (* (+ )((+|(*) . 1 0.9 ¬ . 1 0.1 (* (, )((,|(*) . 2 0.6 ¬ . 2 0.05 ! %, ¬-, ', ), ¬+ = ! %)! ¬ - ! ' %, ¬- ! ) ' !(¬+|') '! '$ '& '\" '' From Bayesian network representation to joint distribution A B C P(A, B, C) 0 0 0 1 − ) (1 − +3)(1 − -3) 0 0 1 1 − ) (1 − +3)-3 0 1 0 1 − ) +3(1 − -#) 0 1 1 1 − ) +3-# 1 0 0 )(1 − +#)(1 − -3) 1 0 1 )(1 − +#)-3 1 1 0 )+# (1 − -#) 1 1 1 )+#-# B A P(B|A) 1 0 !! 1 1 !\" C B P(C|B) 1 0 -3 1 1 -# A P(A) 0 1 − ) A B C Exercise: choose numbers ', %&, %!, .&, .! and complete tables A B C P(A, B, C) 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 B A P(B|A) 1 0 1 1 C B P(C|B) 1 0 1 1 A P(A) 0 A B C Exercise: from previous table (from ', %&, %!, .&, .!) calculate A B C P(A, B, C) 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 B C P(B|C) 1 0 1 1 A B P(A|B) 1 0 1 1 C P(C) 0 A B C Marginal Conditional Exercise: from previous table (from ', %&, %!, .&, .!) calculate A B C P(A, B, C) 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 C B P(C|B) 1 0 1 1 A B P(A|B) 1 0 1 1 B P(B) 0 A B C Marginal Conditional Same joint distribution consistent with 3 Bayesian networks A B C P(A, B, C) 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 A B C Marginal Conditional A B C A B C Joint Common-cause/confounder Multiple cause/collider/selection Rain Sprinkler Grass Wet Assume grass will be wet if and only if it rained last night, or if the sprinklers were left on: Multiple causes in BN Compute probability it rained last night, given the evidence that the grass is wet (diagnostic reasoning) Rain Sprinkler Grass Wet Multiple causes in BN Compute probability it rained last night, given the evidence that the grass is wet: ! \" #) ! / 0 = ' 0 / '()) ∑!\"#\" ' 0 /,1, ' )\" '(-\") = '()) ' ) ' - .' ¬) ' - .' ) '(¬-) ! / 0 = !(/) ! 1 + ! / !(¬1) ≥ !(/) Rain Sprinkler Grass Wet Multiple causes: one cause among many is observed Compute probability it rained last night, given the evidence that the grass is wet and the sprinkler was left on: ! \" #, &) ! / 0, 1 = ' 0 /, 1 '()|-) '(1|-) = ! × '()) ! = !(/) ! / 0, 1 = ! / ≤ !(/|0) Observation of one cause reduces probability of another potential cause. Semantics of graphical models (Bayesian networks)ExerciseExerciseCorrelation is not causation. What about causation? • Causal analysis goes beyond fixed structures. • Observation not enough, need experiments • Manipulation of causal graph • Subject of a separate lecture 17. Planning Richard Watson (Inherited from Seth Bullock) 2 Planning • The planning problem • Planning with state-space search • Partial-order planning 3 What is Planning? • Generate and search over possible plans: sequences of actions to perform tasks and achieve objectives. – States, actions and goals • Classical planning environment: fully observable, deterministic, finite, static and discrete. • Assists humans in practical applications – design and manufacturing – military operations – games – space exploration 4 Difficulty of real world problems • Which actions are relevant? – Exhaustive search vs. backward search • What are good heuristic functions? – Good estimate of the cost of the state? – Problem-dependent vs. problem-independent • How to decompose the problem? – Most real-world problems are nearly decomposable. 5 Planning language • What is a good language? – Expressive enough to describe a wide variety of problems. – Restrictive enough to allow efficient algorithms to operate on it. – Planning algorithm should be able to take advantage of the logical structure of the problem. • STRIPS (Stanford Research Institute Problem Solver) - Fikes & Nilsson in 1971 - and ADL (Action description language) allows ‘unknowns’. 6 General language features • Representation of states – Decompose the world in logical conditions and represent a state as a conjunction of positive literals. • Propositional literals: Poor Ù Unknown • FO-literals (grounded and function-free): At(Plane1, Melbourne) Ù At(Plane2, Sydney) – Closed world assumption • Representation of goals – Partially specified state and represented as a conjunction of positive ground literals – A goal is satisfied if the state contains all literals in goal. 7 General language features • Representations of actions: Action = PRECOND + EFFECT – Action: Fly(p, from, to), – PRECOND: At(p, from) Ù Plane(p) Ù Airport(from) Ù Airport(to) – EFFECT: ¬At(p, from) Ù At(p, to)) • Action schema (“p”, “from”, “to” need to be instantiated) – Action name and parameter list – Precondition (conjunction of function-free literals) – Effect (conjunction of function-free literals) • Add-list vs. delete-list 8 Language semantics? • How do actions affect states? – An action is applicable in any state that satisfies its preconditions. – For first-order action schema, applicability involves a substitution q for the variables in the PRECOND. At(P1, JFK) Ù At(P2, SFO) Ù Plane(P1) Ù Plane(P2) Ù Airport(JFK) Ù Airport(SFO) Satisfies: At(p, from) Ù Plane(p) Ù Airport(from) Ù Airport(to) With q ={p/P1, from/JFK, to/SFO} Thus the action is applicable. 9 Language semantics? • The result of executing action a in state s is the state s’ – s’ is same as s except • Any positive literal P in the effect of a is added to s’ • Any negative literal ¬P is removed from s’ EFFECT: ¬At(p, from) Ù At(p, to): At(P1, SFO) Ù At(P2, SFO) Ù Plane(P1) Ù Plane(P2) Ù Airport(JFK) Ù Airport(SFO) – STRIPS assumption: (avoids representational frame problem) every literal NOT in the effect remains unchanged 10 Expressiveness and extensions • STRIPS is simplified: function-free literals – Allows for propositional representation – Function symbols lead to infinitely many states/actions • Extension: Action Description language (ADL) • Allows negative literals and disjunctions (or) Action(Fly(p:Plane, from:Airport, to:Airport), PRECOND: At(p, from) Ù (from ¹ to) EFFECT: ¬At(p, from) Ù At(p, to)) • Standardization : Planning Domain Definition Language (PDDL) 11 Example: air cargo transport Init(At(C1, SFO) Ù At(C2, JFK) Ù At(P1, SFO) Ù At(P2, JFK) Ù Cargo(C1) Ù Cargo(C2) Ù Plane(P1) Ù Plane(P2) Ù Airport(JFK) Ù Airport(SFO)) Goal(At(C1, JFK) Ù At(C2, SFO)) Action(Load(c, p, a) PRECOND: At(c, a) Ù At(p, a) Ù Cargo(c) Ù Plane(p) Ù Airport(a) EFFECT: ¬At(c, a) Ù In(c, p)) Action(Unload(c, p, a) PRECOND: In(c, p) Ù At(p, a) Ù Cargo(c) Ù Plane(p) Ù Airport(a) EFFECT: At(c, a) Ù ¬In(c, p)) Action(Fly(p, from, to) PRECOND: At(p, from) Ù Plane(p) Ù Airport(from) Ù Airport(to) EFFECT: ¬At(p, from) Ù At(p, to)) [Load(C1, P1, SFO), Fly(P1, SFO, JFK), Load(C2, P2, JFK), Fly(P2, JFK, SFO)] 12 Example: Spare tire problem Init(At(Flat, Axle) Ù At(Spare, trunk)) Goal(At(Spare, Axle)) Action(Remove(Spare, Trunk) PRECOND: At(Spare, Trunk) EFFECT: ¬At(Spare, Trunk) Ù At(Spare, Ground)) Action(Remove(Flat, Axle) PRECOND: At(Flat, Axle) EFFECT: ¬At(Flat, Axle) Ù At(Flat, Ground)) Action(PutOn(Spare, Axle) PRECOND: At(Spare, Ground) Ù ¬At(Flat, Axle) EFFECT: At(Spare, Axle) Ù ¬At(Spare, Ground)) Action(LeaveOvernight EFFECT: ¬At(Spare, Ground) Ù ¬At(Spare, Axle) Ù ¬At(Spare, trunk) Ù ¬At(Flat, Ground) Ù ¬At(Flat, Axle)) Beyond STRIPS: negative literal in pre-condition (ADL description) 13 Example: Blocks world Init(On(A, Table) Ù On(B, Table) Ù On(C, Table) Ù Block(A) Ù Block(B) Ù Block(C) Ù Clear(A) Ù Clear(B) Ù Clear(C)) Goal(On(A, B) Ù On(B, C)) Action(Move(b, x, y) PRECOND: On(b, x) Ù Clear(b) Ù Clear(y) Ù Block(b) Ù (b¹ x) Ù (b¹ y) Ù (x¹ y) EFFECT: On(b, y) Ù Clear(x) Ù ¬On(b, x) Ù ¬Clear(y)) Action(MoveToTable(b, x) PRECOND: On(b, x) Ù Clear(b) Ù Block(b) Ù (b¹ x) EFFECT: On(b, Table) Ù Clear(x) Ù ¬On(b, x)) 14 Planning with state-space search • Both forward and backward search possible • Progression planners – Forward state-space search – Consider the effect of all possible actions in a given state • Regression planners – Backward state-space search – To achieve a goal, what must have been true in the previous state. 15 Progression and regression 16 Progression algorithm • Formulation as state-space search problem: – Initial state = initial state of the planning problem • Literals not appearing are False – Possible Actions = those with satisfied preconditions • Add positive effects, delete negative – Goal test = does the state satisfy the goal – Step cost = each action costs 1 • No functions … any graph search that is complete is a complete planning algorithm, e.g., A* 17 Regression algorithm • How to determine predecessors? – Which states are immediately prior to the goal? Goal state = At(C1, B) Ù At(C2, B) Ù … Ù At(C20, B) Relevant action for first conjunct: Unload(C1, p, B) Works only if pre-conditions are satisfied. Previous state = In(C1, p) Ù At(p, B) Ù At(C2, B) … (Subgoal At(C1, B) should not be present in this state.) • Actions must not undo desired literals (consistent) • Main advantage: only relevant actions are considered. – Often much lower branching factor than forward search. 18 Regression algorithm • General process for predecessor construction – Give a goal description G – Let A be an action that is relevant and consistent – The predecessors is as follows: • Any positive effects of A that appear in G are deleted. • Each precondition literal of A is added , unless it already appears. • Any standard search algorithm can be added to perform the search. • Termination when predecessor satisfied by initial state. 19 Heuristics for state-space search • Neither progression nor regression are very efficient without a good heuristic. – How many actions are needed to achieve the goal? – Exact solution is NP hard, find a good estimate • Two approaches to find admissible heuristic: – The optimal solution to the relaxed problem. • Remove all preconditions from actions – The sub-goal independence assumption: The cost of solving a conjunction of sub-goals is approximated by the sum of the costs of solving the sub-problems independently. 22 Partial-order planning • Progression and regression planning are totally ordered plan search forms. – They cannot take advantage of problem decomposition. – Decisions must be made on how to sequence actions on all the subproblems • Least commitment strategy: – Delay choices (that don’t need to be made yet) during construction on the plan 23 Shoe example Init() Goal(RightShoeOn Ù LeftShoeOn) Action(RightShoe, PRECOND: RightSockOn, EFFECT: RightShoeOn) Action(RightSock, PRECOND: EFFECT: RightSockOn) Action(LeftShoe, PRECOND: LeftSockOn EFFECT: LeftShoeOn) Action(LeftSock, PRECOND: EFFECT: LeftSockOn) Planner: combine two action sequences (1) leftsock, leftshoe; (2) rightsock, rightshoe 24 Partial-order planning(POP) Any planning algorithm that can place two actions into a plan without fixing which comes first is a PO plan. Why search in POP space rather than FOP? • When sub-goals are semi-independent, the number of fully- ordered plans can be exponentially more than number of partially ordered plans. • Thus searching in space of POPs is much more efficient (much smaller search space) than searching in space of FOPs. (And, when they are not independent, its not significantly worse) • Secondarily – there is flexibility when executing the plan. 2526","libVersion":"0.3.2","langs":""}