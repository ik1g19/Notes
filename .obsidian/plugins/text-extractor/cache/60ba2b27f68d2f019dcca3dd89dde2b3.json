{"path":"Drive Sync/Lecture Slides/PLC-Connected.pdf","text":"Welcome COMP2212 - Programming Language Concepts Dr Julian Rathke 3 Welcome to Programming Language Concepts • The key aims of this course are for you to • Understand the various concepts that arise in different programming languages. • Use these concepts in a practical way to help you understand new programming languages. • To achieve these we’ll look at a variety of concepts from how languages are built to type systems to concurrency. • We’ll get to explore these ideas by designing our own domain specific language. 4 The Teaching Team • Dr Julian Rathke (that’s me!) is the module leader : I’ll be giving the lectures in the first half of the module jr2@ecs.soton.ac.uk • Dr Jian Shi (in the middle) will be giving lectures in the second half of the module Jian.Shi@soton.ac.uk • Dr Andrew Sogokon (on the right) will also be giving lectures in the second half of the module as1g29@ecs.soton.ac.uk 5 Administration • The teaching sessions are organised online only : • Each week there will be • Pre-recorded Lectures • Online tutorials on Thursdays 10am - 11am on Teams • Laboratory Help Sessions on Tuesdays 9am to 11am on Teams • The Laboratory Help sessions - we can provide feedback on your solutions to the weekly exercises that are to be set and help with Haskell programming issues. • The tutorial sessions discuss the course material covered in pre- recorded lectures and do some worked exercises. • You are expected to attend the timetabled tutorial sessions each week. 6 More administration • This module is assessed by a mixture of coursework (programming exercises) and examination. • There will be the following assessments : • Coursework : worth 40% of module total • First submission due : 22nd Apr 2021 • To be done in groups of three • Instructions for the coursework and its submission will be posted on the module website nearer the time. • The coursework will be a programming language design and implementation task using Haskell. You will also need to write a report on your language design. • The remaining 60% of the module total is provided by the examination at the end of Semester 2. This will be an online exam based on concepts and skills from the module. 7 So what topics are we studying? • Introduction to Programming Language Concepts • From Syntax to Execution - lexing and parsing • Type Systems • Semantics of Programming Languages • Concurrency in Programming Languages • Reasoning about concurrency • A look at modern programming languages • The module doesn’t follow any particular textbook but “Types and Programming Languages” by Pierce and “Java Concurrency in Practice” by Brian Goetz may be useful. Next Lecture Introduction to Programming Language Concepts COMP2212 PROGRAMMING LANGUAGE CONCEPTS Dr Julian Rathke INTRODUCTION REASONS FOR STUDYING PL CONCEPTS • Increased capacity for expressing ideas • different concepts of a programming language will cause you to solve problems in different ways • on the other hand, people ﬁnd it more difﬁcult to conceptualise structures for which there is no explicit language support • Be able to choose the right language for the job • different languages have different strengths and weaknesses REASONS FOR STUDYING PL CONCEPTS • Increased ability to learn new languages   • technology continues to change • knowing a wide range of languages will make it easier to master languages of the future • Better use of already known languages • understand parts of language that were mysterious • use techniques from other language in another APPLICATION DOMAINS • Scientiﬁc computing   • Fortran historically used for mathematical applications • Mathematica, MatLab  • Business applications • COBOL is still used - since 1959! • more recently, languages for business processes: BPEL, web services, etc. APPLICATION DOMAINS • Artiﬁcial Intelligence   • LISP and Scheme (functional languages) • logic programming (e.g. Prolog) • Machine Learning in Python, R, Julia  • Systems • C / C++, Rust • Web • markup: HTML, XHTML, CSS • scripting: Perl, PHP, Ruby, Javascript, … MAJOR PROGRAMMING LANGUAGE FAMILIES • Imperative Languages • C, C++, Java, C#, Pascal, Rust, Go, Swift • Declarative Languages • Functional Programming • OCaml, Haskell, ML, Lisp, Scheme, F#, Clojure • Logic Programming • Prolog and its variants • Domain Speciﬁc • HTML, Ant, SQL, XSLT, SOAP, etc .. MERGESORT IN PASCAL const INFTY = maxint; var a, b, c : array [1..10000] of integer; procedure merge (l, m, r : integer); var i, j, k : integer;  begin for i := l to m do b[i] := a[i];  i := l; b[m+1] := INFTY;  for j := m+1 to r do c[j] := a[j];  j := m+1; c[r+1] := INFTY; k := l; while (b[i]<INFTY) or (c[j]<INFTY) do if b[i]<c[j] then begin a[k] := b[i]; inc (i); inc(k); end else begin a[k] := c[j]; inc (j); inc(k); end; end; procedure mergesort (l, r : integer); var m : integer;  begin if l < r then begin m := (l+r) div 2; mergesort (l, m); mergesort (m+1, r); merge (l, m, r); end; end; MERGESORT IN OCAML open Future;; open List;; (* sort a list of integers l with parallelism *) let rec mergesort (l:int list) : int list = (* merge two sorted lists recursively *) let rec merge (l1,l2) = match (l1, l2) with ([], []) -> [] | (l, []) -> l | ([],l) -> l  | (hd1::tl1, hd2::tl2) -> if (hd1 < hd2) then hd1::merge (tl1,l2) else hd2::merge (l1,tl2) in (* split a list l in half *)  let split l : int list * int list = match l with [] -> ([], []) | hd::[] -> (l, [])  | _ -> let (l1, l2, _) = List.fold_left (fun (l1, l2, n) nxt ->  if (n > 0) then (nxt::l1, l2, n-1)  else (l1, nxt::l2, n-1)) ([],[],(List.length l)/2) l in (List.rev l1, List.rev l2) in (* perform mergesort with multiple threads *) match l with []->[]  | hd::[] -> [hd]  | _ -> let (l1, l2) = split l in let f = Future.future mergesort l1 in let l2' = mergesort l2 in  let l1' = Future.force f in  merge (l1', l2’) ;; MERGESORT IN PROLOG mergesort([],[]). /* covers special case */ mergesort([A],[A]). mergesort([A,B|R],S) :- split([A,B|R],L1,L2), mergesort(L1,S1), mergesort(L2,S2), merge(S1,S2,S). split([],[],[]). split([A],[A],[]). split([A,B|R],[A|Ra],[B|Rb]) :- split(R,Ra,Rb). merge(A,[],A). merge([],B,B). merge([A|Ra],[B|Rb],[A|M]) :- A =< B, merge(Ra,[B|Rb],M). merge([A|Ra],[B|Rb],[B|M]) :- A > B, merge([A|Ra],Rb,M) SOFTWARE DESIGN METHODOLOGIES • Data Oriented • Abstract Data Types (ADTs) • object-oriented design: encapsulates data together with code, concepts of class, object, inheritance, ... • Procedure Oriented • emphasises decomposing code into logically independent actions, often emphasised in concurrent programming CROSS-FERTILISATION • Functional programming concepts are used in imperative languages, e.g. Java lambdas, C#, Rust • Features such as monads allow functional programmers to write code in imperative style • Declarative code (e.g. regexp, SQL, HTML, ...) is sometimes allowed to be intermingled with ordinary code EVALUATION CRITERIA • A language ought to be • easy to write programs in, • result in readable code, • help the programmer to avoid bugs, • provide an appropriate level of abstraction, • make the code run fast, ... READABILITY • How easy is it to read and understand code? • Is more time spent on debugging others’ code rather than writing your own • Readability vs Convenience - this can be a trade-off • Obfuscation - • Simplicity can be deceiving: • Familiar things may seem “simpler” even when they actually are more convoluted • Feature multiplicity - incrementing a variable e.g. • Orthogonality - e.g. arrays in C, cannot be returned by functions, leads to confusion between pointer and array types etc. int composeAndApply(int (*x)(int), int (*y)(int), int arg) { return ((*y)((*x)(arg))); } i++ ABSTRACTION • The history of programming languages is a trend towards higher-level languages • Dijkstra’s “GOTO considered harmful” • Functions, modules • Classes, Objects • More abstraction means further separation from the machine • 1980s games programmers understood every single feature of their target hardware • Programmers don’t tend to understand the complexities of modern hardware today • Pipelining, memory models, multicore, multi-level cache • Smart compilers must come to the rescue here EFFICIENCY • Sometimes certain language features are difﬁcult to implement efﬁciently, or make it difﬁcult to proﬁle code execution • e.g. lazy evaluation in Haskell can make it difﬁcult to ﬁnd bottlenecks in evaluation • Good compilers will often do a better job of optimising your code than you could hope to • languages such as C have brilliant optimising compilers and libraries. A VERY BRIEF HISTORY OF PROGRAMMING LANGUAGES • Early 1950s: Introduction of Fortran, ﬁrst widely used high-level compiled programming language • IBM704 hardware with ﬂoating-point support • extremely primitive type system • Late 1950s: McCarthy at MIT and LISP - the ﬁrst functional programming language • Dominated in AI applications • Scheme, a LISP dialect is still popular • 1960s: ALGOL designed by committee. Led to BNF, procedural design, orthogonal design of language features (ALGOL68) • Inﬂuenced Pascal, Ada, Modula, C, Simula, Java etc A VERY BRIEF HISTORY OF PROGRAMMING LANGUAGES • 1960s: COBOL for business applications. Emphasises data processing as opposed to control ﬂow. • 1970s: C and Unix machines • 1970-80s: BASIC for microcomputers • 1970s: PROLOG - logic programming • 1970s: Ada, emphasised security and reliability. Exception handling. • 1980s: Milner and ML, everything is typed at compile time via a revolutionary type system. • 1980s: Smalltalk and Object-Oriented programming • 1990s: Java and the JVM - ﬂexibility and portability • 2000s: the rise of scripting languages - Perl, Python, PHP, Javascript, Ruby … WHAT’S NEXT FOR PROGRAMMING LANGUAGES? • Google are focussing on Big Data computing (map/ reduce) with language support for this (Google Go) • Component-based programming and frameworks - bridging the gap between software and hardware design • Libraries are hugely important but due a rethink - scalability, orthogonality issues • Concurrency is a major challenge to make it compatible with other language features. • Types for reliable, safe and secure code are becoming more prominent NEXT LECTURE: VARIABLES, NAMES AND BINDING COMP2212 PROGRAMMING LANGUAGE CONCEPTS Dr Julian Rathke VARIABLES, NAMES AND BINDING NAMES • Names are essential in programming languages - we use them to identify the virtual entities that we manipulate in programs • They are also referred to as identiﬁers • There are ad hoc design choices used in what is acceptable as a name • Some languages are case sensitive, others not • Some languages have restricted or ﬁxed length names • Some enforce lexical rules, e.g. must begin with an alpha, or must contain only alphanumerics • Sometimes names can clash with reserved words (keywords) in languages, some languages forbid this • It is not presently clear where a canonical choice for naming schemes would come from so we are perhaps stuck with these ad hoc choices for a while VARIABLES • The original meaning of variable is that of a memory location whose contents may change • i.e. a reference to a memory location • This meaning is now generalised to be “a placeholder for a value of some possibly complex type” • e.g. in functional languages variables can store closures of arbitrary higher- order types ((int → int) → int), say. • This is a semantic concept - not only a memory location • Where variables do refer to explicit memory locations, some languages allow you to obtain the location information • &var in C takes variables to pointers, the address of the variable in virtual memory. • In Java, you can use the Unsafe class (but really shouldn’t) • The term aliasing refers to two variables pointing to the same memory location. • This situation is generally wished to be avoided - why? VARIABLES • A variable typically has six attributes associated with it : • We’ll say more about these latter attributes in due course A name An address (aka an L-Value, i.e. the left hand side of an assignment) A value (aka an R-Value, i.e. the right hand side of an assignment) Type Extent Scope BINDING • A binding is an association between an entity and some attribute • e.g. between a variable and its type • or between a variable and its scope • Does a runtime system need to know about the type of a variable? • What may happen at runtime that necessitates this? • In general, some entities do not need to be bound at runtime and some entities must be • It is useful then to make the distinction between static and dynamic binding STATIC VS DYNAMIC BINDING • Static binding occurs before execution (compile time) and remains unchanged throughout execution • Dynamic binding ﬁrst occurs during execution (runtime) or changes during execution (runtime) ALLOCATION AND DEALLOCATION • One of a variable’s attributes is its address • We refer to the binding of a variable to its address as allocation • In complement, we refer to the unbinding of a variable to its address as deallocation • Allocation can be static (initialisation time) or dynamic (during runtime) • Deallocation is largely a dynamic concept • A variable’s extent is the time between its allocation and deallocation THE FOUR KINDS OF VARIABLES • Static Variables (aka global variables) • Bound to a memory location at initialisation time • e.g. Static class variables in Java are static variables • Stack-Dynamic Variables (aka local variables) • Memory is allocated from a runtime stack and bound when a declaration is executed and deallocated when the procedure block it is contained in returns. • e.g. Local variables in a method declaration THE FOUR KINDS OF VARIABLES • Explicit Heap-Dynamic Variables • Nameless abstract memory locations that are allocated/deallocated by explicit runtime commands by the programmer. • e.g. new/delete in C++, all objects in Java using new() • Implicit Heap-Dynamic Variables • Memory in heap is allocated when variables are assigned to values. It is deallocated and reallocated with re-assignment. Error prone and inefﬁcient. Used in ALGOL 68, LISP, C and JavaScript (for arrays) STATIC TYPE BINDING • Static type binding is typically done through explicit type declaration, or through type inference • Type Declaration is most commonly used. At the point at which a variable is introduced, it is done so with a type and possibly an initial value which it stores (e.g. used in Ada, Algol, Pascal, Cobol, C/C++, Java) • Type Inference does not require type annotation of variable declarations but the type is inferred from the usage of the variable or by following a ﬁxed naming scheme. • Primitive type inference (arguably another form of explicit declaration) - e.g. in Fortran I, J, K, L, M and N are Integer types, otherwise Real assumed. In Perl $p is a number or a string, @p an array, %p a hash • More sophisticated - Hindley-Milner inference introduced in ML has few annotations and the compiler deduces a most general type for a variable by its usage. The most general type is typically expressed using polymorphism or generics. DYNAMIC TYPE BINDING • Dynamic type binding typically occurs as a variable is assigned to a value at runtime. • A variable’s type (i.e. its type binding) can change during execution simply by assigning to it a value of a different type • This is commonly used in scripting languages such as JavaScript, Lua, Perl, PHP, Python, Ruby • Because type checking is done at runtime, this can have implications for efﬁciency (both time and space) • On the other hand, there may be advantages in readability and coding convenience. NEXT LECTURE: EXTENT AND SCOPE COMP2212 PROGRAMMING LANGUAGE CONCEPTS Dr Julian Rathke EXTENT AND SCOPE EXTENT • The extent of a variable (aka lifetime) refers to the periods of execution time for which it is bound to a particular location storing a meaningful value • It is a semantic concept and depends on the execution model • A running program may enter and leave a given extent many times, as in the case of a closure. • The different kinds of variables have different extent • Static variables have an extent of whole program execution • Stack-dynamic variables have an extent of a particular stack frame or procedure call • Explicit heap-dynamic variables have an extent from explicit allocation to explicit deallocation (cf. garbage collection and memory leak) • Implicit heap-dynamic variables have an extent from implicit allocation to implicit deallocation (values may persist in memory but addresses are freed) SCOPE • The scope of a variable is the part of the code in which it can be referenced. • It is the part of a program where a variable’s name is meaningful. • A variable’s scope affects its extent. A no longer reference-able value may be considered as a meaningless value. Garbage collectors are based on this principle. • Local variables are declared within a program block, the block is the scope of the variable • Static variables have whole program scope except where they are temporarily hidden by a locally scoped variable with the same name. • We refer to lexical scope where scope is aligned to statically determined areas of source code e.g. a class deﬁnition, a code block, or method body • It is a lexical concept, not a semantic concept DYNAMIC SCOPE ( NOT REALLY SCOPE ) • In contrast to lexical scope, some languages support dynamic scope for variables. • Dynamic scope is determined at runtime only as it depends on control ﬂow. Imagine a stack of value bindings for each variable that is updated with the control stack. • A variable is in a dynamic scope if its name is meaningful within the bindings of the current call stack. • This is uncommon in modern programming languages as it ﬂies in the face of referential transparency. • It is however used in Perl and Lisp • Here, y is lexically scoped and is local to first • But x is dynamically scoped and is still in scope when calling second() • If second() were called not via first() then x would not be in scope first(); sub first { local $x = 1; my $y=1; second(); } sub second { print \"x=\", $x, \"\\n\"; print \"y=\", $y, \"\\n\"; } Perl EXAMPLES OF SCOPE IN JAVASCRIPT // global scope var a = 1; function one() { alert(a); // alerts '1' } one() // prints ‘1’ //local scope function two(a) { alert(a); } two(2) // prints ‘2’ // Intermediate: no // such thing as // block scope in javascript function four() { if (true) { var a = 4; } alert(a); } four() // alerts ‘4’ function Five() { this.a = 5; } alert(new Five().a); // alerts ‘5’ var six = (function() { var a = 6; return function() { // JavaScript \"closure\" means I // have access to 'a' in here, // because it is defined in the // function in which I was // defined. alert(a); }; })(); six() // alerts '6' function Seven() { this.a = 7; } // [object].prototype.property loses to // [object].property in the lookup chain. For example... // Not reached, because 'a' is set in the constructor above. Seven.prototype.a = -1; // Is reached, even though 'b' is NOT set in the constructor. Seven.prototype.b = 8; alert(new Seven().a); // alerts '7' alert(new Seven().b); // alerts '8' // local scope again function three() { var a = 3; alert(a); } three() // alerts ‘3’ Output: 1,2,3,4,5,6,7,8 http://stackoverﬂow.com/questions/500431/javascript-variable-scope EXAMPLE OF SCOPE IN OCAML let add_polynom p1 p2 = let n1 = Array.length p1 and n2 = Array.length p2 in let result = Array.create(max n1 n2) 0 in for i = 0 to n1 - 1 do result.(i) <- p1.(i) done; for i = 0 to n2 - 1 do result.(i) <- result.(i) + p2.(i) done; result;; let rec fact n = n * fact (n - 1 ) in fact (10);; Parameters p1,p2 in scope in whole function body Local variables n1,n2 in scope after ‘in’ Recursive local variable fact in scope even before ‘in’ NEXT LECTURE: SYNTAX AND GRAMMARS COMP2212 PROGRAMMING LANGUAGE CONCEPTS Dr Julian Rathke SYNTAX AND GRAMMARS SYNTAX VS SEMANTICS • Syntax • This refers to the structure of statements in a program. • It typically follows a grammar based upon certain lexical symbols (e.g. keywords in a language) • Semantics • This refers to the meaning of programs and how programs execute. • The role of an interpreter or compiler of a language is to transform syntax in to semantics. LANGUAGE SYNTAX AS GRAMMARS • You have learned about regular languages and context free languages in COMP2210 • Most programming languages are context free languages expressible using a Context Free Grammar (CFG) • You have learned about Backus-Naur Form (BNF) for describing Context-Free Grammars • BNF is the de facto standard for deﬁning the grammar of a programming language. NON-TERMINALS VS TERMINALS • BNF acts as a metalanguage for deﬁning languages • It is a convenient meta-syntax for deﬁning the grammar of a language. • Non-terminals represent different states of determining whether a string is accepted by the grammar • In programming language terms these refers to the different kinds of expressions one may have in the language • e.g. class level declarations, method declarations, statements, expressions • Terminals represent the actual symbols that appear in the strings accepted by the grammar • These are sometimes called tokens or lexemes and refer to the reserved words, variable names and literals of our programs. EXAMPLE BNF GRAMMAR • The following is an example of a BNF grammar for a simple language of assignments <program> ::= begin <stmt_list> end <stmt_list> ::= <stmt> | <stmt> ; <stmt_list> <stmt> ::= skip | <assgn> <assgn> :: = <var> = <expr> <var> ::= X | Y | Z <expr> ::= <var> + <var> | <var> - <var> | <var> • The non-terminals are written as <follows> and the terminals are written in bold Example program: begin X = Y + Z ; skip ; Y = Z ; Z = Z - X end PARSE TREES • The legal programs of a language are those strings for which there is a derivation in the BNF grammar for the language. • A derivation of a string in a BNF grammar can be represented as a tree • At each node, the tree represents which rule of the grammar has been used to continue deriving the string • The child nodes represent the matches of the substrings according to the grammar • We call such trees parse trees AN EXAMPLE PARSE TREE • Consider the grammar • We can construct the following parse tree for “2 + 3 * 4” <expr> ::= <expr> + <expr> | <expr> * <expr> | <lit> <lit> ::= 1 | 2 | 3 | 4 | 5 | … <expr> <expr> + <expr> <lit> <lit> <expr> * <expr> <lit> 2 3 4 SYNTAX TO EXECUTION • So we can understand programs as a string of text, parsed as a tree according to some grammar, usually expressed in BNF. • But how do we ﬁnd the derivation of a string in a grammar? • Step 1 - Lexing : involves translating the particular symbols or characters in the string that make up the terminals of the grammar in to tokens. • Step 2 - Parsing : involves translating the sequence of tokens that make up the input string in to a tree. The parser must follow the rules of the grammar and build a tree representing the derivation. • In this latter step we often move away from parse trees and work with Abstract Syntax Trees (AST) ABSTRACT SYNTAX TREES • We learned in COMP2209 that abstract syntax trees are used to remove unnecessary detail regarding how a term was parsed. • I’ll present the two slides from those lectures that compare concete (parse) trees and abstract syntax trees again to remind ourselves. • Consider the grammar below PROG ::= EXP ; | while BOOLEXP do { PROG } | print EXP ; EXP ::= VAR | LIT | EXP OP EXP | ( EXP ) BOOLEXP ::= EXP < EXP | EXP == EXP | VAR ::= … Concrete Syntax Trees (aka Parse Trees) • Consider this program: • while (x < 4) do { print (x++) * 4 ; } • This can be seen as a Concrete Syntax Tree while-do < x 4 ( ) print ( ) * 4 x ++ ; But some of these nodes are unnecessary Let’s remove them: Abstract Syntax Trees • while (x < 4) do { print (x++) * 4 ; } LOOP LT x 4 PRINT MUL 4 x INC This tree retains the structure of the code but abstracts away the syntax used only to shape the tree. We call these Abstract Syntax Trees or ASTs These are the structures that compilers/ interpreters work with NEXT LECTURE: AMBIGUOUS GRAMMARS COMP2212 PROGRAMMING LANGUAGE CONCEPTS Dr Julian Rathke AMBIGUOUS GRAMMARS AN EXAMPLE PARSE TREE • Consider the grammar • Recall this parse tree for “2 + 3 * 4” <expr> ::= <expr> + <expr> | <expr> * <expr> | <lit> <lit> ::= 1 | 2 | 3 | 4 | 5 | … <expr> <lit> <expr> + <expr> <expr> * <expr> <lit> <lit> 2 3 4 <expr> ::= <expr> + <expr> | <expr> * <expr> | <lit> <lit> ::= 1 | 2 | 3 | 4 | 5 | … ANOTHER EXAMPLE PARSE TREE • The eagle-eyed among you would have spotted that • we can also construct a different parse tree for the same string “2 + 3 * 4” and grammar <expr> <expr> * <expr> <lit> <expr> + <expr> <lit><lit> 42 3 Is this a problem? AMBIGUOUS GRAMMARS • We say that a grammar G is ambiguous if there exists a string s for which there exist two or more different parse trees for s using the rules of G • Ambiguity in programming language grammars is generally considered a bad thing: • Two different parse trees for the same string of symbols implies two potentially different semantics for the same “program” • e.g. what does “2 + 3 * 4” evaluate to in the above language? RESOLVING AMBIGUOUS GRAMMARS • How do we remove ambiguity from a grammar? • We could just put parentheses everywhere • This is effective but impacts on readability (cf. Lisp) • We could use operator precedence • We can ask that one operator “binds tighter” than another operator, we say that the operator would have higher precedence • e.g. * binds more tightly than + so * has higher precedence than + • We understand “2 + 3 * 4” implicitly as “2 + (3*4)” • n.b. higher precedence operators will appear lower in the parse tree RESOLVING AMBIGUOUS GRAMMARS EXAMPLE • Consider how we might rewrite the previous grammar of + and * to resolve ambiguity <expr> ::= <mexpr> + <expr> | <mexpr> <mexpr> ::= <bexpr> * <mexpr> | <bexpr> <bexpr> ::= ( <expr> ) | <lit> <lit> ::= 1 | 2 | 3 | 4 | 5 | … • Here the level of the non-terminals determines precedence. • Parentheses are used to “reset” precedence. • Note how the string “2 + 3 * 4” now has a unique parse tree. ASSOCIATIVITY • Using the previous grammar for + and * consider the string “2 + 3 + 4” <expr> ::= <mexpr> + <expr> | <mexpr> <mexpr> ::= <bexpr> * <mexpr> | <bexpr> <bexpr> ::= ( <expr> ) | <lit> <lit> ::= 1 | 2 | 3 | 4 | 5 | … • The operator + has the same precedence as itself so how is ambiguity resolved? • Following the above grammar this string is implicitly derived as “2 + 3 + 4” • This is known as being right associative CHANGING ASSOCIATIVITY • Does associativity matter? • 2 + (3 + 4) means the same as (2+3) + 4 anyway. • But 2 - (3 - 4) is not the same as (2 - 3) - 4 ! • We’ve seen how to guarantee right associativity, so how would we guarantee left associativity instead? <expr> ::= <expr> + <mexpr> | <mexpr> <mexpr> ::= <mexpr> * <bexpr> | <bexpr> <bexpr> ::= ( <expr> ) | <lit> <lit> ::= 1 | 2 | 3 | 4 | 5 | … Notice the change in order! Be careful with this approach - left recursive grammars don’t work well with recursive descent (more on this later). THE DANGLING ELSE PROBLEM • In many programming languages we can write an “if- then” statement without an “else” branch. • A grammar for this might contain … <ifstmt> ::= if <expr> then <stmt> else <stmt> | if <expr> then <stmt> … if true then if false then skip else loop • Does the following program loop or terminate? Which “if” does the “else” correspond to? This grammar is ambiguous HOW DO REAL LANGUAGES RESOLVE A DANGLING ELSE? • You can see a solution for dangling else in the Java grammar • Additional non-terminals are used to determine a precedence that a nested conditional in a “then” branch cannot use a single branch conditional. IfThenStatement: if ( Expression ) Statement   IfThenElseStatement: if ( Expression ) StatementNoShortIf else Statement    IfThenElseStatementNoShortIf: if ( Expression ) StatementNoShortIf else StatementNoShortIf http://docs.oracle.com/javase/specs/ • The program on the previous slide would loop when understood as a Java program. NEXT LECTURE: LEXING CONCEPTS COMP2212 PROGRAMMING LANGUAGE CONCEPTS Dr Julian Rathke LEXING CONCEPTS BASIC CONCEPTS • We’ve learned that lexical analysis or lexing is the process of converting a source input string in to a sequence of tokens • A lexeme is a pattern of characters in the input string that are considered meaningful in the given programming language • these may be deﬁned by regular expressions • A token is a lexeme that has been identiﬁed and “tagged” with a meaning and possibly including a value • e.g. while is a lexeme from the characters ‘w’, ‘h’, ’i’, ’l’, ’e’ identiﬁed as the while-command token. Or true is a boolean token with value “true”. SCANNING AND EVALUATION • Lexemes are identiﬁed use a scanner that does the pattern matching using “maximal munch” • For example, for a pattern [1-9]+ and the input “ … 456,234 …” • there are lexemes “456” and “234” matching that pattern • For the pattern [1-9]+,[1-9]+ we would have “456,234” as a lexeme • Tokens are created using an evaluator whose job it is to analyse the lexemes, tag them appropriately and identify any associated value. • For example, for a pattern [1-9]+ and the input “ … 42 … ”, as above we have the lexeme “42” and the evaluator would identify this as a “integer” token with value being the actual integer 42. • In Haskell, the function read is very useful for evaluation HOW TO WRITE A LEXER : DON’T • In order to write a lexer then we need to have code that Scans the input string String [ String ] Evaluates the input string [ Token ] • This code can be reasonably generic if • We know what the lexemes look like • Know which lexemes correspond to which tokens and how to associate values with them • If we have a means of describing these things then we could just automatically generate the code to do the scanning and evaluating • This is great for code re-use leading to productivity gains and robustness of code LEXER GENERATORS • A lexer generator is a software tool that, given an input that describes the lexemes and what tokens they produce, will generate code for you that performs lexical analysis. • Lexer generators are widely and freely available for most programming languages • The majority of them are based around the C/Unix based tools called lex and the variant of this called ﬂex. • e.g, JLex is a Java version of lex, ply.lex is a Python version etc • The input languages for these versions are mostly similar to lex so learning to use one version will make it easy to learn a version in another language also. • We will be using the Haskell version of lex, which is called “Alex” NEXT LECTURE: LEXING IN HASKELL COMP2212 PROGRAMMING LANGUAGE CONCEPTS Dr Julian Rathke LEXING IN HASKELL THE ALEX TOOL • The Alex tool is a code generation tool for automatically generating lexers in Haskell. • The user provides an Alex speciﬁcation • i.e. a list of lexemes and a tokenisation action for each lexeme • Alex generates a Haskell function named alexScan that does the job of a scanner but also identiﬁes tokens and actions to be taken • Alex is parametrisable in the way it scans and evaluates - in order to customise it you can provide implementations for the following • type AlexInput • alexGetByte :: AlexInput ! Maybe (Word8, AlexInput) • alexInputPrevChar :: AlexInput ! Char • And provide an evaluator function alexScanTokens that does the evaluation • This may seem a little complicated but fortunately there is a basic “wrapper” that provides default implementations of these for off-the-shelf use THE BASIC WRAPPER • This is a simple way of getting a function String → [ Token ] type AlexInput = (Char, [Byte], String) -- previous char -- rest of the bytes for the current char -- rest of the input string alexGetByte :: AlexInput -> Maybe (Byte,AlexInput) alexGetByte (c,(b:bs),s) = Just (b,(c,bs,s)) alexGetByte (c,[],[]) = Nothing alexGetByte (_,[],(c:s)) = case utf8Encode c of (b:bs) -> Just (b, (c, bs, s)) alexInputPrevChar :: AlexInput -> Char alexInputPrevChar (c,_,_) = c alexScanTokens :: String -> [Token] alexScanTokens str = go ('\\n',[],str) where go inp@(_,_bs,str) = case alexScan inp 0 of AlexEOF -> [] AlexError _ -> error \"lexical error\" AlexSkip inp' len -> go inp' AlexToken inp' len act -> act (take len str) : go inp' This is all coded for you Note the type of actions here : String → Token ANATOMY OF AN ALEX FILE { module Lexer where } %wrapper “basic\" $digit = 0-9 $alpha = [a-zA-Z] tokens :- $white+ ; \"--\".* ; let { \\s -> Let } in { \\s -> In } $digit+ { \\s -> Int (read s) } [\\=\\+\\-\\*\\/\\(\\)] { \\s -> Sym (head s) } $alpha [$alpha $digit \\_ \\’]* { \\s -> Var s } . . . . . . { -- Each action has type :: String -> Token -- The token type: data Token = Let | In | Sym Char | Var String | Int Int deriving (Eq,Show) } Pre-amble: Haskell code copied directly to the output Optional Wrapper declaration - this just says, use the basic wrapper Optional Macro deﬁnitions Optional Delimiter :- to begin rules the name ‘tokens’ is irrelevant Lexemes to be matched Along with corresponding actions for the evaluator Post-amble: Haskell code copied directly to the output. The datatype Token is usually deﬁned here Optional WRAPPERS • There are other pre-deﬁned wrappers available: posn, monad, monadUserState, and ByteString wrappers • You are unlikely to need any of these other than ‘posn’ • The posn wrapper keeps track of line and column numbers of tokens in the input text. data AlexPosn = AlexPn !Int -- absolute character offset !Int -- line number !Int -- column number type AlexInput = (AlexPosn, -- current position, Char, -- previous char [Byte], -- rest of the bytes for the current char String) -- current input string alexScanTokens :: String -> [Token] alexScanTokens str = go (alexStartPos,'\\n',[],str) where go inp@(pos,_,_,str) = case alexScan inp 0 of AlexEOF -> [] AlexError ((AlexPn _ line column),_,_,_) -> error $ \"lexical error at \" ++ (show line) ++ \" line, \" ++ (show column) ++ \" column\" AlexSkip inp' len -> go inp' AlexToken inp' len act -> act pos (take len str) : go inp' Again, note the type of actions: AlexPosn → String → Token NEXT LECTURE: PARSING CONCEPTS COMP2212 PROGRAMMING LANGUAGE CONCEPTS Dr Julian Rathke PARSING CONCEPTS BASIC CONCEPTS • We have learned that parsing is the process of converting a stream of lexical tokens in to an abstract syntax tree representation of a programme by matching it against a grammar for the language. • Parsing is done after lexical analysis in the compiler/interpreter tool chain. • There are many different approaches to parsing, two common ones are • top down or recursive descent • builds parse tree from the root down • LL Grammars (reads Left to Right, forms Leftmost derivations) • bottom up • builds parse tree from leaves up • LR Parsers (reads Left to Right, forms Rightmost derivations) • More complicated but handles a wider range of grammars than LL parsing SHIFT AND REDUCE • The most common approach to implementing LR parsers is to use shift-reduce parsing • Here, the token stream is scanned and a forest of partial parsing subtrees are built from the bottom up. • At each stage the parser can either • Shift → read the next input token (a new single node parse subtree) • Reduce → apply a completed grammar rule to join subtrees together in to a larger subtree • Continues until input empty and a single parse tree is formed • These act like state machines with actions ERROR, SHIFT, REDUCE, STOP • Conﬂicts occurs when the state machine is non-deterministic, i.e. two or more actions can apply - more on these later HOW TO WRITE A PARSER : DON’T (ISH) • In order to write a parser then we need to have code that Scans the token stream Next, Rest Matches Grammar until empty [ Token ] • This code can be reasonably generic if • We know what the tokens look like • We know what the grammar is and which sequences of tokens corresponding to which parse (sub)trees • Again, if we could specify these things then the actual parsing code that creates a state machine based on the grammar and scans the input can be automatically generated. • Again, this is great for code re-use leading to productivity gains and robustness of code. Builds Parse Tree PARSER GENERATORS • A parser generator is a software tool that, given an input that describes the rules of your grammar and what actions to perform when each rule is used, will generate code for you that performs parsing. • Just as for lexing, there are a number of widely and freely available tools that will generate parsing code for us. • The range of these tools is wider than for lexing owing to the wider variation in parsing algorithms. • e.g. • yacc is the C/Unix based tool that pairs nicely with lex • Bison is the variant of yacc that pairs with ﬂex • yacc and Bison are both LALR (Lookahead LR parsers) • ANTLR4 is a lexer and parser generator combined, it is an adaptive LL parser • JavaCC is also a lexer/parser generator and uses the LL(k) algorithm • We’ll be using a yacc variant for Haskell called Happy NEXT LECTURE: PARSING IN HASKELL COMP2212 PROGRAMMING LANGUAGE CONCEPTS Dr Julian Rathke PARSING IN HASKELL THE HAPPY TOOL • The tool Happy is Haskell’s version of the well-known yacc tool for automatically generating parsing code for a given input grammar • The user provides a Happy speciﬁcation • i.e. a list of tokens and grammar rules • The rules determine valid patterns of tokens along with actions to perform for each pattern • Happy will generate Haskell code with a function named of your choosing of type [ Token ] → T where Token and T are both user-deﬁnable ANATOMY OF A HAPPY FILE { module Grammar where } %name parseCalc %tokentype { Token } %error { parseError } %token let { TokenLet } in { TokenIn } int { TokenInt $$ } var { TokenVar $$ } '=' { TokenEq } '+' { TokenPlus } '-' { TokenMinus } '*' { TokenTimes } '/' { TokenDiv } '(' { TokenLParen } ')' { TokenRParen } … %% Exp : let var '=' Exp in Exp { Let $2 $4 $6 } | Exp1 { Exp1 $1 } Exp1 : Exp1 '+' Term { Plus $1 $3 } | Exp1 '-' Term { Minus $1 $3 } | Term { Term $1 } Term : Term '*' Factor { Times $1 $3 } | Term '/' Factor { Div $1 $3 } | Factor { Factor $1 } Factor : int { Int $1 } | var { Var $1 } | '(' Exp ')' { Brack $2 } { parseError :: [Token] -> a parseError _ = error \"Parse error\" data Exp = Let String Exp Exp | Exp1 Exp1 deriving Show data Exp1 = Plus Exp1 Term | Minus Exp1 Term | Term Term deriving Show data Term = Times Term Factor | Div Term Factor | Factor Factor deriving Show data Factor = Int Int | Var String | Brack Exp deriving Show } Pre-amble: Haskell code copied directly to the output Name Directive - the name of the generated parse function Tokens directive - gives names to the Token values seen in the input stream - it’s best if these names match those you have used in Alex but they don’t have to. Post-amble: Haskell code copied directly to the output. The datatype of ASTs is usually deﬁned here Token Type Directive - the type of the tokens Error Directive - the name of the function to call if a parse error is encountered The productions of the grammar - see next slide %% is a necessary delimiterTHE GRAMMAR • The grammar section is the bit after the %% delimiter • It includes a number of productions - these are rules of how to rewrite a non-terminal by matching against terminals (patterns of tokens) and other non-terminals • e.g. the example on the previous slide has four productions for the non- terminals Exp, Exp1, Term and Factor • The general form of a production is • where each rule describes a pattern of tokens and non-terminals and each action is Haskell code that describes what should be done when that particular grammar rule is met. • The typical use of actions is to build an abstract syntax tree • This is what the actions on the previous slide do <non-terminal> : rule1 { action1 } | rule2 { action2 } … | ruleN { actionN } actions are Haskell expressions with occurrences of $n to represent the nth matched value (indexed from 1) ENTRY POINT TO THE GRAMMAR • You will have noticed in the previous example that there are production rules for four different non-terminals : Exp, Exp1, Term and Factor • Suppose I have a list of Tokens that I feed to the parse function generated by Happy? Which non-terminals does the parser choose to try match against? • Parsers generated by Happy have, by default, a single entry point - this means that the parser will try to match against a single speciﬁc non- terminal. • This is determined by the order the non-terminals are listed in the ﬁle - in particular, the ﬁrst non-terminal listed is the entry point to the grammar. DIRECTIVES • There are a number of different directives that you can give Happy • The main ones you are likely to use are • The name directive %name - this speciﬁes the name of the generated parse function • The token type directive %tokentype - this speciﬁes the type of the tokens on the input stream. You should have deﬁned this type in your lexer input ﬁle so expect to import your lexing module. • The error directive %error - this speciﬁes the name of a function of your choosing that will be called when a parsing error occurs. This allows you to customise the error message. • The type of your error function should be [ Token ] ! a TOKEN DIRECTIVES • The tokens themselves are speciﬁed using the %token directive • The tokens can be given a convenient name for use as terminals in the grammar • Use $$ to represent the value of a token matched in the grammar. e.g. • Use _ to act as a wildcard for matching arguments to tokens that you don’t need to further use. • e.g. the following could be used for matching of a boolean Token that carried either a true or false Haskell value %token let {TokenLet } in {TokenIn } int {TokenInt $$ } var {TokenVar $$ } … %token bool {TokenBool _ } … Any match for int in the grammar refers to the value carried by the TokenInt and not the TokenInt itself. RETURNING OTHER DATATYPES • The return type of grammar actions does not have to be an AST - you can return any Haskell value • You can even evaluate as you parse: e.g. • This does not construct an AST but instead calculates an Integral value • Note that all actions must have the same type • Note the indexing used to select the correct value from the matched token … %% Exp : Exp '+' Exp1 { $1 + $3 } | Exp '-' Exp1 { $1 - $3 } | Exp1 { $1 } Exp1 : Exp1 '*' Factor { $1 * $3 } | Exp1 '/' Factor { $1 `div` $3 } | Factor { $1 } Factor : int { $1 } | '-' Factor { (0 - $2) } | '(' Exp ')' { $2 } … AMBIGUITIES AND SHIFT/REDUCE CONFLICT • Suppose we wrote a simple grammar such as • This grammar is ambiguous : • When matching “1 + 2 + 3” after reading “ 1 + 2 ” the parser could either • Shift and read the next token “+” or • Reduce and complete the production Exp → Exp + Exp • We haven’t told the parser what to do • This is what is known as a shift/reduce conﬂict • The grammar on the previous slide avoids this by layering the non- terminals. • “1 + 2 + 3” must reduce after “1 + 2” as the input cannot be parsed as “1 + (2 + 3)” because the production Exp → Exp + Exp1 doesn’t allow this • Another way of removing shift/reduce conﬂicts is to specify precedence for operators using directives. Exp : Exp '+' Exp { $1 + $3 } | Exp '-' Exp { $1 - $3 } | Exp ‘*’ Exp { $1 * $3 } | Exp ‘/’ Exp { $1 / $3 } | int { $1 } PRECEDENCE • We can use directives %left and %right to specify associativity of operators. • Moreover, the order in which we use these determines tightness of binding • e.g. for the previous example we can write to say that addition and subtraction are left associative, multiplication and division are right associative and * and / bind more tightly than + and - • So we would have “ 1 + 2 - 3 ” parsed as “(1 + 2) - 3” by left associativity • And we would have “1 + 2 * 3” parsed as “1 + (2 * 3)” because the precedence directive for “*” appears later than that for “+” • We can also use %nonassoc where operators cannot be used associatively • e.g. we may want to disallow “1 > 2 > 3” as an expression so we would declare ‘>’ as non-associative %left ‘+’ ‘-’ %right ‘*’ ‘/’ %% Exp : Exp '+' Exp { $1 + $3 } | Exp '-' Exp { $1 - $3 } | Exp ‘*’ Exp { $1 * $3 } | Exp ‘/’ Exp { $1 / $3 } | int { $1 } CONTEXT DEPENDENT PRECEDENCE • Occasionally you can’t associate a precedence to a particular token as it may depend on its use in context • To allow for this you can create a fake token, assign a precedence to that and then allow a rule to inherit the precedence from the fake token • This is best shown by example: … %right in %left '+' '-' %left '*' '/' %left NEG %% Exp : let var '=' Exp in Exp { Let $2 $4 $6 } | Exp '+' Exp { Plus $1 $3 } | Exp '-' Exp { Minus $1 $3 } | Exp '*' Exp { Times $1 $3 } | Exp '/' Exp { Div $1 $3 } | '(' Exp ')' { $2 } | '-' Exp %prec NEG { Negate $2 } | int { Int $1 } | var { Var $1 } … This rule has precedence inherited from the fake token NEG Creates a fake token with left associativity and tight binding So - 5 * 3 + 1 is parsed as ( (-5) * 3 ) + 1 NEXT LECTURE: INTRODUCTION TO TYPES COMP2212 PROGRAMMING LANGUAGE CONCEPTS Dr Julian Rathke INTRODUCTION TO TYPE SYSTEMS WHEN PROGRAMS GO RIGHT A major concern of Software Engineers should be the correctness of the software that they produce. However, correctness can mean many different things: Software Testing For a lot of Software Engineers the issue of correctness stops here. Model Checking Theorem Provers Denotational Semantics Hoare Logics Formal Methods Algebraic Speciﬁcation Static Analysis Runtime Monitoring For a lot of Software Engineers these techniques are considered too heavyweight in practice Type Systems We all use these already! Lightweight formal methods. WHAT IS A TYPE SYSTEM? “A type system is a tractable syntactic method for proving the absence of certain program behaviours by classifying (program) phrases according to the kinds of values they compute” Benjamin Pierce So what are types? They are the particular classiﬁcations of program behaviours that one makes. For example, in simple type systems for C functions we classify programs according to the number of and primitive type of the arguments along with the type of the result of the function. Types are abstract descriptions of programs We can study the correctness properties of interest by abstracting away all of the low-level details Types are precise descriptions of program behaviours We can use mathematical tools to formalise and check these interesting properties WHAT DO TYPES DO FOR US? • We mentioned above that Types Systems are concerned with correctness and that we use type systems to guarantee the absence of certain behaviours: e.g. • application of an arithmetic expression to the wrong kind of data • existence of a method or ﬁeld when invoked on a particular object • array lookups of the correct dimension • Type Systems can also be used to enforce higher-level modularity properties • maintain integrity of data abstractions • check for violation of information hiding • In doing this, Type Systems enforce disciplined programming • type systems form the backbone of module based languages for large-scale composition • types are the interfaces between the modules • encourages abstract design • types also are a form of documentation HOW DO WE USE TYPES? Most programming languages use at least some notion of types for programs or the data that programs manipulate. But there are several different approaches to using types. • Strongly Typed • Languages check use of data to prevent program errors e.g. accessing private data, from corrupting memory, from crashing the machine etc • Weakly Typed • Languages do not always prevent errors but use types for other compilation purposes such as memory layout. • Static Typing • Refers to the point at which type checking occurs - compile time. • Dynamic Typing • Refers to type checking that is delayed until run time. Strong / Weak typing are opposite approaches. Static / Dynamic typing are opposites approaches. The Strong/Weak and Static/Dynamic approaches are independent. E.g. we can say a language has Strong Static Typing “The internal SRI software exception was caused during execution of a data conversion from 64-bit ﬂoating point to 16-bit signed integer value. The ﬂoating point number which was converted had a value greater than what could be represented by a 16-bit signed integer. This resulted in an Operand Error. “ (from the Ariane 501 Inquiry Board Report) • Liskov and Zilles described “Strong” typing by the requirement that “whenever an object is passed from a calling function to a called function, its type must be compatible with the type declared in the called function.” • This ‘deﬁnition’ generalises to the same requirement on any consumer of data. • Let’s consider a language to have “Weak” typing otherwise. • One implication of Strong Typing is that intended types must be declared or inferred with functions/methods. • This can make languages verbose - Java suffers from this • An implication of Weak Typing is that data of the ‘wrong’ type may be passed to a function - and the function is free to choose how to behave in that case. • Typically, weakly typed languages attempt to implicitly coerce data to be of the required type. This can go disastrously wrong : Software Engineers favourite example is Ariane 5. STRONG VS WEAK TYPINGSTATIC VS DYNAMIC TYPING • Statically typed languages necessarily use an approximation of the run time types of values. • Why ? Because statically determining control ﬂow is undecidable (in sufﬁciently rich languages). • Compile time checking can avoid costly run time errors though. • Where types are used for memory layout, static typing is appropriate (e.g. C) • Dynamically typed languages check the types of data at point of use in run time. • Exact types can be used. This implies no false negative type errors. • It is quite common to allow variables to change the type of data they store, or objects to dynamically grow new methods. Some programmers ﬁnd this convenient. • Very common in scripting languages and web programming. • Should not be used for Critical Systems as errors may be detected too late. LANGUAGES AND THEIR TYPES Weak Strong Dynamic PHP, Perl, Javascript, Lua Python, Lisp, Scheme Static C, C++ Ada, OCaml, Haskell, Java, C# sort of Visual Basic (.NET) ?!??!!? Future programming languages are likely to feature controllable Dynamic / Statically typed regions of code - these are likely to be able to interact in safe ways. WHEN DO WE CHECK TYPES? • Type checking is performed as part of compilation for statically typed languages • in the compiler front end - type checking is done on abstract syntax tree of the program and is sometimes referred to as “semantic” analysis • Most strongly typed languages insist that all program variables are manifestly typed, some (e.g. OCaml, Haskell) allow type inference so that the compiler automatically works out suitable types. • So this means that there is a part of the implementation of the compiler that guarantees a certain amount of correctness with respect to programs that pass its type checker. • This is some sort of algorithm coded up in a high-level language, or even in C. • How do we know we can trust this type checking algorithm? Do we really know that it gives us what is called type safety? Type safety, for strong statically typed languages, can be loosely described with the quote: “Well-typed programs never go wrong” (Robin Milner) WHAT DOES IT TAKE TO PROVE TYPE SAFETY? • You need to know what “well-typed” means - precisely • You need to know what “programs” means - precisely • You need to know what “never go” means - precisely • You need to know what “wrong” means - precisely “Well-typed programs never go wrong” • We give an inductively deﬁned typing relation - a bit like a proof in propositional logic • We give an inductively deﬁned reduction relation - i.e. how the program runs • We give a description of the error states of the program - these vary widely • We use the mathematical descriptions to try prove that type safety holds for a given language This is the where the mathematics comes in! This is of course all very language dependent - the larger the language the more work there is WHAT WILL WE DO IN THE NEXT FEW WEEKS • We’ll focus on small example languages to give you an introduction to the techniques. • We’ll learn how to deﬁne operational semantics for a language • We’ll learn how to design and deﬁne a type system for a language • We’ll learn how to relate the two mathematically How does the mathematics correspond to the code in the compiler that does the type checking? • We’ll learn how to write an interpreter in Haskell that implements our operational semantics • We’ll learn how to write a type checker in Haskell that implements our type system • We’ll learn how to do this for a variety of language features NEXT LECTURE: TYPE DERIVATION RULES COMP2212 PROGRAMMING LANGUAGE CONCEPTS Dr Julian Rathke TYPING RULES HOW DO WE KNOW WHETHER CODE IS WELL-TYPED? • Most typed languages have some sort of annotation to indicate the type of data. E.g. • C asks variables to be declared with a (weak) type so that the compiler can allocate enough memory to store values of that type. • Java asks for methods to declare the type of their input parameters as well as their return value • The way that the compiler makes use of these annotations determines whether the type system is strong/weak. • We’ll talk about strong (static) type systems for the remainder of this lecture. • The compiler needs to check that, at any point in a program’s execution, the program doesn’t attempt to consume data of the wrong type. • Certain operators of each programming language consume data. So the compiler needs to ﬁnd uses of these operators. • Clearly then, in order to check well-typedness of programs, this will amount to checks on abstract syntax trees according to the types of the data being used. A TOY LANGUAGE Let’s play a little game with ASTs for a Toy language: T , U ::= Int | Bool E ::= n | true | false | E < E | E + E | x | | if E then E else E | let (x : T) = E in E where n ranges over natural numbers and x ranges over some set of variable names. Consider the following example and its AST let (x : Int) = if (y < 3) then 0 else (y + 1) in x + 42 let - - in - x:Int if - then - else - + < 0 + x 42 y 3 y 1 ABSTRACT ABSTRACT SYNTAX TREES Let’s rewrite the previous example with Types in place of the values and value expressions! e.g. let ( Int ) = if ( Int < Int ) then Int else Int + Int in Int + Int Now, one way of checking types is to traverse this type abstracted AST and see whether the operators that consume data are given data of the correct type. The operators that consume data are : if-then-else (this reads a Bool), < consumes two Ints and + consumes two Ints let - - in - x:Int if - then - else - + < 0 + x 42 y 3 y 1 We can track the types during computation also! COMPLICATE THE EXAMPLE let (x : Int) = if ( if ( y < 0) then false else true ) then 0 else (y + 1) in x + 42 Let’s make it more fun Every subtree in the AST needs to have a type ! Every program fragment needs to have a type ! Int Bool let - - in - x : Int if - then - else - + Int Int Int Bool Bool if - then - else - COMPLICATE AGAIN let (x : Int) = if ( if ( 1 < 0) then 0 else false ) then 0 else 1 in x + 42 So is this program well typed? In a dynamically typed language you might say so. What about a static type? Bool let - - in - x : Int if - then - else - + Int Int Int Int Int Bool if - then - else - So what is the type of the subtree rooted here? We know it is either an Int or Bool - but which one in general depends on the outcome of the conditional check. This can depend on run time information - e.g. user inputs - even without this, it is undecidable in general. WHAT HAVE WE LEARNED? • Every program fragment E needs to be given a type T in order to build up a picture of whether the whole AST is well-typed. • We need to deﬁne the ‘typing relation‘ written ⊢ E : T and read ‘E’ has type ‘T’) • We need to deﬁne this for every possible program E ! • We will want to do local checking on syntax trees • The type of a program op ( E1, E2, ... , En ) should depend only on the types of E1, E2, ... , En • Only certain operators generate actual checks for correct usage of types • We may need to approximate the type where we can’t determine it statically • To deﬁne a relation over the set of all programs is an interesting challenge. • But we know something special about the shape of the set of all programs! • This is going to help us enormously. • But what would the deﬁnition of such a relation look like? TYPE DERIVATION RULES AND TREES The general form of a type derivation rule is This can be read as “ If the relation holds for the things above the line then the relation holds for things below the line also”. Sometimes there are no premises above the line. ` E1 : T1 ` E2 : T2 .. . ` En : Tn ` E : T In general, a programming language will be given a set of such rules. Then, in order to show that ⊢\tE\t\t:\tT\tholds, the rules must be formed in to a tree such that the leaf nodes of the tree have no premises. For example ` E1 : T1 ` E2 : T2 ` E : T ` E0 : T0 ` E3 : T3 ` E4 : T4 ` E5 : T5 Let’s consider how this leads to a relation between all programs and types. NEXT LECTURE: INDUCTIVELY DEFINED SETS COMP2212 PROGRAMMING LANGUAGE CONCEPTS Dr Julian Rathke INDUCTIVELY DEFINED SETS THINGAMYS AND WHATSITS - A BRIEF EXCURSION • Let suppose we want to build an inﬁnite set of Things : here are some rules for building it • You can start with a Doobry or a Whatsit • Given anything in the set already, you can put a Thingamy on top of it. • Given anything with a Thingamy on top you can add a BlahBlah. • The set is the smallest set that you can build using these rules. • Do we know anything about the shape of this set? Yes, lots. Doobry Whatsit Thingamy Doobry Whatsit Thingamy Thingamy Doobry Thingamy BlahBlah Thingamy Doobry Thingamy Doobry Thingamy Thingamy Thingamy Doobry Thingamy Thingamy BlahBlah and so on ... This is an example of an inductively deﬁned set. INDUCTIVELY DEFINED SETS • The interesting thing is that, because the resulting set is the smallest such set, and we know all of the constructors for the set, then everything in the set must take one of these shapes. • Let’s write a little grammar for the set: • E ::= Doobry | Whatsit| Thingamy E| BlahBlah(Thingamy E) • The language accepted by this grammar is a description of this whole set of Thingamys and Whatsits. • Similarly, we can think of the set of all programs of a language as being inductively deﬁned by the operations of the language. • This is a very helpful viewpoint. SHAPE SHAPE USING INDUCTIVELY DEFINED SETS • In order to deﬁne a function or relation on the set of Thingamys and Whatsits, it is sufﬁcient to deﬁne the function or relation inductively by specifying it on the generating shapes. e.g. • and then asking for the smallest function that satisﬁes these rules. • Similarly, in order to deﬁne a function or relation on the set of all programs of a programming language then, we just need to deﬁne it on each program construct of the language • This is handy because we need to deﬁne typing relations ⊢\tE : T for all programs in the toy language. Because the set of all programs is inductively deﬁned by its operators. We can do this one operator at a time! fun BlahCount ( Doobry ) = 0 fun BlahCount ( Whatsit ) = 0 fun BlahCount ( Thingamy E ) = BlahCount ( E ) fun BlahCount ( BlahBlah ( Thingamy E ) ) = 1 + BlahCount (E) EXAMPLE RULES FOR THINGAMYS AND WHATSITS • Let’s write some derivation rules for a relation over Thingamys and Whatsits. • The relation will describe “balanced” terms - i.e. those with an equal number of Thingamys and BlahBlahs • There is no rule for Thingamy alone as this would not be balanced. • To show that a Thingamy and Whatsit is balanced with provide a derivation tree by combining instances of these rules - the leaf nodes have no premises. • e.g. ⊢ Doobry ⊢ Whatsit ⊢ BlahBlah ( Thingamy E) ⊢ E ⊢ BlahBlah (Thingamy BlahBlah (Thingamy Doobry)) ⊢ BlahBlah (Thingamy Doobry) ⊢ Doobry proves that this term is balanced. NEXT LECTURE: TYPE RULES FOR TOY COMP2212 PROGRAMMING LANGUAGE CONCEPTS Dr Julian Rathke TYPE RULES FOR TOY REMINDER : GRAMMAR OF THE TOY LANGUAGE The grammar for the Toy language we are using is as follows: T , U ::= Int | Bool E ::= n | true | false | E < E | E + E | x | | if E then E else E | let (x : T) = E in E REMINDER : TYPE DERIVATION RULES AND TREES The general form of a type derivation rule is “If the relation holds for the things above the line then the relation holds for things below the line also”. ` E1 : T1 ` E2 : T2 .. . ` En : Tn ` E : T In order to show that ⊢\tE\t\t:\tT\tholds, the rules must be formed in to a tree such that the leaf nodes of the tree have no premises. For example ` E1 : T1 ` E2 : T2 ` E : T ` E0 : T0 ` E3 : T3 ` E4 : T4 ` E5 : T5 RULES FOR THE TOY LANGUAGE Let’s write type derivation rules for our toy language, one construct at a time, then. First, rules for the values: ` n : Int TInt ` b : Bool TBool ` Eb : Bool ` E1 : T ` E2 : T ` if Eb then E1 else E2 : T TIf It can be useful to give the each type derivation rule a name (cf. TInt and TBool). These are the key points Let’s look at conditional expressions: TYPING RULES FOR LET EXPRESSIONS let ( x : T ) = E in EConsider the local variable construct: What type does this whole expression have? As a ﬁrst guess at a type rule we could write: This is hopelessly wrong! ` E1 : T ` E2 : U ` let (x : T )= E1 in E2 : U TLet? TYPING EXPRESSIONS WITH FREE VARIABLES let ( x : Int ) = 10 in x + 1 Consider the following example: An instantiation of the broken rule on the previous slide to this example is But how can we show that ⊢\tx\t+\t1\t:\tInt\t\tholds without knowing anything about x ? ` 10 : Int ` x +1 : Int ` let (x : Int) = 10 in x +1 : U TLet? TYPE ENVIRONMENTS In order to give a type to expression E, we need to have assumptions about the types of the free variables in E. We call these assumptions Typing Environments and we use the greek letter Γ to represent them. Formally, a type environment Γ is a mapping from variable names to Types. We write entries in this mapping comma-separated e.g. x : Int , y : Bool , z : Int , ... A CORRECT RULE FOR LET EXPRESSIONS Our type relation ⊢\tE : T needs to be modiﬁed to include the type environment. We will write Γ⊢ E : T to mean, in environment Γ, expression E has type T. Is this better? We will always need to do this for constructs that bind variables. Think about the scope of x ! Is x in scope in E2, ? Yes. So, enlarge the type environment with it. Is x in scope in E1 ? No. So don’t. Unless we want recursive expressions … This means extend the mapping with a distinct new entry Not quite, we need to enlarge the environment in which E2 is typed \u0000 ` E1 : T \u0000 ` E2 : U \u0000 ` let (x : T )= E1 in E2 : U TLet? \u0000 ` E1 : T \u0000,x : T ` E2 : U \u0000 ` let (x : T )= E1 in E2 : U TLet NEARLY THERE We still need a type rule for comparisons: And that’s it. All we need to do now is ask that ⊢\tis the smallest relation that satisﬁes all of the type rules for this language. By the induction principle, this deﬁnes the relation for every program of the language! And type rules for variables: x : T 2 \u0000 \u0000 ` x : T TVar And a type rule for addition: \u0000 ` E1 : Int \u0000 ` E2 : Int \u0000 ` E1 <E2 : Bool Tlt \u0000 ` E1 : Int \u0000 ` E2 : Int \u0000 ` E1 + E2 : Int TAdd NEXT LECTURE: ADDING FUNCTIONS TO TOY COMP2212 PROGRAMMING LANGUAGE CONCEPTS Dr Julian Rathke ADDING FUNCTIONS TO TOY HOW ABOUT ADDING FUNCTIONS TO TOY? • We could consider how to add Lambda Calculus like functions to our Toy language. • We would need to add function abstraction and application as operations. • We introduced Lambda Calculus in COMP2209 where we looked at it as an untyped language. • Recall that, in its untyped form, it is Turing complete - that is, all computation can be expressed in it ! • This situation changes somewhat if we introduce simple types. A note on notation : the exact syntax for lambda calculus varies from source to source so sometimes you will see λ x → E , sometimes λ x . E and sometimes λ (x) E - they all mean the same thing. SOME EXAMPLE LAMBDA CALCULUS TERMS The identity function: λ x . x is a function that takes a single argument and simply returns it. First projection: λ x . λ y . x is a function that takes two arguments and returns the ﬁrst Second projection: λ x . λ y . y is a function that takes two arguments and returns the second Twice : λ f . λ x . f ( f ( x ) ) is a function that takes a single-argument function and an argument and twice applies the supplied function f to the argument x. Composition : λ g . λ f . λ x . f ( g ( x ) ) takes two functions and an argument and returns the composed function f ; g Let’s think about what types these expressions might have? We need to allow some kind of type for functions e.g. T → U as a function that takes data of type T and returns data of type U SIMPLY-TYPED LAMBDA CALCULUS We can formalise a simple type system for lambda calculus. Without some base types though this is a very uninteresting language so let’s look at it combined with the Toy language. We’ll call the resulting language λ Toy T , U ::= Int | Bool | T ! T E ::= n | true | false | E < E | E + E | x | | if E then E else E | λ (x : T) E | | let (x : T) = E in E | E E The type rules for the added constructs are straightforward given what we have learnt already about type environments: \u0000 ` E1 : T ! U \u0000 ` E2 : T \u0000 ` E1 E2 : U TApp \u0000,x : T ` E : U \u0000 ` \u0000(x : T )E : T ! U TLam TYPES IN LAMBDA CALCULUS ? The identity function: λ (x:T) x First projection: λ (x:T) λ (y : U) x Second projection: λ (x : T) λ (y : U ) y Twice : λ (f : T → T) λ (x : T) f ( f ( x ) ) Composition : λ (g : T → U) λ (f : U → V) λ (x : T) f ( g ( x ) ) : T → T : T → ( U → T ) : T → ( U → U ) : (T → T) → T → T : (T → U) → (U → V) → (T → V) Try using the typing rules from the previous slide and rule TVar to write type derivations of each of these lambda expressions. TYPES IN LAMBDA CALCULUS - DERIVATION 1 The identity function: λ (x:T) x : T → T ⊢ λ ( x : T ) x : T → T x : T ⊢ x : T x : T ∈ { x : T } TVar TLam TYPES IN LAMBDA CALCULUS - DERIVATION 2 First projection: λ (x:T) λ (y : U) x : T → (U → T) ⊢ λ ( x : T ) λ ( y : U ) x : T → ( U → T ) x : T ⊢ λ ( y : U ) x : U → T x : T ∈ { x : T , y : U } TVar TLam x : T , y : U ⊢ x : T TLam TYPES IN LAMBDA CALCULUS - DERIVATION 3 Second projection: λ (x : T) λ (y : U ) y : T → U → U ⊢ λ ( x : T ) λ ( y : U ) y : T → ( U → U ) x : T ⊢ λ ( y : U ) y : U → U y : U ∈ { x : T , y : U } TVar TLam x : T , y : U ⊢ y : U TLam TYPES IN LAMBDA CALCULUS - DERIVATION 4 Twice : λ (f : T → T) λ (x : T) f ( f ( x ) ) : (T → T) → T → T ⊢ λ ( f : T → T) λ ( x : T ) f ( f x ) : (T → T) → T → T f : T → T ⊢ λ ( x : T ) f ( f x ) : T → T x : T ∈ { f : T → T , x : T } TLam f : T → T , x : T ⊢ f ( f x ) : T TLam TVar f : T → T , x : T ⊢ x : T f : T → T ∈ { f : T → T , x : T } TVar f : T → T , x : T ⊢ f : T → T f : T → T , x : T ⊢ f x : T TApp f : T → T ∈ { f : T → T , x : T } TVar f : T → T , x : T ⊢ f : T → T TApp TYPES IN LAMBDA CALCULUS - DERIVATION 5 Composition : λ (g : T → U) λ (f : U → V) λ (x : T) f ( g ( x ) ) : (T → U) → (U → V) → T → V ⊢ λ ( g : T → U) λ ( f : U → V ) λ (x : T) f ( g ( x ) ) : (T → U) → (U→ V) → T → V g : T → U ⊢ λ ( f : U → V ) λ (x : T) f ( g ( x ) ) : (U→ V) → T → V TLam g : T → U , f : U → V , x : T ⊢ g x : U TVar g : T → U , f : U → V , x : T ⊢ g : T → U TApp TApp TLam g : T → U , f : U → V ⊢ λ (x : T) f ( g ( x ) ) : T → V TLam g : T → U , f : U → V , x : T ⊢ f ( g ( x ) ) : V g : T → U , f : U → V , x : T ⊢ f : U → V f : U → V ∈ { g : T → U , f : U → V , x : T } TVar g : T → U ∈ { g : T → U , f : U → V , x : T } g : T → U , f : U → V , x : T ⊢ x : T x : T ∈ { g : T → U , f : U → V , x : T } TVar THE TYPE SYSTEM IN FULL (FOR REFERENCE) \u0000 ` n : Int TInt \u0000 ` b : Bool TBool \u0000 ` Eb : Bool \u0000 ` E1 : T \u0000 ` E2 : T \u0000 ` if Eb then E1 else E2 : T TIf Notice the Γ in the TInt, TBool and TIf rules. Phew, I’m glad that real programming languages don’t have more constructs than λ Toy! x : T 2 \u0000 \u0000 ` x : T TVar \u0000 ` E1 : Int \u0000 ` E2 : Int \u0000 ` E1 <E2 : Bool Tlt \u0000 ` E1 : Int \u0000 ` E2 : Int \u0000 ` E1 + E2 : Int TAdd \u0000 ` E1 : T \u0000,x : T ` E2 : U \u0000 ` let (x : T )= E1 in E2 : U TLet \u0000 ` E1 : T ! U \u0000 ` E2 : T \u0000 ` E1 E2 : U TApp \u0000,x : T ` E : U \u0000 ` \u0000(x : T )E : T ! U TLam NEXT LECTURE: TYPE CHECKING COMP2212 PROGRAMMING LANGUAGE CONCEPTS Dr Julian Rathke TYPE CHECKING IMPLEMENTING THE TYPE RULES • In the previous lectures we saw how to specify a type system • We gave very precise descriptions of which programs have which types. • We used type inference rules to form an inductive relation • In this lecture we will look at how to implement such sets of type rules • This is where the pain of using inductive rules pays off for us • Recall that the typing relation Γ⊢\tE : T is deﬁned as the smallest relation which satisﬁes the set of rules. • Logically then, if a given program E is in this relation with type T then the only way it got in to the relation was by using one of the rules. • But which one ? SYNTAX DIRECTED RULES • A set of inference rules S deﬁned over programs used to deﬁne an inductive relation R is called syntax directed if, whenever a program (think AST) E holds in R then there is a unique rule in S that justiﬁes this. Moreover, this unique rule is determined by the syntactic operator at the root of E. • For example: in our Toy language this term is well typed and has type Int. • Note that the last rule used to derive this fact must have been let foo = λ (x : Int) if (x < 3) then 0 else (x + 1) in foo (42) the full derivation would have also required use of TIf , TLt, TAdd, TLam, TApp, and TInt \u0000 ` E1 : T \u0000,x : T ` E2 : U \u0000 ` let (x : T )= E1 in E2 : U TLet STRUCTURE OF TYPE DERIVATION TREES Interestingly, for a syntax directed set of type rules we see that the structure of type derivation trees matches the structure of the AST of the program that we are deriving a type for. let foo = λ ( x : Int ) if (x < 3) then 0 else (x + 1) in foo (42) 42 let apply if x 0 foo λ < 3 + 1 x x : Int ` if (x< 3) then 0 else x +1 : Int TIf x : Int ` 0: Int TInt x : Int ` x +1 : Int TInt foo : Int ! Int ` foo(42) : Int TApp foo : Int ! Int ` 42 : Int TInt foo : Int ! Int ` foo : Int ! Int TFVar x : Int ` (x< 3) : Bool TBool ` \u0000(x : Int)if (x< 3) then 0 else x +1 : Int ! Int TLam ` let (foo : Int ! Int)= \u0000(x : Int)if (x< 3) then 0 else x +1 in foo(42) : Int TLet x : Int ` x : Int TVar x : Int ` 1: Int TInt x : Int ` x : Int TVar x : Int ` 3: Int TInt INVERSION LEMMA • Another important property that we desire of a typing relation is that of Inversion. • This refers to the ability to infer the types of subprograms from the type of the whole program - essentially by reading the type rules from bottom to top. • Here is the Inversion Lemma for the Toy Language • If Γ ⊢ n : T then T is Int • If Γ ⊢ b : T then T is Bool • If Γ ⊢ x : T then x : T is in the mapping Γ • If Γ ⊢ E1 < E2 : T then Γ ⊢ E1 : Int and Γ ⊢ E2 : Int and T is Bool • If Γ ⊢ E1 + E2 : T then Γ ⊢ E1 : Int and Γ ⊢ E2 : Int and T is Int • If Γ ⊢ if E1 then E2 else E3 : T then Γ ⊢ E1 : Bool and Γ ⊢ E2 : T and Γ ⊢ E3 : T • If Γ ⊢ λ (x : T) E : U’ then Γ , x : T ⊢ E : U and U’ is T → U • If Γ ⊢ let (x : T) = E1 in E2 : U then Γ , x : T ⊢ E2 : U and Γ ⊢ E1 : T • If Γ ⊢ E1 E2 : U then Γ ⊢ E1 : T ➝ U and Γ ⊢ E2 : T for some T Lemma (Inversion) This is easy to prove - but more importantly, yields a direct algorithm for working out types! A TYPE CHECKING ALGORITHM IN PSEUDOCODE Input : term E, environment Γ, match E with n -> return Int b -> return Bool x -> look up x in Γ, return its mapped type E1 < E2 -> check E1, E2 have type Int, return Bool E1 + E2 -> check E1, E2 have type Int, return Int if E1 then E2 else E3 -> check E1 has type Bool, check E2 and E3 have same type T , return this T lambda (x:T)E -> check E has type U in environment Γ, x : T , return type T -> U let(x : T) = E1 in E2 -> check E1 has type T in environment Γ , check E2 has type U in environment Γ, x : T, return type U E1 E2 -> check E1 has some type T->U, check E2 has type T, return type U. It’s very straightforward to turn this in to Haskell code. NEXT LECTURE: TYPE INFERENCE COMP2212 PROGRAMMING LANGUAGE CONCEPTS Dr Julian Rathke TYPE INFERENCE IMPLICIT TYPE ANNOTATIONS • You will have noticed that in our Toy language we explicitly declared the type of arguments to functions and local variables • λ ( x : T ) E and let (x : T) = E1 in E2 • This is common practice in many mainstream programming languages - especially statically typed ones (cf. C, C++, Java) • This is one of the points that advocates of dynamically typed languages often pick on when criticising static typing. It is a burden to the programmer. • What would be better then is a statically typed language in which the programmer isn’t obliged to declare the types of every variable, function, method, etc. • In this case, the type checker would need to infer the types of these entities from their usage in the code. • For example, let x = 20 in y + x , would reasonably allow the type checker to understand that both x and y have type int by their usage. • This is common practice in many functional programming languages. e.g. you have already being doing this in Haskell. • For languages with implicit types, we often refer to the type checking part of compilation as Type Inference rather than Type Checking. • Type Inference is algorithmically more complicated than Type Checking. • Let’s look at the rule for lambda in our Toy languages to see why. • Suppose instead that we don’t know T and U as part of the syntactic deﬁnition. • Then the rule would have to look like this: • and algorithmically we would have • λ (x) E -> check E has some type U in some environment Γ , x : ?? return type ?? -> U You can see how this complicates things. TYPE INFERENCE \u0000,x : T ` E : U \u0000 ` \u0000(x : T )E : T ! U TLam \u0000,x :?? ` E : U \u0000 ` \u0000(x)E :?? ! U TLam TYPE VARIABLES AND UNIFICATION • The approach often taken to solve this problem is to introduce Type Variables. • These are symbolic values that represent an unknown, or unconstrained type. • When typing a function with unknown types, type variables are used and type checking continues. • As part of type checking, certain constraints on these type variables will arise. e.g. if an argument to a function of unknown type is used as a guard of an IF statement then it must be a boolean. • So, the type checking algorithm will produce, for each well-typed program, a type that may contain variables, along with a collection of constraints on these type variables. • To obtain an actual type for the program we need to solve the constraints. That, is we ﬁnd a substitution of type variables such that all of the constraints hold. • This latter process is called uniﬁcation. • This is the basis of type inference in Haskell - there type variables are represented as types written as a , b and c EXAMPLE OF UNIFICATION let foo = λ(x) if (x < 3) then 0 else (x + 1) in let cast = λ(y) if (y) then 1 else 0 in cast (foo (42)) Let’s try infer types for this Toy program (with implicit types) Step 1- unfold the ﬁrst let. foo : a , λ x : if (x<3) then 0 else (x+1) : a, let cast = ... : b Step 2 : unfold the λx expression: constraint a = c -> d and if (x<3) then 0 else (x+1) : d assuming x : c . Step 3 : x < 3 : bool , 0 : int , (x+1) : int this requires x : c to have type int so a constraint c = Int is generated. Also we know d = Int Step 4 : unfold the second let statement : cast : e , λ(y) if ( y ) then 1 else 0 : e, and cast(foo(42)) : f. This generates the constraint b = f Step 5: unfold the λy expression: constraint e = g -> h and if (y) then 1 else 0 : h assuming y : g Step 6 : unfold the if. y : Bool (so g = Bool) and 1 : Int and 0 : Int. This generates the constraint h = Int Step 7: unfold the applications: cast (foo (42 )) : f with the constraint f = h and further, by unwinding foo(42) we get c = Int. We can also infer g=d because d is the return type of foo. a = c -> d c = Int, d = Int, b = f e = g -> h g = Bool h = Int f = h and g = d THIS EXAMPLE IS ILL-TYPED NEXT LECTURE: INTRODUCTION TO SEMANTICS COMP2212 PROGRAMMING LANGUAGE CONCEPTS Julian Rathke and Pawel Sobocinski INTRODUCTION TO SEMANTICS (LECTURE 7) WHAT DOES IT TAKE TO PROVE TYPE SAFETY? • You need to know what “well-typed” means - precisely • You need to know what “programs” means - precisely • You need to know what never “go” means - precisely • You need to know what “wrong” means - precisely “Well-typed programs never go wrong” • We give an inductively deﬁned typing relation - a bit like a proof in propositional logic • We give an inductively deﬁned reduction relation - i.e. how the program runs • We give a description of the error states of the program - these vary widely • We use the mathematical descriptions to try prove that type safety holds for a given language This is the where the mathematics comes in! ✅ ✅ ✅ BUT WHAT DOES IT ALL MEAN? • In this lecture we look at the topic of Semantics of Programs. • “Semantics” refers to the meaning of programs. • a semantics of a program is a speciﬁcation of a program’s runtime behaviour. That is, what values it computes, what side-effects it has etc. • the semantics of a programming language is a speciﬁcation of how each language construct affects the behaviour of programs written in that language. • Perhaps the most deﬁnitive semantics of any given programming language is simply its compiler or interpreter. • If you want to know how a program behaves then just run it ! • However, there are reasons we shouldn’t be satisﬁed with this as a semantics. WHY WE NEED FORMAL SEMANTICS • Compilers and interpreters are not so easy to use for reasoning about behaviour. Why ? • not all compilers agree! • compilers are large programs, it is possible (and common) that they contain bugs themselves. So the meaning of programs is susceptible to compiler writer error! • the produced low-level code is often inscrutable. It is hard to use compiler source code to trace the source of subtle bugs in your code due to strange interpretations of language operators. • compilers optimise programs (allegedly in semantically safe ways) for maximum efﬁciency. This can disturb the structure of your code and make reasoning about it much harder. ADVANTAGES OF FORMAL SEMANTICS • In contrast, a formal semantics should be precise (like a compiler) but written in a formalism more amenable to analysis. • this could be some form of logic or some other mathematical language. • don’t need to worry about efﬁciency of execution and can focus on unambiguous speciﬁcation of the meaning of the language constructs. • can act as reference ‘implementations’ for a language: any valid compiler must produce results that match the semantics. • they can be built in compositional ways that reﬂect high-level program structure. APPROACHES TO SEMANTICS • There are three common approaches to giving semantics for programs: • Denotational Semantics advocates mapping every program to some point in a mathematical structure that represents the values that the program calculates. • e.g. [[ if (0<1) then 0 else 1 ]] = 0 • Operational Semantics uses relational approaches to specify the behaviour of programs directly. Typically inductively deﬁned relations between programs and values they produce, or states the programs can transition between are used. • e.g. if (0<1) then 0 else 1 ! if (true) then 0 else 1 ! 0 • Axiomatic Semantics take the approach that the meaning of a program is just what properties you can prove of it using a formal logic. • e.g. Hoare Logic. • We’ll look at the ﬁrst two of these in a little more detail in the next two lectures. NEXT LECTURE: DENOTATIONAL SEMANTICS COMP2212 PROGRAMMING LANGUAGE CONCEPTS Julian Rathke and Pawel Sobocinski DENOTATIONAL SEMANTICS DENOTATIONAL SEMANTICS • To give a denotational semantics one must ﬁrst identify the semantic domain in to which we will map programs. • Elements in the semantic domain represent the ‘meanings’ of programs. • e.g. for programs that return a positive integer, a reasonable choice of semantic domain is the natural numbers. • for programs that represent functions from integers to integers we would choose the set of all functions between naturals. • for programs that return pairs of integers we take the semantic domain to be the cartesian product of the set of naturals with itself, etc. • Semantic domains are built by following the structure of the types of the language. • In an ideal language, the structures on the types would make for well-known, simple mathematical structures in the semantic domain! • This is not always the case, side-effects, loops and recursion complicate things DENOTATIONAL SEMANTICS FOR THE TOY LANGUAGE • Let’s try write a denotational semantics for our Toy language: First, we choose the semantic domains. These will be 𝐍 and a different two element set 𝐁 = {true, false}. We will also make use of the function spaces between these sets. Let’s deﬁne [[ T ]] to be 𝐍 when T is Int and 𝐁 when T is Bool and deﬁne [[ T → U ]] = [[ T ]] → [[ U ]] Our aim now is to provide a function [[ - ]] from well-typed programs E of type T to the semantic domain [[ T ]] , that is … Given ⊢ E : T , then [[ E ]] should be a value in [[ T ]]. T , U ::= Int | Bool | T ! T E ::= n | true | false | E < E | E + E | x | | if E then E else E | λ (x : T) E | | let (x : T) = E in E | E E INTERPRETING TYPE ENVIRONMENTS • Of course, in trying to interpret functions we will need to interpret function bodies. • These may contain free variables. • We will need to interpret terms with possibly free variables in them. • We need to have an environment to provide values for the variables. • Given a term 𝛤 ⊢ E : T then we need an interpretation [[ E ]] that makes use of an environment σ that maps each free variable in 𝛤 to a value in the semantic domain. We write [[ E ]]σ to denote this. • We say that σ satisﬁes 𝛤, written as σ ⊨ 𝛤 , if whenever 𝛤(x) = T then σ ( x ) is a value in [[ T ]] • We require the property that, for 𝛤 ⊢ E : T and for all σ such that σ ⊨ 𝛤, then [[ E ]]σ : [[ T ]] DEFINING THE DENOTATION FUNCTION FOR TOY Let’s start with the values and variables of the language and arithmetic expressions: [[ true ]] σ = true [[ false ]] σ = false where n is the corresponding natural in 𝐍[[ n ]] σ = n [[ E < E’ ]] σ = true if [[ E ]] σ < [[ E’ ]] σ [[ E < E’ ]] σ = false otherwise [[ E + E’ ]] σ = [[ E ]] σ + [[ E’ ]] σ [[ if E then E’ else E’’ ]] σ = [[ E’ ]] σ if [[ E ]] σ = true [[ if E then E’ else E’’ ]] σ = [[ E’’ ]] σ if [[ E ]] σ = false [[ λ (x : T) E ]] σ = v ↦ [[ E ]] σ [ x ↦ v ] [[ let (x : T) = E in E’ ]] σ = [[ E’ ]] σ [ x ↦ [[ E ]] σ ] [[ E E’ ]] σ = [[ E ]] σ( [[ E’ ]] ) σ σ [ x ↦ v ] means update the mapping σ with a map from x to value v where σ maps x to v [[ x ]] σ = v COMMENTS ON DENOTATIONAL SEMANTICS • A criticism one might have of denotational semantics at this point is that they don’t give a very clear account of how the program is actually supposed to execute. • Instead, they give a very precise and nicely compositional account of what values the program is supposed to calculate. This abstracts away all of the execution steps. • This can be useful for modelling pure functional languages, but it can be trickier for modelling languages with, say, mutable state or concurrency. • Modelling recursion denotationally can also be challenging - what value does a non- terminating recursive loop get mapped to ? • A major criticism of the above denotational model of the Toy language is that there is a lot of “junk” in the model … • The semantic domain [[ Int → Int ]] is all functions from 𝐍 to 𝐍 - this will include uncomputable functions. The model is “too big” in a sense. • There is lots of research in to ﬁnding denotational models that are “just right” - this is a difﬁcult task in general, even for small Toy languages. NEXT LECTURE: OPERATIONAL SEMANTICS COMP2212 PROGRAMMING LANGUAGE CONCEPTS Julian Rathke and Pawel Sobocinski OPERATIONAL SEMANTICS OPERATIONAL SEMANTICS • An alternative approach to semantics is to build a inductive binary relation between terms of the language. • We call this approach operational semantics • There are two ﬂavours of operational semantics: big step and small step • In big step semantics the binary relation is between terms and values. It represents the values that a term can evaluate to. • We typically write E ⇓ V to mean program E evaluates to value V. • The meaning of a program is given by the values it can evaluate to • Modelling the run time environment (e.g. a heap) is quite straightforward in this approach by deﬁning the relation between run time states and values. • The program operators are often easily speciﬁed in terms of “what they do” rather than “what they mean”. • One disadvantage of this approach is that it still doesn’t account for the effects of non-terminating programs very easily. Assume that each Toy literal n corresponds to a mathematical n BIG STEP TOY SEMANTICS • Let’s give an inductive relation for the big-step semantics for the Toy language: • The form of the relation will be E V “expression E evaluates to value V “ \u0000(x : T )E + \u0000(x : T )En + n b + b E1 + n E2 + m n< m E1 <E2 + true E1 + n E2 + m n 6<m E1 <E2 + false E1 + n E2 + m n + m = n0 E1 + E2 + n0 E1 + true E2 + V if E1 then E2 else E3 + V E1 + false E3 + V if E1 then E2 else E3 + V BIG STEP TOY SEMANTICS • Let’s give an inductive relation for the big-step semantics for the Toy language: • The form of the relation will be E V “expression E evaluates to value V “ We need to deﬁne what the substitution E [ V / x ] means exactly E1 + \u0000(x : T )E0 1 E2 + V2 E0 1[V2/x] + V 0 E1E2 + V 0 Big Step semantics still don’t model the sequence of computation steps a program may take though - this can be problematic when modelling e.g. concurrency. E1 + VE2[V /x] + V 0 let (x : T )= E1 in E2 + V 0 SMALL STEP OPERATIONAL SEMANTICS • In contrast, small step operational semantics are given by an inductive relation between terms representing run time states of programs. • Run time state includes the heap, stack, program counter etc. • We typically represent changing program counters as changing terms of a language. For example we write E → E’ for our reduction relation. • This means, program state E evaluates in ‘one’ step of evaluation to program state E’ • By considering the evaluation of a program step-by-step then we can see clearly how a program behaves. • To determine whether a program calculates a given return value then we repeatedly follow the single steps of evaluation until a value is reached. • non-termination is an activity (inﬁnite sequence of steps) rather than failure to calculate a value. • useful for analysing at what point programs begin to diverge and what effects they may have during divergence. • Let’s write a small-step semantics for the Toy language. SMALL STEP TOY SEMANTICS The form of this relation is E → E’ this represents a single step of computation of program state E reach the run time state E’ ( which can be represented as another expression ). n 6<m n < m ! false n< m n < m ! true n + m = n0 n + m ! n0 E ! E0 n <E ! n <E0 E ! E0 n + E ! n + E0 E1 ! E0 E1 + E2 ! E0 + E2 E1 ! E0 E1 <E2 ! E0 <E2 if true then E2 else E3 ! E2 if false then E2 else E3 ! E3 E1 ! E0 if E1 then E2 else E3 ! if E0 then E2 else E3 SMALL STEP TOY SEMANTICS CONTINUED The form of this relation is E → E’ this represents a single step of computation of program state E reach the run time state E’ ( which can be represented as another expression ). let (x : T )= V in E2 ! E2[V /x] E1 ! E0 let (x : T )= E1 in E2 ! let (x : T )= E0 in E2 \u0000(x : T )E1 V ! E1[V /x] E2 ! E0 \u0000(x : T )E1 E2 ! \u0000(x : T )E1 E0 E1 ! E0 E1 E2 ! E0 E2 EXAMPLE AND BIG STEP PROOF TREE • We’ll take an example Toy program and consider how to evaluate it using big step semantics, and then with small step semantics. • The example is : • The inductive rules in the big step semantics form a proof tree to justify the ﬁnal conclusion. • This tree is as follows let (x : Int) = if (10 < 3) then 0 else (10 + 1) in x + 42 let (x : T )= if (10 < 3) then 0 else (10 + 1) in x + 42 + 53 if (10 < 3) then 0 else (10 + 1) + 11 (10 < 3) + false (10 + 1) + 11 (x + 42)[11/x] + 53 10 + 10 10 + 10 1 + 13 + 3 11 + 11 42 + 42 SMALL STEP PROOF TREES For our example, small step semantics requires ﬁve evaluation steps to reach the value 53. Each single step is given by a proof tree that justiﬁes it. Step 4let (x : T ) = 11 in x + 42 ! (x + 42)[11/x] Tree for Step 1 let (x : T )= if (10 < 3) then 0 else (10 + 1) in x + 42 ! let (x : T )= if false then 0 else (10 + 1) in x + 42 if (10 < 3) then 0 else (10 + 1) ! if false then 0 else (10 + 1) 10 < 3 ! false Tree for Step 2let (x : T )= if false then 0 else (10 + 1) in x + 42 ! let (x : T ) = (10 + 1) in x + 42 if false then 0 else (10 + 1) ! (10 + 1) Tree for Step 3let (x : T ) = (10 + 1) in x + 42 ! let (x : T ) = 11 in x + 42 10 + 1 ! 11 Step 5(x + 42)[11/x] ! 53 SMALL STEP PROOF TREES For our example, small step semantics requires ﬁve evaluation steps to reach the value 53. Each single step is given by a proof tree that justiﬁes it. We sometimes write this sequence of steps without showing the proof trees.. let (x : T )= if false then 0 else (10 + 1) in x + 42 let (x : T )= if (10 < 3) then 0 else (10 + 1) in x + 42 let (x : T ) = (10 + 1) in x + 42 let (x : T ) = 11 in x + 42 11 + 42 53 ! ! ! ! ! RELATING SMALL STEP AND BIG STEP SEMANTICS • Ideally, if we have deﬁned our semantics correctly, then we should have a strong relationship between the big step and small step semantics. • They should specify the same behaviours - albeit in different ways. • To formalise this we ﬁrst deﬁne the following: E →* E’ • if and only if there exists a (possibly empty) sequence • E = E1 → E2 → … → En = E’ • We can prove the following Theorem for the Toy language semantics. Theorem For all E , V : E ⇓ V if and only if E →* V The proof of this is a straightforward proof by induction over the big step and small step derivation trees. NEXT LECTURE: TYPE SAFETY COMP2212 PROGRAMMING LANGUAGE CONCEPTS Julian Rathke and Pawel Sobocinski TYPE SAFETY A STATEMENT OF TYPE SAFETY • Recall type safety slogan “well-typed programs never go wrong” • we understand what it means to be well-typed for a program: inductive typing relation between programs and types. • we know what it means for a program to “go” : by inductively deﬁning operational semantics for that program. • We need to understand what “wrong” means • Notice that the operational semantics we gave for the Toy language did not depend on the terms being well-typed. • e.g. if(if(true) then false else 0) then true else 34 is ill-typed but according to the big and small step semantics, it would evaluate to the value 34. • consider instead if(if(false) then false else 0) then true else 34 . This term is also ill-typed but if one tries to evaluate it, it gets ‘stuck’ if(if(false) then false else 0) then true else 34 ! if(0) then true else 34 ! ??? STUCK TERMS AND DIVERGENCE • For the small step semantics, we can model our run time errors as ‘stuck’ terms. That is, terms that can not evaluate any further and are not values of the language. • For the big step semantics, we could do the same, but unfortunately, were we to introduce recursion or while loops to the language then big step semantics could not distinguish between stuck and divergent terms. • to model run time errors in a big step semantics we must explicitly model either errors. This would be done by introducing an error relation E ↯ that describes when E has reached an error state. • e.g. if ( n ) then E else E‘ ↯ would hold for any literal n. • we won’t go in to details of all of the error cases, let’s just assume ↯ is deﬁned. • This means that we could now distinguish between a stuck term and a divergent term in the big step semantics in the presence of recursion. • Unfortunately, this still doesn’t allow us to state type safety for the big step semantics. • The big step semantics only considers ‘complete’ computations. It cannot refer to what happens during computation. • All that we can easily specify with a big step semantics is a weak notion of type correctness: preservation. A WEAK STATEMENT OF TYPE SAFETY - PRESERVATION A typed language satisﬁes (weak) preservation if the following holds: for all closed well-typed terms E : if ⊢ E : T and E ⇓ V then ⊢ V : T This states that programs do not change their type after run time. More precisely it says that that values that programs produce actually are of the expected type. The situation for small-step semantics is much better: A typed language satisﬁes preservation if the following holds: for all closed well-typed terms E : if ⊢ E : T and E → E‘ then ⊢ E’ : T This states that, at every step of evaluation, programs do not change their type. PROGRESS • Preservation alone is not a sufﬁcient property to describe type safety. • It does not capture the fact that well-typed terms stay in semantically ‘good’ states. • For example, if we have a (broken) type system that allows a stuck term to be well- typed, then this stuck term will trivially satisfy preservation. • There are no reductions from the stuck term so types are preserved ! • To capture type safety more fully we need to know that well- typed terms never become stuck. • This property is what we call progress. A typed language satisﬁes progress if the following holds: for all closed well-typed terms E : if ⊢ E : T then either E → E’ (for some E’) or E is a value V Type safety = preservation and progress. PROVING PROGRESS FOR TOY LAMBDA Base Cases: (Let’s remind ourselves of the typing rules) if ⊢ E : T then E → E’ or E is a value VTheorem: Proof: We use a proof by induction over the typing derivation ⊢ E : T THE TYPE SYSTEM IN FULL (FOR REFERENCE) \u0000 ` n : Int TInt \u0000 ` b : Bool TBool \u0000 ` Eb : Bool \u0000 ` E1 : T \u0000 ` E2 : T \u0000 ` if Eb then E1 else E2 : T TIf x : T 2 \u0000 \u0000 ` x : T TVar \u0000 ` E1 : Int \u0000 ` E2 : Int \u0000 ` E1 <E2 : Bool Tlt \u0000 ` E1 : Int \u0000 ` E2 : Int \u0000 ` E1 + E2 : Int TAdd \u0000 ` E1 : T \u0000,x : T ` E2 : U \u0000 ` let (x : T )= E1 in E2 : U TLet \u0000 ` E1 : T ! U \u0000 ` E2 : T \u0000 ` E1 E2 : U TApp \u0000,x : T ` E : U \u0000 ` \u0000(x : T )E : T ! U TLam PROVING PROGRESS FOR TOY LAMBDA Inductive Cases: (we’ll show TIf as an example) Suppose the last rule used to derive the type of E is TIf. Then we know that E is of the form if E1 then E2 else E3 where ⊢ E1 : Bool and ⊢ E2 , E3 : T. Now, if E1 is not a value, then by the inductive hypothesis we must have that E1 → E1’ for some E1’. This allows us to derive if E1 then E2 else E3 ! if E1’ then E2 else E3 as required. Suppose then that E1 is a value. We know that the only values of type Bool are true and false. In either case we get a reduction if E1 then E2 else E3 ! E2 or if E1 then E2 else E3 ! E3. This means, whatever the form of E1, we have a reduction and hence progress. We must consider every type rule in the language and perform similar reasoning in each case Proof: We use a proof by induction over the typing derivation ⊢ E : T Base Cases: if the proof trees consist of just the rule TInt or TBool then E is a value. if ⊢ E : T then E → E’ or E is a value VTheorem: PROVING PRESERVATION FOR TOY If ⊢ E : T and E → E’ then ⊢ E’ : T Theorem: Proof: We use a proof by induction over the typing derivation ⊢ E : T Base Cases: rule TInt or TBool , E is a value so the hypothesis is trivially satisﬁed Inductive Cases: (we’ll show rule TIf as an example) Suppose the last rule used to derive the type of E is TIf. Then we know that E is of the form if E1 then E2 else E3 where ⊢ E1 : Bool and ⊢ E2 , E3 : T. Now, consider the possible reductions that can originate from E : either (i) it is a reduction of E1 to some E1’, that is E1 → E1’ (so that E’ is if E1’ then E2 else E3) or (ii) E1 is a boolean literal and if E1 then E2 else E3 ! E2 (or similar for E3) so that E’ is E2 (or E3). In case (ii), we know that both E2 and E3 have type T so therefore ⊢ E’ : T as required. In case (i) we apply the inductive hypothesis to E1. We know ⊢ E1 : Bool and by induction, we see that ⊢ E1’ : Bool also. Hence ⊢ if E1’ then E2 else E3 : T as required. NEXT LECTURE: STRUCTURED TYPES COMP2212 PROGRAMMING LANGUAGE CONCEPTS LECTURE 10 Julian Rathke and Pawel Sobocinski STRUCTURED TYPES THE SHAPE OF DATA • Programs manipulate data. • Data comes in different shapes and sizes. • To check that programs our doing what we want we can at least specify their behaviour in terms of the shape of the data they manipulate. • For example, a pair of integers is different shape data to two integers stored in an array. • Structured Types can make this distinction. • Goal: catch common programming errors where data being passed to a function or library call is in a different format to what the function or library code is expecting. • So what are the commonly used shapes of data and how do we express them as types and how do we type check programs that use structured types? EXAMPLE 1: BOOL • Easy. The Bool type is simple: it has two distinct elements. Programs can pattern match over each of these. e.g. match b with true ➝ ... | false ➝ ... • This is a surprisingly useful type and is easy to understand. • It has exactly one value, usually written as ( ). We’ll write the type as unit • Indeed, if you enter :type () at the Haskell top-level you get ()::() in response. In Haskell, the unit type is, confusingly, also written as ( ) • This is the type of Java methods that take no parameters: int foo( ){ ... } • It can be used to suspend the evaluation of an expression until a later point in the computation. For example compare • let x = print “Hello” in ..x.. ;; and • let x = fun () ➝ print “Hello” in ..x().. ;; • In a CBV language the former immediately prints hello and goes on with the evaluation. • The latter wraps the print statement in to a function for later use. • This operation of wrapping an expression with a function of unit type is called thunking . We can talk of unthunking a thunked value too. • The type rule for unit values is remarkably easy: THE UNIT TYPE ` () : unit PAIRS AND TUPLES • Pairs are another very common structured type. • Given two pieces of data of types T and U then we can form a piece of data of shape T x U. We usually write this as a pairing operation (E1 , E2) • The type rule for this is straightforward: • It is also straightforward to generalise this to arbitrary tuples: • The constructor for general tuples is (E1,E2, ... , En) • and the type rule is ` E1 : T ` E2 : U ` (E1,E2): T ⇥ U ` E1 : T1 ` E2 : T2 .. . ` En : Tn ` (E1,E2,. .., En): T1 ⇥ T2 ⇥ .. . ⇥ Tn DESTRUCTORS • We have referred to the operation (E1,E2) as the constructor for the pair datatype. • There are corresponding operations that we refer to as destructors for pairs. • These are called projections. • The ﬁrst projection is called fst. It returns the ﬁrst component of the pair. The second projection snd returns the second component of the pair. • There are type rules for these operations: • We can also use pattern matching to take apart the structured data • Of course, for generalised n-tuples we need n projection functions. ` E : T ⇥ U ` fst E : T ` E : T ⇥ U ` snd E : U match E with (E1, E2) ➝ ... RECORD TYPES • We can generalise tuples even further. What they are essentially are are collections of n elements of data indexed by integers. • More generally, we could use labels to index the items. This is what is known as a record we use these extensively in C (as struct types) and in Java ( as objects ! ) • The usual syntax for record constructors is { l1 = E1, l2 = E2, ... , ln = En } where the li labels are the ﬁelds of the record and the Ei are the values stored. • The type of data of this shape is written similarly: { l1 : T1, l2 : T2 , ... , ln : Tn } • The destructors for this type is called selection and has a very familiar notation: we write R . li to select the ﬁeld labelled li from record R • The type rule for records looks like this: ` Ei : Ti for 1  i  n ` {l1 = E1,l2 = E2,. .., ln = En} : {l1 : T1,l2 : T2,. .., ln : Tn} ` R : {l1 : T1,l2 : T2,. .., ln : Tn} ` R.li : Ti • List structures are very common across mainstream languages. • Given type T we can form the type T List • The constructor for Lists is cons, often written as :: • Destructors for lists are head and tail • Type rules: • Arrays feel similar but in fact aren’t really a structural type. They have no constructor to form elements of the type and we can’t pattern match across them. • However, it still makes sense to have type rules for them (using an array-like syntax) : OTHER STRUCTURES? LISTS AND ARRAYS ` E : T ` ES : T List ` E :: ES : T List ` ES : T List ` hd ES : T ` ES : T List ` tl ES : T List ` Ei : T for 1  i  n ` [E1,. ..En]: T Array ` E : T Array ` I : Int ` E[I]: T ANOTHER NON-STRUCTURE TYPE • A very common type former that we see in one form or another in mainstream languages is that of function types. • Given a type T and a type U we can form the type T ➞ U. Functions from T to U. • For higher-order functional languages (such as OCaml, Haskell etc) T can be any type at all (including function types). • For ﬁrst-order languages, such as C, C++, then T is restricted to being a primitive type or a structure built from primitive types. • Interestingly, the function type is not a structure type: we can’t pattern match over it. • The constructor for the type is lambda abstraction • Can you see why this is not a structured value? \u0000,x : T ` E : U \u0000 ` fun (x : T ) ! E : T ! U NEXT LECTURE: SUM TYPES COMP2212 PROGRAMMING LANGUAGE CONCEPTS LECTURE 10 Julian Rathke and Pawel Sobocinski SUM TYPES SUM TYPES • A pair types T x U represent structures in which data of both type T and U is present. • Sometimes we want a type that represents the possibility of data of either type T or type U being present. • This is known as a sum type. It is usually written as T + U • The constructors for this type are called injections and are written inl and inr • For example, inl 5 could be an element of type Int + Bool, or inr 10 could be an element of type Int + Int • Let’s look at the type rules for injections: • The only destructor for this type is the pattern matching operator. • Sometimes this is called case rather than the more general match • For example, case E of inl x ➝ E1 | inr x ➝ E2 ` E : T ` inl E : T + U ` E : U ` inr E : T + U UNIQUENESS OF SUM TYPES • You might have noticed that I said above that inl 3 could have type Int + Bool • In fact, it could also have type Int + Int, or indeed Int + AnyOtherType • If you look at the type rule for injection again you can see why: • Where does U come from ? • With such a rule, expressions in a language with these sums would fail to have unique types. This can be a complication for type checking - but not a deal breaker. • One way out of this is to choose the names of the injections to be unique for each different sum type. For example, Int + Int may have different injections to Int + Bool. • For example, we could choose inlft 2 to be of type Int + Int, whereas inL 2 would be uniquely of type Int + Bool. • Of course, the choice of names here is arbitrary. They are just labels l1 and l2 . • Indeed, we need not stop at two summands in the type. We could have a type made from T1 + T2 + ... + Tn with injections ll , l2 , ... , ln etc • This is starting to look familiar. ` E : T ` inl E : T + U VARIANT TYPES • Just in the same way that record types are a generalisation of tuples. Variant types are a generalisation of sums. • The type of variants is written : < l1 : T1, l2 : T2 , ... , ln : Tn > • The constructors for the values are injections named with labels: <li = E> • The type rules are ` E : Ti `hli = Ei : hl1 : T1,l2 : T2,. .., ln : Tni ` E : hl1 : T1,l2 : T2,. .., ln : Tni` x : Ti ` Ei : T for 1  i  n ` case E of hl1 = xi! E1 | .. . | hln = xi! En : T UNIQUENESS OF VARIANT TYPES • Consider two different variant types : • T = < Left : Int , Right : Int > and U = < Wrong : Int , Right : Int > • What type does the value < Right = 0 > have ? • It is both of type T and U • But we seem to have lost uniqueness of our types again. • Where variant types allow arbitrary labels, it is useful to insist that labels are unique among different types. • What Haskell does in this situation: it doesn’t give an error but simply infers that any value tagged with ‘Right’ to be of the most recently deﬁned variant using that tag. ENUMERATIONS • Sometimes you just want to write a variant type for its labels • E.g. a type for days of the week. • In this case having to use a variant type and give a type to each of these labels would be annoyingly verbose. • We can easily choose the unit type for this: • which will have values of the form < Mon = () > etc. • An enumerated type is simply a sugar for exactly this structure though. • Many languages allow us to write • The values of the type are simply the labels Mon, Tue etc. data Day = Mon of unit | Tue of unit | ... | Sun of unit data Day = Mon | Tue | Wed | Thu | Fri | Sat | Sun OPTION TYPES • We can also mix and match labels of type unit, suitably sugared, with labels of non unit type. • The most prominent example in Haskell is the option type. • data Maybe a = Nothing | Just a • This is a variant type with a Nothing ﬁeld (of implicit unit type) and a Just ﬁeld of some other type. • Nothing is a genuine value of this type. A TYPE RULE FOR MATCH ? • Suppose that we wanted to introduce pattern matching in to our Toy language. • Similar to the operator case - of - → in Haskell. • What would be the type rule for such an operator ? • It would be used as a general operator for tearing apart data structures. • The general form of the operator syntax would something like • The type rule would be something like (very roughly): • where pi* embodies assumptions about the variables bound in the pattern match. • This requires a type system for the language of patterns. • It gets a bit complicated. ` E : U ` pi : Up⇤ i ` Ei : T ` match E with p1 ! E1 | .. .pn ! En : T match E with p1 ➝ E1 | ... | pn ➝ En where the pi are patterns. NEXT LECTURE: SUBTYPING COMP2212 PROGRAMMING LANGUAGE CONCEPTS Julian Rathke and Pawel Sobocinski SUBTYPING • Suppose that we are in a language of functions and record types. • Consider the expression ( fun ( r : { x : int } ) -> r . x ) { x = 2 , y = 3 } • The function, call it F, just needs a record with a ﬁeld labelled x, but the argument also has a ﬁeld labelled y • Is this a problem? • The declared argument type of the function and the actual type of the argument need to match for this rule to be a valid instantiation. • This expression is not well-typed. Even though it seems perfectly sensible. WHY IS SUBTYPING A GOOD IDEA? ` F : {x : Int} ! Int ` {x =2,y =3} : {x : Int,y : Int} \u0000 ` F {x =2,y =3} : Int OVER SPECIFYING TYPE PARAMETERS • There are occasions where type systems forces us to be too speciﬁc. • e.g. example on previous slide • function F morally needs an argument of any type that has at least ﬁeld x:Int • another example: a function that returns the length of a array doesn’t need to care about what type of elements are in the array. • What we are looking for in examples such as the above is some sort of generic type, or polymorphic type for the type of the parameters to functions. • Many mainstream programming languages support generics or polymorphism. • They do so in different ways - in terms of both speciﬁcation and implementation. POLYMORPHISM • ‘Polymorphic’ literally means “many shaped” • A polymorphic function may be applied to many different types of data • There are different varieties of polymorphism: • Parametric polymorphism (C++ Templates, Java Generics, OCaml) • Subtype polymorphism (Obj Oriented, C++, Java etc) • Ad hoc polymorphism (overloading of functions and methods) • The latter, while useful, is often not so interesting as it simply boils down to clever naming schemes that include types for internal representation of functions and methods. • The former is a very rich topic that is closely related to type inference and uniﬁcation a la ML. • This lecture is about subtype polymorphism SUBTYPING • We could think of types as (structured) sets and simply say that a type T is a subtype of U if, in their interpretations as sets [[ T ]] is a subset of [[ U ]]. • This is a nice general deﬁnition but it doesn’t give us a convenient syntactic description of subtyping. • Two other strong possibilities are evident : • We could look at the capabilities of the types and say that T is a subtype of U if every operation that can be performed on U can also be performed on T. • This deﬁnition incorporates lots of structural properties of the types. E.g., pairs must be subtypes of pairs because of the projection operations. • This is called structural subtyping • We could explicitly declare what types we want to be subtypes of others and then make sure that any operations valid on a supertype are valid on the subtype. • This is the approach taken in object oriented languages, via inheritance. It is often called nominal subtyping. SUBSUMPTION AND THE SUBTYPE RELATION • Either way we look at it, we can extract the following property of subtyping: • If T is a subtype of U then every value of type T can also be considered as a value of type U. • This property is called subsumption. • It can be formalised in the following very general type rule • this relies on the subtyping relation T <: U between types. • The same rule is used for both structural and nominal subtyping systems. • So the obvious next question is how to deﬁne the subtyping relation. • This is where the structural and nominal subtyping systems differ greatly. ` E : TT <: U ` E : U TSub NEXT LECTURE: NOMINAL SUBTYPING COMP2212 PROGRAMMING LANGUAGE CONCEPTS Julian Rathke and Pawel Sobocinski NOMINAL SUBTYPING NOMINAL SUBTYPING • Types are distinguished by their names - even if they represent the same structure! • e.g.: struct Foo = {x : int , y : int} and struct Bar = { x : int , y : int } refer to different named types. • A function that expects a Foo value cannot accept a Bar value, even though they are structurally identical. • Type names hide the underlying structure. • Consider the following example: • type Address= { name : String , address : String } • type Email = { name : String , address : String } • These two types are conceptually different and are intended to be used differently. • Distinct names for the same structure allows the programmer to enforce the distinction at the type level. NOMINAL SUBTYPING RELATION • Need to explicitly specify the relationships between the named types. • A common approach is to provide the subtyping relation with the declaration of the type names themselves. e.g. type Foo subtypes Bar = { ... } • Or as we know from OO languages: class Foo extends Bar { ... } • As a type rule this looks something like (depending on syntax): • Such declarations alone aren’t typically enough to deﬁne the subtyping relation • We also ask that this relation is reﬂexive and transitive • Java adds an extra rule in that states every type is a subtype of Object. type T subtypes U = {.. . } T<: U SubDecl T<: T SubReﬂ T<: UU <: V T<: V SubTrans STRUCTURAL PROPERTIES VIA NOMINAL SUBTYPES • Of course, just declaring that one type is a subtype of another may break the important subsumption property that for T <: U every value of type T can be considered as a value of type U also. • e.g. if T is a pair type and U is a triple — which two of the three values do we take? • Java overcomes such difﬁculties by only allowing subtyping between a single form of structuring - classes. • Inheritance simply forces that every member in the supertype also exists in the subtype. • This approach is typical of nominal type systems. NEXT LECTURE: STRUCTURAL SUBTYPING COMP2212 PROGRAMMING LANGUAGE CONCEPTS Julian Rathke and Pawel Sobocinski STRUCTURAL SUBTYPING STRUCTURAL SUBTYPING • Relies purely on the structure of the type to deﬁne the subtyping relation. • First, the relationship between base, or primitive types, needs to be speciﬁed. • For example, one could want short <: int , float <: double • Then structure determines the rest. For example, a subtype of the pair type T x U is a pair of subtypes of T and U separately. • Record types are a generalisation of pair types, and the subtype relation on records generalises in an interesting way too. • Record types don’t rely on syntactic positioning in the values for their indexing. This means we can write a record with some ﬁelds missing and have it be perfectly well formed as a record value (of another type). We can’t do the same with a tuple. T1 <: T2 U1 <: U2 T1 ⇥ U1 <: T2 ⇥ U2 SubPair STRUCTURAL RECORD SUBTYPING • The rules for subtyping on records can be built up in stages. • First we have the generalisation of what we saw above for pairs. • This is called depth subtyping for records: • Then we have the notion of width subtyping in which there may be extra ﬁelds in the subtype: • And ﬁnally, we allow re-ordering of the listed ﬁelds: • Not all languages adopt all of these principles. Indeed, Java does not allow depth subtyping. Methods or ﬁelds in a subclass cannot have subtypes of that with which they are declared in the supertype. Ti <: Ui 1  i  n {l1 : T1,. ..ln : Tn} <: {l1 : U1,. .., ln : Un} SubRecDepth \u0000 a permutation of 1 .. .n {l1 : T1,. ..ln : Tn} <: {l\u0000(1) : T\u0000(1),. .., l\u0000(n) : T\u0000(n)} SubRecPerm {l1 : T1,. .., ln : Tn,ln+1 : Tn+1} <: {l1 : T1,. ..ln : Tn} SubRecWidth EXAMPLE OF RECORD SUBTYPING { x : int , y : int , z : int } <: { x : int } { w : int } <: { } { x : int , y : int } <: { x : int } { x : int , y : int , z : int } <: { x : int , y : int } { S : { x : int , y : int , z : int } , R : { w : int } <: { S : { x : int } , R : { } } SubRecWidth SubRecDepth SubTrans SubRecWidth SubRecWidth STRUCTURAL SUBTYPING FOR VARIANTS • For sum types T + U, it is intuitive that any value of type T can be considered as also being of type T + U + V as the value of type T is still just injected in to the sum, albeit in to a larger sum. • For generalised, variant, types. We have the same notions of width, depth and permutation subtyping as we do for records. • There is however, a subtle inversion in the rule for width subtyping: • We see that we can inject in to a larger variant type • The notions of depth and permutations are identical though. Ti <: Ui 1  i  n hl1 : T1,. ..ln : Tni <: hl1 : U1,. .., ln : Uni SubVarDepth hl1 : T1,. ..ln : Tni <: hl1 : T1,. .., ln : Tn,ln+1 : Tn+1i SubVarWidth \u0000 a permutation of 1 .. .n hl1 : T1,. ..ln : Tni <: hl\u0000(1) : T\u0000(1),. .., l\u0000(n) : T\u0000(n)i SubVarPerm NEXT LECTURE: VARIANCE COMP2212 PROGRAMMING LANGUAGE CONCEPTS Julian Rathke and Pawel Sobocinski VARIANCE COVARIANCE AND CONTRAVARIANCE • We have seen that in the type formers for pairs (records) and sums (variants) there is a relationship between the subtyping on the types of substructure and the subtyping of the structure itself. • For example, if T <: U and V <: V then T x V <: U x V • Notice that the ordering between T and U is somehow ‘preserved’ in the latter subtyping relation. • This preservation of order has a particular name. We say that the pair type former is a covariant type constructor. • We saw above that records and variants are both covariant type formers. • There are type formers that do not behave like this. • Indeed, suppose we had a type former called Foo. • So that given a type T, then Foo T is a new type. • If subtyping for Foo is such that: if T <: U then Foo U <: Foo T then we call Foo a contravariant type former. • Let’s look at real example of a contravariant type former. FUNCTION TYPES AND CONTRAVARIANCE • Subtyping interacts with function types in a very interesting way. • Let’s just look at the (general) type rule: • What we see is that the function type former is covariant in its return type and contravariant in its argument type! • Let’s try and explain why this is: • Suppose we have a function f:T1 ! T2 then what can we do with it? • We can apply it to an argument x, say of type T1, • But every U1 is also a T1 so f will accept any argument of type U1 also. • Now, what does f return. It returns a value of type T2. • But every value of type T2 is also of type U2, so f returns a value of type U2. • That is, f can accept any argument of type U1 and will return a value of type U2. • That is, f is also a function of type U1! U2 U1 <: T1 T2 <: U2 T1 ! T2 <: U1 ! U2 SubFun FUNCTION TYPES AND CONTRAVARIANCE • Let’s try see that pictorially: U1 <: T1 T2 <: U2 T1 ! T2 <: U1 ! U2 SubFun f T1 T2 x x v v f U1 U2<:<:<:??? OTHER STRUCTURES • The List type former is covariant. • This gives the simple subtyping rule • The situation for Arrays is a little more complicated. • An array is non-structural in the sense that we don’t build elements of the type using data constructors. Elements in the array can be modiﬁed. • That is, the array must both consume data that it is given (array write) and produce data when requested (array read). • These two operations have different variance requirements! T<: U T List <: U List SubList ARRAYS AND SUBTYPING We say that arrays are an invariant type former with subtyping rule: T<: UU <: T T [] <: U [] SubArray When an array is read, the Array type should be covariant When an array is written, the Array type should be contravariant T <: U would imply T[ ] <: U[ ] so I can treat a T array as a U array. If I fetch data from the U array and actually get a value of T, that is okay because I can treat this T value as a U value. If T[ ] <: U[ ] and I write to what I think is a U[ ] ( but is actually a T[ ] then this will be ﬁne if I write a U value and U <: T because the T[ ] can accept this U value by pretending it is a T value. SUBTYPING ARRAYS IN JAVA • The type rule for Arrays in Java is not the rule I showed on the previous slide. • Instead Java supports covariant array types • This may seem appealing as it certainly looks more ﬂexible than invariant arrays. • Yes, but it has the small drawback of being unsafe! • That’s right. Java is not type safe! Due to covariant array types. • Here is proof: T<: U T [] <: U [] SubJavaArray class B extends A { } class A { public static void main(String[] args){ B[] b = new B[1]; A.oops(b); } static void oops(A[] a){ a[0] = new A(); } } This code passes the Java type checker but throws a run time type error! OOPS. NEXT LECTURE: TYPES FOR OBJECTS COMP2212 PROGRAMMING LANGUAGE CONCEPTS Julian Rathke and Pawel Sobocinski TYPES FOR OBJECTS TYPE CHECKING OBJECT-ORIENTED PROGRAMS • What a type system for Java would look like? • Some points of order: • Java is a large language and providing type rules for all of its constructs is very time consuming. • First we’ll carve out a small core of Java that captures most of the essential OO features. We’ll call this small language Java Jr. • This should include at least classes, subclasses, objects, methods and ﬁeld accesses. • Other features like exceptions, concurrency etc can be added later. • We’ll formalise the type rules in the style that we have seen already. • We’ll use nominal typing to deﬁne the subtyping relation. SYNTAX OF JAVA JR - ON ONE SLIDE! At top level, a Java Jr program is made up of a sequence of class declarations: P ::= C K ::= c(t f, u g){super(f ); this.g = g; } C ::= class c extends t {K F M } F ::= tf ; M ::= public tm(t x){return E; } E ::= v | x | E.m(E) | E.f | E.f = E | new t(E) | (E == E?E : E) Assume a class ‘Main’ with a nullary constructor and a method ‘main’. A class contains a constructor, its ﬁeld types and method deﬁnitions. The constructor methods are default and the only code they contain is to initialise the ﬁelds. Classes have sequences of these ﬁeld declarations. Methods always have a return type (no void), they return the evaluation of a expression. Expressions include base values, variables, method call, ﬁeld accesses, new objects and conditionals. Types t include Object, int and bool, Values v include integer and boolean literals, Variables x include the keyword ‘this’ JAVA JR IS A SUBSET OF JAVA • If you look carefully, you can see that Java Jr is actually a valid subset of Java. • This means one can write a Java Jr program and compile it using javac. • We’ll assume the following Java execution harness: • This will let us run Java Jr programs using the command java JavaJRExec • It will print out the result of evaluating a program. class JavaJRExec { public static void main(String[] args){ Main m = new Main(); System.out.println(m.main().toString()); } } SOME USEFUL FUNCTIONS • To give formal type rules for Java Jr it is useful to deﬁne a number of lookup functions on the source code: • name ( class c extends t { … } ) = c • P . t = C if name( C ) = t • P . Object . methods = { } • P . t . methods = P . u . methods + M where P . t = class t extends u { K F M } • Here M + M’ is deﬁned to replace occurrences of methods named m in M where they also appear in M’ • P . Object . ﬁelds = { } • P . t . ﬁelds = P . u . ﬁelds + F where P . t = class t extends u { K F M } • Similar for F + F’ : ﬁelds named f in F are replaced by any ﬁelds declared in F’ • P . Object . sigs = { } • P . t . sigs = P . u . sigs + M . sigs where P . t = class t extends u { K F M } • (public t m(t x){ … } ) . sigs = t m( t x ) WELL FORMED TYPES AND SUBTYPING Let’s start by considering what the well-formed types of the language are. This is a relation between the Program P and the type names ranged over by t: P ` Object : type P ` int : type P ` bool : type P. t = class c extends u {K F M } P ` t : type Now deﬁne the (nominal) subtyping relation: P. t = class c extends u {K F M } P ` u : type P ` t<: u P ` t : type P ` t<: t P ` t : type P ` t<: Object P ` t<: uP ` u<: s P ` t<: s TYPE CHECKING RULES - FROM THE TOP We’ll work through the Java Jr grammar from the top down: C ` Ci : class for each Ci 2 C ` C : program This says that for a whole program to be well-typed, then each separate class deﬁnition must be well-typed (with respect to the other deﬁned types in the program). We’ll make an abbreviation in the above rule as follows: C ` C : class ` C : program We intend it to mean exactly the same as the above rule we are just writing it more concisely. We’ll use this notation a lot. TYPE RULE FOR CLASS DEFINITIONS This rule is a bit of a monster: It captures what it means to be a valid class deﬁnition. In words : The declared supertype must actually be a type. The types of the ﬁelds in this class must actually be types. The methods declared in the class must be well-formed methods in that class. The signature of methods declared in the class must be compatible with the signature of any overridden methods in the superclass. The ﬁelds of the superclass must be properly listed in the parameters of the default constructor. P ` s : type P ` u : type P ` M : method in c P.s.sigs ✓ P.c.sigs P.s.f ields = t f ; P ` class c extends s {c(t f, u g){super(f ); this.g = g ; } u g ; M } : class TYPE RULE FOR WELL-FORMED METHODS In this case we see that in order to be a well-formed method we need: The return type of the method to be a well-formed type. The types of the formal parameters to the method must be well-formed types. The method body must be a well-typed expression. n.b. Method bodies may contain free occurrences of the formal parameter variables, and hence we need a type environment for these. Also note that the ‘this’ keyword is a free variable in the body E and we need a type environment for that also. P ` t : type P ` t : type P | x : t, this : c ` E : t P ` public tm(t x){return E; } : method in c TYPE RULES FOR EXPRESSIONS We’ll start off with the simple expressions: values and variables. Note that type judgements for expressions also carry the type environment Γ for the free variables in the expression. P | \u0000 ` n : int P | \u0000 ` b : bool x : t 2 \u0000 P | \u0000 ` x : t Now consider the type rule for creating new objects. P ` c : class P.c.ﬁelds = t fP | \u0000 ` E : t P | \u0000 ` new c(E): c So c must be a valid class in P and the ﬁelds of this class (included inherited ﬁelds) must match the types of the arguments to the constructor. MORE TYPE RULES FOR EXPRESSIONS The type rule for conditional expressions should be straightforward: P | \u0000 ` E1 : uP | \u0000 ` E2 : uP | \u0000 ` ET : tP | \u0000 ` EF : t P | \u0000 ` (E1 == E2?ET : EF ): t We can compare objects of the same type, and the branches must have the same type. By subsumption (subtyping) we will be able to compare things of any two, possibly unrelated reference types. Rules for ﬁeld accesses are as follows: P | \u0000 ` E : ut f ; 2 P.u.ﬁelds P | \u0000 ` E.f : t P | \u0000 ` E : ut f ; 2 P.u.ﬁelds P | \u0000 ` E0 : t P | \u0000 ` E.f = E0 : t We look up the ﬁelds in the type of E and check that the type of their usage is matched. EVEN MORE TYPE RULES FOR EXPRESSIONS The type rule for method calls is similar to ﬁeld access: P | \u0000 ` E : ut m(t f ); 2 P.u.sigs P | \u0000 ` E : t P | \u0000 ` E.m(E): t We look at the type of the receiver Then look up the method signature for method m. The arguments to the method must match the types of the formal parameters. The return type from the method call is the return type declared in the signature. Finally, we need to include the subsumption rule to allow subtyping: P | \u0000 ` E : tP ` t<: u P | \u0000 ` E : u NEXT LECTURE: SIMPLY TYPED LAMBDA CALCULUS COMP2212 PROGRAMMING LANGUAGE CONCEPTS Julian Rathke and Pawel Sobocinski SIMPLY TYPED LAMBDA CALCULUS LET’S REMIND OURSELVES OF THE LAMBDA CALCULUS • In its purest form it contains just two primitives: function abstraction and application The grammar of the language is simply: E ::= x | \u0000x.E | EE x is drawn from a set of Variables The values are called abstractions. V ::= x | \u0000x.E • Application associates to the left • E1 E2 E3 always means ( E1 E2 ) E3 rather than E1 ( E2 E3 ) • Bodies of λ-abstractions extend as far to the right as possible • λ x . λ y . x y means λ x . ( λ y . x y), not λ x . ( λ y . x ) y • The variable x is bound in λ x . t - the scope of this binding is t . • A variable in a term that is not in the scope of an abstraction binding is called free • x and y are bound and z is free in λ x . λ y . x y z CALL-BY-VALUE OPERATIONAL SEMANTICS (\u0000x.E) V ! E[V /x] E1 ! E0 1 E1 E2 ! E0 1 E2 E2 ! E0 2 VE2 ! VE0 2 \u0000x.E + \u0000x.E The small step semantics are: And the big step semantics are: But what is this E [ V / x ] syntax ? E1 + \u0000x.E0 1 E2 + V2 E0 1[V2/x] + V E1 E2 + V Substitution! We deﬁned it in Programming III SUBSTITUTION - REMINDER x [ V / x ] = V y [ V / x ] = y if x ≠ y ( λ y . E ) [ V / x ] = λ y . ( E [ V / x ] ) if x ≠ y and y ∉ free-vars( V ) ( λ x . E ) [ V / x ] = λ x . E ( E1 E2 ) [ V / x ] = ( E1 [ V / x ] ) ( E2 [ V / x ] ) α-equivalence is the least equivalence relation that contains the following ( λ x . E ) ∼ ( λ y . E [ y / x] ) This is deﬁned up to α-equivalence : REDUCTION IN LAMBDA • Interestingly, the pure λ-calculus has no stuck closed terms : • Every closed term is either an abstraction or an application • For applications E1 E2 we inductively argue that E1 is not stuck and hence either has a reduction or is an abstraction. In the latter case then E1 E2 has a reduction. • However, not all terms reduce to a value ! • The pure calculus contains terms that have divergent behaviour. • Let Ω be ( λ x . x x ) ( λ x . x x ) • Let’s look at a reduction from Ω • Ω → Ω will reduce forever DENOTATIONAL SEMANTICS FOR UNTYPED LAMBDA (NON EXAMINABLE) • Paradoxically, that the λ-calculus is untyped is what gives it its computational power. • The term Ω doesn’t make sense in a typeful language • Ω contains terms applied to themselves !?! • Indeed, consider how one might build a denotational semantics for untyped lambda. • What should the semantic domain be? • λ-abstractions are functions so take some semantic domain D for them • But functions should be interpreted in some space D → D • We need functions denoted in D that are also denoted in D → D !?! • What we need is that D and D → D are the same domain • We need a structure that is equal (or isomorphic) to its own function space. • We know from set theory that this isn’t possible for D to be just a set of values. • Such structures do exist! • You can follow up on this by reading about Domain theory to ﬁnd out more. In essence D is an partial order like structure with certain extra properties. THE SIMPLY TYPED LAMBDA CALCULUS • It is apparent that the untyped λ-calculus is something of a complicated language. • While interesting, it is not perhaps the best way to design a language: • Recursion should be a feature that is explicitly added rather than something that sneaks in. • Types can actually help out here. • The pure simply typed λ-calculus is not actually that interesting so it useful to start including some primitive base types and values also. • In fact, our Toy calculus that we have been studying is exactly this. • We have already seen that Type Safety holds for this language. MORE INTERESTING PROPERTIES • Another important property that holds is Termination. • Every well-typed term of the simply typed lambda calculus always evaluates to a value ! That is - no loops, no recursion. • The proof of this is tricky but very interesting. • See Benjamin Pierce, Types and Programming Languages, Chapter 12.1. • The semantics for λ-calculus and our Toy language is deterministic. • We say that a set of reduction rules is deterministic if for all (closed) terms E then there is at most one E’ such that E → E’ . • Another way of saying this is, if E → E’ and E → E’’ then E’ = E’’ • To see that this holds for the Toy language we notice that while the set of rules aren’t quite syntax directed, they are such that, for any given term, there is a unique pattern match, if any, to the lhs of the conclusion of each rule. WHAT HAVE WE LOST ? • The Toy language is a straightforward enough language that enjoys nice properties. • But, we have no structured data types. • These are easily added in. (e.g. products and records) • But, we have lost expressive power - it is not Turing complete - because every term terminates. • In order to recover this we would need to explicitly introduce recursion. • There are different possibilities for doing this - recursive types, recursive functions, recursion via let-blocks, Y-combinator or ﬁx point operator. E.G. ADDING A FIX POINT OPERATOR We are going to introduce an operator ‘ﬁx’ that creates recursive behaviour. For example take: F = λ (isEven : Nat → Bool) . λ ( x : Nat ) . if ( isZero x ) then true else if ( isZero (x - 1)) then false else isEven ( x - 2 ) Then (ﬁx F) returns a function that recursively evaluates the isEven predicate. We need to write small step operational rules and a type rule for this new operator : \u0000 ` E : T ! T \u0000 ` ﬁx E : T ﬁx (\u0000(x : T ).E) ! E[ﬁx (\u0000(x : T ).E)/x] E ! E0 ﬁx E ! ﬁx E0 NEXT LECTURE: THE CURRY-HOWARD CORRESPONDENCE COMP2212 PROGRAMMING LANGUAGE CONCEPTS Julian Rathke and Pawel Sobocinski THE CURRY-HOWARD CORRESPONDENCE NATURAL DEDUCTION Do we all remember the rules of (Intuitionistic) Natural Deduction? (Pause for groan) System NJ0 : Deduction rules Formulas (where p is an atomic proposition): A, B ::= p | A ) B | A ^ B | A _ B | > | ? Deduction rules: \u0000,A ` A (Ax) \u0000 `> (> I) \u0000 `? \u0000 ` A (? E) \u0000,A ` B \u0000 ` A ) B () I) \u0000 ` A ) B \u0000 ` A \u0000 ` B () E) \u0000 ` A1 \u0000 ` A2 \u0000 ` A1 ^ A2 (^I) \u0000 ` A1 ^ A2 \u0000 ` Ai (^iE) \u0000 ` Ai \u0000 ` A1 _ A2 (_iI) \u0000 ` A1 _ A2 \u0000,A1 ` B \u0000,A2 ` B \u0000 ` B (_E) 4 / 20 Rules: System NJ0 : Deduction rules Formulas (where p is an atomic proposition): A, B ::= p | A ) B | A ^ B | A _ B | > | ? Deduction rules: \u0000,A ` A (Ax) \u0000 `> (> I) \u0000 `? \u0000 ` A (? E) \u0000,A ` B \u0000 ` A ) B () I) \u0000 ` A ) B \u0000 ` A \u0000 ` B () E) \u0000 ` A1 \u0000 ` A2 \u0000 ` A1 ^ A2 (^I) \u0000 ` A1 ^ A2 \u0000 ` Ai (^iE) \u0000 ` Ai \u0000 ` A1 _ A2 (_iI) \u0000 ` A1 _ A2 \u0000,A1 ` B \u0000,A2 ` B \u0000 ` B (_E) 4 / 20 Formulas: PROOFS AND PROGRAMS There is a strong relationship between Natural Deduction proofs and programs of the simply typed lambda calculus. Way back in 1969, William Howard made an amazing observation, building on much earlier work of Haskell Curry that there is a close correspondence between Up to this point, these two formal systems were considered separately in different areas of computation and logic. They appeared unrelated : natural deduction is a model of formal reasoning, lambda-calculus is a model of computation. Let’s remind ourselves of the pure simply typed lambda calculus : proofs of natural deduction and programs of the lambda-calculus \u0000,x : U ` E1 : T \u0000 ` \u0000(x : U ).E1 : U ! T\u0000,x : T ` x : T \u0000 ` E1 : T ! U \u0000 ` E2 : T \u0000 ` E1 E2: U THE ROOT OF THE CORRESPONDENCE • To see the correspondence, start by identifying the relationship between formulas and types : • Function types in lambda T → U correspond to implication A B in natural deduction. • Variables of base types in lambda correspond to propositions p in natural deduction. • Now, the typing rules of lambda correspond directly to inference rules of natural deduction: \u0000,x : T ` x : T \u0000,A ` A \u0000,x : U ` E1 : T \u0000 ` \u0000(x : U ).E1 : U ! T \u0000,A ` B \u0000 ` A ) B \u0000 ` E1 : U ! T \u0000 ` E2 : U \u0000 ` E1 E2 : T \u0000 ` A ) B \u0000 ` B \u0000 ` B Where \u0000 = x1 : T1,x2 : T2,. .., xn : Tn corresponds to \u0000 = A1,A2,. .., An And each Ti corresponds to Ai A THE CURRY-HOWARD CORRESPONDENCE Theorem A1,A2,. .., An ` A x1 : T1,x2 : T2,. .., xn : Tn ` t : T Let each formula A correspond to a type T, then is derivable in minimal intuitionistic logic (natural deduction) if and only if is well-typed for some term t such that free-vars(t) ⊆ { x1, x2, ... , xn } What this theorem states is that every proof of intuitionistic natural deduction maps to a term of the simply typed lambda-calculus and vice-versa. This statement can be made stronger by saying that there is an isomorphism between the space of natural deduction proofs and simply typed lambda terms. So programs can be viewed as proofs. And proofs can be viewed as programs. A stronger statement still can be made that reduction steps of programs correspond to well-known elimination techniques in natural deduction. OTHER LOGICAL CONNECTIVES • The above Curry-Howard correspondence only dealt with minimal intuitionistic logic (i.e. implication and propositions) • What about the other logical connectives - conjunction, disjunction ?Extension to products The conjunction _ ^ _: \u0000 ` A \u0000 ` B \u0000 ` A ^ B (^I) \u0000 ` A ^ B \u0000 ` A (^1E) \u0000 ` A ^ B \u0000 ` B (^2E) corresponds to the product type _ ⇥ _: \u0000 ` t : T \u0000 ` u : U \u0000 ` ht, ui : T ⇥ U (⇥I) \u0000 ` t : T ⇥ U \u0000 ` ⇡1t : T (⇥1E) \u0000 ` t : T ⇥ U \u0000 ` ⇡2t : U (⇥2E) The reduction of redexes (^I)/(^iE): ... ⇧1 \u0000 ` A1 ... ⇧2 \u0000 ` A2 (^I) \u0000 ` A1 ^ A2 (^iE) \u0000 ` Ai B ... ⇧i \u0000 ` Ai corresponds to \u0000-reduction for products and pairs: ⇡1ht1,t2i B\u0000 t1 ⇡2ht1,t2i B\u0000 t2 . I We identify the product type _ ⇥ _ with the conjunction _ ^ _ 15 / 20 Extension to sums The intuitionistic disjunction _ _ _: \u0000 ` A \u0000 ` A _ B (_1I) \u0000 ` B \u0000 ` A _ B (_2I) \u0000 ` A _ B \u0000, A ` C \u0000, B ` C \u0000 ` C (_E) corresponds to the sum type _ + _: \u0000 ` t : Ti \u0000 ` init : T1 + T2 (+iE) \u0000 ` t : T + U \u0000, x : T ` v : V \u0000, y : U ` w : V \u0000 ` case t {in1x 7! v | in2y 7! w} : V (+E) The reduction of redexes (_iI)/(_E): ... ⇧ \u0000 ` Ai (_iI) \u0000 ` A1 _ A2 ... ⇧1 \u0000, A1 ` C ... ⇧2 \u0000, A2 ` C (_E) \u0000 ` C B ... « ⇧i[⇧/Ai] » \u0000 ` C corresponds to \u0000-reduction for sums: case (in1t) {in1x1 7! u1 | in2x2 7! u2} B\u0000 u1[t/x1] case (in2t) {in1x1 7! u1 | in2x2 7! u2} B\u0000 u2[t/x2] . I We identify the sum type _ + _ with the disjunction _ _ _. 16 / 20 TRUTH AND FALSITY AND UNITS • The truth value ⊤ it is the unit for conjunction - and hence corresponds to the empty tuple type, that is: unit • Falsity ⊥ corresponds to the unit for disjunction - and hence corresponds to the empty sum type aka void A full system with units Terms t, u 2 ⇤ ::= x | \u0000x.t | tu | ht, ui | ⇡1t | ⇡2t | in1t | in2t | hi | case? t {} | case t {in1x1 7! u1 | in2x2 7! u2} Types T, U ::=  | U ! T | T ⇥ U | T + U | Unit | Void \u0000, x : T ` x : T (Ax) \u0000 ` hi : Unit (Unit I) \u0000 ` t : Void \u0000 ` case? t {} : T (Void E) \u0000, x : U ` t : T \u0000 ` \u0000x.t : U ! T (! I) \u0000 ` t : U ! T \u0000 ` u : U \u0000 ` tu : T (! E) \u0000 ` t1 : T1 \u0000 ` t2 : T2 \u0000 ` ht1,t2i : T1 ⇥ T2 (⇥I) \u0000 ` t : T1 ⇥ T2 \u0000 ` ⇡it : Ti (⇥iE) \u0000 ` t : Ti \u0000 ` init : T1 + T2 (+iI) \u0000 ` t : T1 + T2 \u0000, x1 : T1 ` u1 : U \u0000, x2 : T2 ` u2 : U \u0000 ` case t {in1x1 7! u1 | in2x2 7! u2} : U (+E) 17 / 20 System NJ0 : Deduction rules Formulas (where p is an atomic proposition): A, B ::= p | A ) B | A ^ B | A _ B | > | ? Deduction rules: \u0000,A ` A (Ax) \u0000 `> (> I) \u0000 `? \u0000 ` A (? E) \u0000,A ` B \u0000 ` A ) B () I) \u0000 ` A ) B \u0000 ` A \u0000 ` B () E) \u0000 ` A1 \u0000 ` A2 \u0000 ` A1 ^ A2 (^I) \u0000 ` A1 ^ A2 \u0000 ` Ai (^iE) \u0000 ` Ai \u0000 ` A1 _ A2 (_iI) \u0000 ` A1 _ A2 \u0000,A1 ` B \u0000,A2 ` B \u0000 ` B (_E) 4 / 20 Corresponds to : Theorem ( Curry-Howard Isomorphism ) A1,A2,. .., An ` A x1 : T1,x2 : T2,. .., xn : Tn ` t : T Let each formula A correspond to a type T, then is derivable in full intuitionistic logic (natural deduction) if and only if is well-typed for some term t (with x , + , unit , void) such that free-vars(t) ⊆ { x1, x2, ... , xn } Moreover, elimination of redexes in is isomorphic to reductions in the term language → USING THE ISOMORPHSIM • We can prove that the reduction relation → in the λ-calculus with products, sums, unit is deterministic and terminating. • Moreover, we know that preservation holds : if ⊢ t : T and t → t’ then ⊢ t’ : T • Every term t reduces to some value. • Closed values of the language are of the form: 〈〉, λx.t , 〈v, w〉, ini v • Because we know that → corresponds to sound elimination of redexes in natural deduction proofs we can infer the following. • If there is a proof of ⊢A then there is a term ⊢ t : T where T corresponds to A. • We know t reduces to some value v and we know that ⊢ v : T also. • This term v corresponds to a (normalised) proof ⊢A • We can use these facts to conclude the following : • ⊢ ⊥ is not derivable ( there is no closed value of this type ) • if ⊢ A ⋁ B then ⊢ A or ⊢ B ( closed values of sum types are injections ) • ⊢ p ⋁ ¬ p is not derivable ( where ¬ A is A ⊥ ) (ditto) NEXT LECTURE: CONCURRENCY COMP2212 Programming Language Concepts Race conditions and mutual exclusion History of concurrency ❖ What is concurrency? ❖ Several computations executing simultaneously, potentially interacting with each other ❖ “Multiprogramming”ﬁrst became a subject in the 1960s with important contributions by Dijkstra, Hoare and Brinch-Hanson ❖ invented foundational concepts of critical regions, mutual exclusion, locks, monitors, etc. that are still being used today Nondeterminism and race conditions Processes, address spaces ❖ A process is an Operating System abstraction. Typically a process involves an ❖ address space ❖ a number of threads ❖ each with its own call-stack, ❖ threads within a single process share an address space and thus can communicate via shared memory Threads and context switch ❖ The operation of switching control from one thread to another is called context switch ❖ context switch happens at the granularity of machine level instructions ❖ a context switch can happen in the middle of a “high-level” operation (e.g. assignment to a variable) ❖ On multi/many core, threads can run truly concurrently on different cores (and so have different caches! we will talk about visibility later this week) ❖ Shared memory concurrency is hard! new ready running waiting terminated interrupt scheduler dispatch I/O or event wait I/O or event completion admitted exit Thread life cycle Race conditions ❖ What happens if we run withdraw(10.0) and credit(20.0) as two threads, supposing that accountBalance is initially 100.0? Threads in CPThreads implementationExperiments different interleavingtest 1 what’s going on?!?!?! hmm... non-deterministic madness! Causes of race conditions ❖ When both of the following happen, race conditions become possible ❖ Aliasing - same location on the heap accessible from multiple threads ❖ no aliasing = no problem (Rust concurrency) ❖ Mutability - data on the heap can be altered ❖ no mutability = no problem (Clojure concurrency) Critical regions and mutual exclusion Critical regions • one solution to problems with shared resources: memory, I/O, ﬁles etc • critical region: part of program where a shared resource is accessed • mutual exclusion: if one thread is in its critical region for a resource then no other threads are allowed to enter their critical regions for that resource. • if no two threads are in a critical region at the same time then there will be no races Solution 1 Thread 1 Thread 2 • lock has initial value 0 • Does this guarantee mutual exclusion? Solution 2 Thread 1 Thread 2 • What properties does this implementation satisfy? • Is it a satisfactory solution? Peterson’s Algorithm ❖ Interesting for historical reasons called before entering critical region called after exiting critical region Busy waiting • The various implementations we have discussed employ busy waiting • blocked threads keep checking the status of lock & turn variables • Modern compilers & hardware reorder instructions within individual threads • (weak) memory models • more about this later (Java memory model) • this can break down classical algorithms such as Peterson’s Hardware support • Home-cooked solutions are non-trivial and sometimes break with modern hardware • memory visibility, compiler optimisations, hardware pipelines, etc. • Atomic operations provided by hardware • Test-and-set lock (TSL) instruction TSL RX,LOCK • reads the value of memory location LOCK • writes the value to register RX • stores a nonzero value at LOCK • guaranteed to be indivisible: memory bus is locked for the duration of operation • other CPUs cannot interfere Mutual exclusion with TSL ❖ Atomicity (hardware-guaranteed) of TSL ensures that races are avoided ❖ In programming languages a similar operation is sometimes given as a language primitive called CAS (compare-and-set) ❖ CAS is used for programming ﬁne-grained concurrent algorithms ❖ more on this next week Mutual exclusion with XCHG • XCHG exchanges two register and memory location atomically • used for similar purposes as TSL • atomicity of operation is the crucial ingredient • used in Intel x86 CPUs Atomicity, Locks, Monitors COMP2212 (Programming Language Concepts) ECS, University of Southampton Problems in Concurrent Programming Race conditions: situations where events may happen in different orders and the result depends on the order. Race conditions become a problem when we have no control over the order of events. Notoriously diﬀicult (and expensive) to debug due to non-determinism. A data race happens when there are two memory accesses that target the same memory location from different threads, at least one of these operations is a write, and there is no concurrency control to prevent them happening at the same time. Atomicity, Locks, Monitors 1/20 The meaning of “racy code” In Java, when there is a data race in our code, sequential consistency is not guaranteed by the Java standard. Sequential consistency is a property which allows us to think about the program “as if its threads are interleaved” (i.e. you can reason about the program as if it’s running on one core). In C and C++, if there is a data race, the behaviour is undefined! The compiler is free to do absolutely anything and will not deviate from the standard! Atomicity, Locks, Monitors 2/20 Concurrent algorithms are very diﬀicult to get right Possible solution: formal verification L. Lamport developed the so-called Bakery Algorithm for mutual exclusion in 1974 (not covered in this course). His earlier attempt at developing a mutual exclusion algorithm contained an error. Introduced the Temporal Logic of Actions (TLA) to help formally reason about correctness in concurrent systems. Leslie Lamport Atomicity, Locks, Monitors 3/20 Concurrency and Verification Optional pointers for those interested in concurrency research, verification, and the Temporal Logic of Actions (TLA): 1 L. Lamport on discovering the Bakery Algorithm: https://www.youtube.com/watch?v=zMSUdp5PH4c 2 Learn TLA+ site: https://learntla.com/introduction/ 3 R. Pressler’s blog tutorial (in IV parts; very accessible) “TLA+ in Practice and Theory”: https://pron.github.io/posts/tlaplus_part1 4 A model of Lamport’s Bakery Algorithm in TLA+: https://github.com/tlaplus/Examples/blob/master/specifications /Bakery-Boulangerie/Bakery.tla Atomicity, Locks, Monitors 4/20 Visibility Modern CPUs have sophisticated caching and each core typically has several levels of cache. [Cache synchronisation can be expensive!] Heap operations may not happen immediately. They may remain in local cache for some time. Therefore, operations performed by one thread may not be visible to another. Modern programming languages provide constructs that reference visibility (e.g. volatile in Java). Atomicity, Locks, Monitors 5/20 Memory Models Modern compilers reorder instructions, which may introduce concurrency bugs! A memory model is a description of how shared memory concurrency works. It can be seen as a contract between the programmer and the compiler designer. The programmer wants to avoid the compiler introducing bugs. The compiler designer wants maximum flexibility to perform optimisations. In Java, access to ordinary variables can be reordered; access to volatile variables cannot. Atomicity, Locks, Monitors 6/20 Atomicity An operation is atomic if it is indivisible: to an external observer it either happens fully, or not at all – it cannot be partially done. For example: balance = balance - withdrawal; is not an atomic operation. Atomic data structures are an example of thread-safe data structures. For example, in Java we have the AtomicInteger class. some methods: int get(), int set(int newValue), int getAndAdd(int delta). Operations are atomic and visible – thus, atomic data structures are like a “better” volatile. Atomicity, Locks, Monitors 7/20 Locks Locks are a classic programming abstraction for concurrency May use hardware atomicity guarantees or OS functionality for implementation. However, locks are a low level abstraction that can be diﬀicult to get right and does not always interact well with other programming features. Atomicity, Locks, Monitors 8/20 Problems with Locks Locks often lead to deadlocks (and livelocks). Classic example: dining philosophers problem There is one fork between each philosopher. A philosopher can either think or eat. To eat a philosopher needs to pick up two forks. Assume philosophers try to pick up the fork to the left first. Atomicity, Locks, Monitors 9/20 Problems with Locks Deadlocks are extremely common in code with multiple locks. Problems are caused by locks being picked up and released in the wrong order, not released at all, … Due to this, some applications with unpredictable/wild interleaving of actions (e.g. GUIs) are often single-threaded. Atomicity, Locks, Monitors 10/20 (Intrinsic) Locks in Java synchronized blocks (the object lock serves as the lock). 1 synchronized (lock) { 2 // access or modify shared state guarded by lock 3 } synchronized methods are simply shorthand for a synchronized block that spans the entire method body. The lock is the object on which the method is invoked. At most one thread may hold a lock. Atomicity, Locks, Monitors 11/20 (Explicit) Locks in Java (since Java 5.0) ReentrantLock is an alternative to intrinsic locking. 1 class X { 2 private final ReentrantLock lock = new ReentrantLock (); 3 // ... 4 5 public void m() { 6 lock.lock (); // block until condition holds 7 try { 8 // ... method body 9 } finally { 10 lock. unlock () 11 } 12 } 13 } Atomicity, Locks, Monitors 12/20 (Explicit) Locks in Java (since Java 5.0) ReentrantLock is an alternative to intrinsic locking. It has some advanced features: interrupt a thread that is waiting for a lock, attempt to acquire a lock without waiting for it indefinitely (trylock()), it is also dangerous: a lock is not automatically released as at the end of a synchronized block. Atomicity, Locks, Monitors 13/20 Volatile Variables in Java When a variable is declared volatile: the compiler and the runtime know that this variable is shared between threads. operations on volatile variables are not reordered by the compiler. writes do not get lost in local caches, a read is guaranteed to always see the last write. Atomicity, Locks, Monitors 14/20 Producer/Consumer Problem Two threads (a producer and a consumer) communicating via a bounded buffer. Producer Bounded buffer Consumer If the producer thread has produced too much (the buffer is full), it must wait for the consumer to “consume” in order to free up space. If the consumer thread has consumed all the data in the buffer, it must wait for the producer to produce data and insert it into the buffer. Atomicity, Locks, Monitors 15/20 Monitors Monitors combine the concept of lock with condition variables – queues of threads that are waiting on a lock. wait(&lock): release the lock, go to sleep. Will automatically reacquire the lock when woken up. signal(): wakeup a single waiting thread (in Java notify()) broadcast(): wakeup all waiting threads (in Java notifyAll()). Atomicity, Locks, Monitors 16/20 Monitors (pseudocode) 1 Lock l o c k ; 2 C o n d i t i o n emptyPlace ; 3 C o n d i t i o n d a t a A v a i l a b l e ; 4 5 i n t count =0; 6 7 v o i d p r o d u c e r { 8 i n t item ; 9 10 w h i l e ( t r u e ) { 11 item=produceItem ( ) ; 12 l o c k . a c q u i r e ( ) ; 13 w h i l e ( b u f f e r . i s F u l l ( ) ) { emptyPlace . w a i t ( ) ; } 14 i n s e r t _ i t e m ( item ) ; 15 d a t a A v a i l a b l e . s i g n a l ( ) ; 16 l o c k . r e l e a s e ( ) ; 17 } 18 } 19 20 v o i d consumer { 21 i n t item ; 22 w h i l e ( t r u e ) { 23 l o c k . a c q u i r e ( ) ; 24 w h i l e ( b u f f e r . isEmpty ( ) ) { d a t a A v a i l a b l e . w a i t ( ) ; } 25 item=removeItem ( ) ; 26 emptyPlace . s i g n a l ( ) ; 27 l o c k . r e l e a s e ( ) ; 28 } 29 } Atomicity, Locks, Monitors 17/20 Java Monitors 1 p u b l i c c l a s s ProducerConsumer { 2 3 s t a t i c f i n a l i n t N=100; 4 s t a t i c p r o d u c e r p = new p r o d u c e r ( ) ; 5 s t a t i c consumer c = new consumer ( ) ; 6 s t a t i c our_monitor mon = new our_monitor ( ) ; 7 8 p u b l i c s t a t i c v o i d main ( S t r i n g a r g s [ ] ) { 9 p . s t a r t ( ) ; 10 c . s t a r t ( ) ; 11 } Atomicity, Locks, Monitors 18/20 Java Monitors 12 s t a t i c c l a s s p r o d u c e r e x t e n d s Thread{ 13 p u b l i c v o i d run ( ) { 14 i n t item ; 15 w h i l e ( t r u e ) { 16 item = produceItem ( ) ; 17 mon . i n s e r t I t e m ( ) ; 18 } 19 } 20 p r i v a t e i n t produceItem ( ) { 21 /∗ . . . ∗/} 22 } 23 24 s t a t i c c l a s s consumer e x t e n d s Thread{ 25 p u b l i c v o i d run ( ) { 26 i n t item ; 27 w h i l e ( t r u e ) { 28 item = mon . remove ( ) ; 29 consumeItem ( ) ; 30 } 31 } 32 p r i v a t e v o i d consumeItem ( ) {/∗ . . . ∗/} 33 } Atomicity, Locks, Monitors 19/20 Java Monitors 34 s t a t i c c l a s s our_monitor ( ) { 35 p r i v a t e i n t b u f f e r [ ] = new i n t [N ] ; 36 p r i v a t e i n t count =0, l o =0, h i =0; 37 38 p u b l i c s y n c h r o n i z e d v o i d i n s e r t ( i n t v a l ) { 39 w h i l e ( count==N) { go_to_sleep ( ) ; } 40 b u f f e r ( h i )=v a l ; 41 h i =( h i +1)%N; 42 count++; 43 i f ( count==1) n o t i f y ( ) ; 44 } 45 46 p u b l i c s y n c h r o n i z e d i n t remove ( ) { 47 i n t v a l ; 48 w h i l e ( count==0){ go_to_sleep ( ) ; } 49 v a l=b u f f e r [ l o ] ; 50 l o =( l o +1)%N; 51 count −−; 52 i f ( count==N−1) n o t i f y ( ) ; 53 r e t u r n v a l ; 54 } 55 56 p r i v a t e go_to_sleep ( ) { 57 t r y { w a i t ( ) ; } 58 c a t c h ( I n t e r r u p t e d E x c e p t i o n ex ) {} 59 } 60 } 61 62 } Atomicity, Locks, Monitors 20/20 COMP2212 Programming Language Concepts Barriers and latches, performance and scalability, CAS and ﬁne-grained concurrency Synchronising threads Barriers and Latches Latches ❖ A latch acts as a gate ❖ until the latch reaches its terminal state no thread can pass ❖ when the terminal state is reached the gate opens and all threads can pass ❖ CountDownLatch class in Java ❖ initialised with some positive integer value ❖ await() and countDown() Example - Test HarnessBarriers ❖ Sometimes it is useful to force several threads to synchronise ❖ all threads must come together at the barrier point at the same time in order for them to proceed ❖ Java CyclicBarrier class - initialised with number of processes ❖ await() on barrier blocks the calling thread until all threads call await() ❖ latches wait for events, barriers wait for other threads Scalability Performance ❖ How to measure: throughput, responsiveness ❖ Avoid premature optimisations: make it correct, then make it fast ❖ Threading increases performance ❖ Independent tasks can be run in parallel ❖ Threading introduces overheads ❖ thread coordination (locking, signalling, memory synchronisation) ❖ context switching, thread creation, OS scheduling overheads Scalability ❖ Scalability means the ability to improve performance when additional resources are added ❖ If I double clock speed then clearly most programs run faster ❖ But if I add another CPU will it increase performance? ❖ clearly if all my code is sequential, there is no performance beneﬁt since the extra hardware parallelism cannot be used Amdahl’s Law ❖ Some tasks are naturally parallel: throwing extra cores at the problem improves performance ❖ Sequential tasks do not beneﬁt at all from extra processors ❖ Amdahl’s law: suppose that F is the fraction of task that is sequential. Let N be the number of processors available. speedup ≤ 1/(F + (1-F)/N) e.g. suppose that half of task is sequential. Then maximal speedup is 2, regardless of number of processors thrown at the problem! Improving scalability ❖ Reduce lock contention (multiple threads competing for the same lock) is a problem ❖ reduce the duration for which locks are held (reducing lock granularity) ❖ reduce the frequency with which locks are requested ❖ replace exclusive locks with other coordination mechanisms that permit greater concurrency ❖ eg. ﬁne-grained concurrency with CAS Fine grained concurrency and CAS Compare and Set (CAS) ❖ Supported by most processor architectures ❖ CAS(V, A, B) ❖ V - memory reference ❖ A - old value ❖ B - new value ❖ There are two possible executions ❖ If V contains A then it is atomically changed to B and CAS returns true ❖ If V does not contain A then nothing happens and CAS returns false ❖ When multiple threads attempt to modify a memory location using CAS, one wins, the others lose ❖ From Java 5.0, CAS is available on int, long and object references: it is used in implementations of many thread safe data structures in java.util.concurrent CAS When there are many competing threads, CAS can repeatedly fail, in that case can use exponential backoff Treiber stack Stacks ❖ Stacks are a relatively simple to implement in ﬁne-grained fashion because there is a single point of contention - the head of the stack ❖ for scalability, even this can be improved by adding elimination arrays ❖ Surprisingly, there are also beautiful solutions for other data structures ❖ e.g. Michael-Scott queue, in java as java.util.concurrent.JavaLinkedQueue CAS How can the last CAS fail? COMP2212 Programming Language Concepts Labelled transition systems Reasoning about concurrent programs ❖ There are some essential aspects of concurrency that make is different from sequential computation, e.g.: ❖ communication - processes communicate with other processes either through shared memory or with message passing ❖ synchronisation - processes must sometimes synchronise their actions to ensure atomicity ❖ nondeterminism - what can be observed about a program changes from one run to the next ❖ Some things that we take for granted need to be rethought: e.g. how to test concurrent code? Nondeterminism example ❖ Suppose we run the above three pieces of code concurrently. What will be printed? Thread 1 Thread 2 Thread 3 x = 1; x = 0; print “x is “.i_to_s(x); x = 2; Possibilities ❖ We can use a mathematical structure called a Labelled Transition System (LTS) in order to capture what can be observed about programs ❖ LTS are a mathematical structure for reasoning about nondeterminism ❖ the labels of transitions say “what can be observed” ❖ eg. x is 0 x is 1 x is 2 States Transitions Labelled transition systems ❖ We have already seen two examples of labelled transition systems ❖ A labelled transition system is a mathematical structure (X, Σ, L) where ❖ X is a set of states ❖ Σ is an alphabet of actions ❖ L ⊆ X x Σ x X LTSs and finite state automata ❖ Labelled transition systems are similar to ﬁnite state automata, which also have states and transitions, but there are important differences ❖ The set of states in an LTS can be inﬁnite: we cannot assume that our systems have only a ﬁnite number of possible states! ❖ LTSs typically do not have initial and ﬁnal states Example, formally x is 0 x is 1 x is 2 X = { s0, s1, s2, s3 } Σ = { “x is 0”, “x is 1”, “x is 2”} L = { (s0, “x is 0”, s1), (s0, “x is 1”, s2), (s0, “x is 2”, s3) } s0 s1 s2 s3 Kinds of nondeterminism ❖ Internal - “the machine chooses” ❖ e.g. the simple code example we have examined ❖ the nondeterminism is resolved by the scheduler ❖ External - “the environment chooses” ❖ e.g. interactive systems such as vending machines ❖ the combination of a vending machine and user can be thought of as a concurrent system External nondeterminism - Coffee machine £ t c x0 x1 x2 x3 X = { x0, x1, x2, x3 } Σ = { £, t, c} L = { (x0, £, x1), (x1, t, x2), (x1, c, x3) } ❖ The user puts in money - this is the £ action ❖ The machine now offers a choice between tea (t) and coffee (c) Process equivalence ❖ Non determinism is inherent in concurrent and interactive systems ❖ What does it mean that a system is correct? ❖ one answer: it should behave like (be equivalent to) some speciﬁcation ❖ but what should equivalent mean? ❖ this is a surprisingly subtle question that has resulted in a lot of research over the last 40 years First try ❖ Give the speciﬁcation as a set of traces ❖ a trace is a sequence of observations from some state ❖ example: ❖ Say that two states are trace equivalent when they have the same traces £ t c Traces from x0: ε, £, £t, £c x0 Traces, example ❖ Some systems have an inﬁnite set of traces b a Traces from x0: b, ab, aab, aaab, aaaab, …. Indeed, all the words matched by the regular expression a*b x0 Example: coffee machines £ t c x0 x1 x2 x3 y0 y1 y2 y3 y4 £ £ t c ❖ Should we consider these two coffee machines as equivalent? ❖ Note that x0 and y0 are trace equivalent, the difference is when the nondeterministic choice happens Moral ❖ In some cases, trace equivalence is too coarse: it equates too much ❖ we want to distinguish the two coffee machine examples ❖ In the next few lectures we will discuss ﬁner ways of distinguishing between labelled transitions systems: simulation and bisimulation COMP2212 Programming Language Concepts Simulations Simulations, intuitively ❖ A state y can simulate a state x if ❖ whenever x can do some action, becoming x’ in the process, y can reply with the same action, becoming y’ in the process ❖ moreover, now y’ can still do everything that x’ can do… and so forth ❖ Closely related to the notion of reﬁnement Simulations, formally ❖ Recall from COMP1215 that a relation on a set X is a subset R ⊆ X x X ❖ Suppose that (X, Σ, L) is a labelled transition system ❖ A simulation is a relation on states that satisﬁes the following condition: ❖ Whenever xRy and x x’ then there exists y’ such that y y’ and (x’,y’) ∈ R ❖ if (x,y) ∈ R we say that y simulates x ❖ we sometimes write x ≤ y if there exists a simulation that contains (x,y) a a Example ❖ Let R={ (y0, x0), (y1, x1), (y2, x1), (y3, x2), (y4, x3) } ❖ check that this is a simulation! £ t c x0 x1 x2 x3 y0 y1 y2 y3 y4 £ £ t c Properties of simulations ❖ Theorem. Unions of simulations are simulations ❖ There exists the largest simulation — it is the union of all simulations ❖ The largest simulation, written ≤, is called similarity ❖ Simulation is an instance of something called coinduction in mathematics. Using simulations ❖ Q. How to show that y simulates x? ❖ A. Construct a simulation that contains the pair (x,y) ❖ Q. How to show that y does not simulate x? ❖ A. Show that all relations that contain (x,y) are not simulations? This sounds tedious… The simulation game ❖ We are playing against a demon. The game starts at position (x,y). 1. The demon picks a move x x’ 2. We must choose a y’ such that y y’ 3. The game goes back to step 1, changing the position to (x’,y’) ❖ If at any point a player cannot make a move, that player loses ❖ If the game goes on forever, we win ❖ If the Demon has a winning strategy, then y does not simulate x ❖ If we have a winning strategy then x ≤ y a a Example ❖ We know that x0 simulates y0. Now let us play the game and check that y0 does not simulate x0. £ t c x0 x1 x2 x3 y0 y1 y2 y3 y4 £ £ t c Example continued ❖ We give a winning strategy for the demon, when starting in position (x0, y0). ❖ The demon picks the £ move to x1 ❖ we have to reply with one of the two available moves, moving either to y1 or to y2 ❖ if we picked y1 then the demon can play the c move and we are stuck (we lose) ❖ if we picked y2 then the demon can play the t move and we are stuck (we lose) £ t c x0 x1 x2 x3 y0 y1 y2 y3 y4 £ £ t c Simulation equivalence ❖ States x and y are said to be simulation equivalent if both x ≤ y and y ≤ x ❖ thus to check that two states are simulation equivalent, we typically need to construct two simulations Example ❖ We know that x0 and y0 are not simulation equivalent. Indeed, we have shown: ❖ y0 < x0, by constructing a simulation ❖ but not x0 < y0, by playing the simulation game £ t c x0 x1 x2 x3 y0 y1 y2 y3 y4 £ £ t c Simulation equivalence and trace equivalence ❖ Theorem. If x and y are simulation equivalent then they are also trace equivalent. ❖ We have seen that the converse is not true: there are trace equivalent states (x0 and y0) that are not simulation equivalent ❖ Sometimes we say that simulation equivalence is a ﬁner equivalence (it distinguishes more) than trace equivalence. Conversely, trace equivalence is coarser than simulation equivalence. Example - an evil coffee machine ❖ z0 can act like x0, by going to z1 but sometimes (maybe a race condition in the implementation?) it may go to z2 or z3 ❖ x0 and z0 are simulation equivalent (why?). Should we be able to distinguish them? £ t c x0 x1 x2 x3 £ t c z0 z1 z4 z5 z2 z3 z6 z7 £ £ t c COMP2212 Programming Language Concepts Bisimulations Example - an evil coffee machine ❖ z0 can act like x0, by going to z1 but sometimes (maybe a race condition in the implementation?) it may go to z2 or z3 ❖ x0 and z0 are simulation equivalent (why?). Should we be able to distinguish them? £ t c x0 x1 x2 x3 £ t c z0 z1 z4 z5 z2 z3 z6 z7 £ £ t c Simulations ❖ Recall that to know that state y simulates x it sufﬁces to construct a simulation that contains (x,y) ❖ In the evil coffee machine it is true that both x0 is simulated by z0 and z0 is simulated by x0 - so they are simulation equivalent states ❖ This “feels” weird: the crucial insight is that the two simulations are not the same relation on states! £ t c x0 x1 x2 x3 £ t c z0 z1 z4 z5 z2 z3 z6 z7 £ £ t c Bisimulation ❖ A bisimulation is a simulation that goes both ways — that is the relation and its reverse are both simulations ❖ so, intuitively if x and y are bisimilar then both x can respond to the moves of, and y can respond to the moves of x — and stay in the same relation Bisimulation, formally ❖ Suppose that (X, Σ, L) is a labelled transition system ❖ A bisimulation is a relation on states that satisﬁes the following conditions, whenever (x, y) ∈ R ❖ if x x’ then there exists y’ such that y y’ and (x’,y’) ∈ R ❖ if y y’ then there exists x’ such that x x’ and (x’,y’) ∈ R ❖ We sometimes write x ~ y if there is a bisimulation that contains (x, y) a a a a Example ❖ { (x0,y0), (x0, y1) } is a bisimulation a a a x0 y0 y1 More examplesExamples of bisimulations, 2 p a q q1 a p1 p2 a ∼ p a q q2 a p1 ∼b q3 q4 q1 a a b b b b a Bisimulation, Games & Hennessy Milner logic – p.13/32 Examples of bisimulations, 2 p a q q1 a p1 p2 a ∼ p a q q2 a p1 ∼b q3 q4 q1 a a b b b b a Bisimulation, Games & Hennessy Milner logic – p.13/32 Examples of bisimulations, 2 p a q q1 a p1 p2 a ∼ p a q q2 a p1 ∼b q3 q4 q1 a a b b b b a Bisimulation, Games & Hennessy Milner logic – p.13/32 Examples of bisimulations, 2 p a q q1 a p1 p2 a ∼ p a q q2 a p1 ∼b q3 q4 q1 a a b b b b a Bisimulation, Games & Hennessy Milner logic – p.13/32 Properties of bisimulation ❖ Theorem Unions of bisimulations are bisimulations ❖ There exists the largest bisimulation, it is the union of all bisimulations ❖ The largest bisimulation, written ~, is called bisimilarity ❖ Just as simulation, bisimulation is a coinductive concept — more on this next week Bisimilarity ❖ To show that x ~ y, it is enough to construct a bisimulation that contains (x, y) (why?) ❖ Again, it is less clear how to show that two states are not bisimilar ❖ just like for similarity, there is a game we can play! The bisimulation game ❖ We play against a demon. The game starts at position (x,y) 1.The demon picks where to play, either at x or at y 2.If demon chose x, he picks a move x x’ ❖ in this case we must respond with y y’ 3.If demon chose y, he picks a move y y’ ❖ in this case we must respond with x x’ 4. The game goes back to step 1, changing position to (x’, y’) a a a a Bisimulation game, continued ❖ If at any point a player cannot make a move, that player loses ❖ If the game goes on forever, we win ❖ If the Demon has a winning strategy, then x and y are not bisimilar ❖ If we have a winning strategy then x ~ y Bisimulation game, example ❖ Here’s a winning strategy for the demon, starting in position (x0, z0) ❖ The demon picks z0 to play in and plays the £ move to z2 ❖ We have to match with the £ move to x1 ❖ The game continues from position (x1,z2) but now the demon switches positions and plays from x1 - and picks the c move to x3 ❖ we are stuck, because there is no c move from z2 - so we lose! £ t c x0 x1 x2 x3 £ t c z0 z1 z4 z5 z2 z3 z6 z7 £ £ t c Anything finer? ❖ We have a new candidate for a relation, bisimilarity, to distinguish processes — but we already had two ❖ trace equivalence ❖ simulation equivalence ❖ bisimilar implies simulation equivalent implies trace equivalent ❖ but the implications do not go the other way ❖ Can we cook up an even more evil coffee machine example to cast doubts on bisimilarity? Answer ❖ No! ❖ An observer cannot tell the difference between any two bisimilar states if all they can see are the capability of performing actions ❖ Bisimilarity is the ﬁnest “reasonable” equivalence ❖ “reasonable” here means roughly that we can only observe behaviour and not look directly at the statespace","libVersion":"0.3.2","langs":""}