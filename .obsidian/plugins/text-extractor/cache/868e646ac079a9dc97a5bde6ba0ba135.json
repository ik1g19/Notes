{"path":"Git Ignore/Heavy Stuff/Matthew Barnes Notes/Computer Vision Notes.pdf","text":"Computer Science / Software Engineering Notes Network Computer Vision Joshua Gregory No tes in tro 10 In tro ductio n 10 Lecture 1: Eye an d Human Visio n 12 The human eye 12 Optics 13 Spectral responses 14 Mach bands 15 Neural processing 16 Lecture 2: Image Fo rmatio n 16 Decomposition 16 Resolution 17 Fourier Transform 17 What the Fourier Transform actually does 18 A pulse and its Fourier transform 19 Reconstructing signal from its Fourier Transform 20 Magnitude and phase of the Fourier transform of a pulse 21 Lecture 3: Image Samp lin g 21 Aliasing in Sampled Imagery 21 Aliasing 22 Sampling Signals 22 Wheels Motion 22 Sampling Theory 23 Transform Pair from Sampled Pulse 23 2D Fourier Transform 24 Reconstruction (non examinable) 26 Shift Invariance 26 Computer Science / Software Engineering Notes Network Rotation Invariance 27 Filtering 27 Other Transforms 28 Applications of 2D Fourier Transform 29 Lecture 4: Po in t Op erato rs 29 Image Histograms 29 Brightening and Image 29 Intensity Mappings 30 Exponential/Logarithmic Point Operators (non examinable) 30 Intensity normalisation and histogram equalisation 31 Histogram Equalisation 31 Applying intensity normalisation and histogram equalisation 32 Thresholding and eye image 32 Thresholding:Manual vs Automatic 33 Lecture 5: Gro up Op erato rs 33 Template Convolution 34 3x3 template and weighting coefficients 35 3x3 averaging operator 36 Illustrating the effect of window size 37 Template convolution via the Fourier transform 37 2D Gaussian function 37 2D Gaussian template 39 Applying Gaussian averaging 39 Finding the median from a 3x3 template 39 Newer stuff (non-examinable) 40 Applying non local means 41 Even newer stuff:Image Ray Transform 41 Applying Image Ray Transform 41 Comparing operators 43 Lecture 6: Edge D etectio n 43 Edge detection 43 First order edge detection 43 Edge detection maths 45 Templates for improved first order difference 45 Edge Detection in Vector Format 45 Templates for Prewitt operator 46 Applying the Prewitt Operator 46 Templates for Sobel operator 47 Applying Sobel operator 47 Generalising Sobel 47 Generalised Sobel (non examinable) 48 Computer Science / Software Engineering Notes Network Lecture 7: Further Edge D etectio n 48 Canny edge detection operator 48 Interpolation in non-maximum suppression 49 Hysteresis thresholding transfer function 50 Action of non-maximum suppression and hysteresis thresholding 51 Hysteresis thresholding vs uniform thresholding 51 Canny vs Sobel 51 First and second order edge detection 53 Edge detection via the Laplacian operator 53 Mathbelts onâ€¦ 54 Shape of Laplacian of Gaussian operator 54 Zero crossing detection 55 Marr-Hildreth edge detection 55 Comparison of edge detection operators 55 Newer stuff - interest detections (non-examinable) 56 Newer stuff - saliency 56 Lecture 8: Fin din g Shap es 57 Feature extraction by thresholding 57 Template Matching 57 Template matching in: 58 In Noisy images 58 In Occluded Images 58 Encore, Monsieur Fourier! (???) (non examinable) 59 Applying Template Matching (non examinable) 59 Applying SIFT in ear biometrics (non examinable) 60 Hough Transform 60 Applying the Hough transform for lines 61 Hough Transform for Lines â€¦ problems 61 Images and accumulator space of polar Hough Transform 62 Applying Hough Transform 62 Lecture 9: Fin din g Mo re Shap es 63 Hough Transform for Circles 63 Circle Voting and Accumulator Space 64 Speeding it up 64 Applying the HT for circles 64 Integrodifferential operator? (non examinable) 64 Arbitrary Shapes 65 R-table Construction 65 Active Contours (non examinable) 66 Geometric active contours (non examinable) 67 Parts-based shape modelling (non examinable) 67 Computer Science / Software Engineering Notes Network Symmetry yrtemmyS (non examinable) 68 Lecture 10 Applications/Deep Learning 68 Where is computer vision used? 68 Deep Learning 68 Conclusions 68 Lecture 1: B uildin g machin es that see 71 Key terms in designing Computer Vision systems 71 Robustness 71 Repeatability 71 Invariance 71 Constraints 71 Constraints in Industrial Vision 72 Software Constraints 72 Colour-Spaces (non examinable?) 72 RGB Colour-space 72 HSV Colour-space 72 Physical Constraints 73 Vision in the wild 73 Lecture 2: Machin e learn in g fo r p attern reco gn itio n 73 Feature Spaces 73 Key terminology 74 Density and Similarity 74 Distance in featurespace 74 Euclidean distance (L2 distance) 75 Manhattan/Taxicab distance (L1 distance) 76 Cosine Similarity 76 Choosing good featurevector representations for machine learning 77 Supervised Machine Learning:Classification 77 Linear Classifiers 77 Non-linear binary classifiers 78 Multiclass classifiers:KNN 80 KNN Problems (non examinable?) 80 Unsupervised Machine Learning:Clustering 81 K-Means Clustering 81 Lecture 3: C o varian ce an d Prin cip al C o mp o n en ts 82 Random Variables and Expected Values 82 Variance 82 Covariance 82 Covariance Matrix 83 Mean Centring 83 Covariance matrix again 84 Computer Science / Software Engineering Notes Network Principal axes of variation 85 Basis 85 The first principal axis 85 The second principal axis 85 The third principal axis 85 Eigenvectors and Eigenvalues 86 Important Equation 86 Properties 86 Finding Values 87 Eigendecomposition 87 Summary 87 Ordering 87 Principal Component Analysis 87 Linear Transform 87 Linear Transforms 88 PCA 88 PCA Algorithm 89 Eigenfaces 89 Making Invariant 89 Problems 90 Potential Solutionâ€¦ Apply PCA 90 Lecture 4: Typ es o f image feature an d segmen tatio n 91 Image Feature Morphology 91 Global Features 91 Grid or Block-based Features 91 Region-based Features 92 Local Features 92 Global Features 92 Image Histograms 92 Joint-colour histogram 93 Image Segmentation 94 What is segmentation? 94 Global Binary Thresholding 94 Otsuâ€™s thresholding method 94 Adaptive / local thresholding 95 Mean adaptive thresholding 95 Segmentation with K-Means 95 Advanced segmentation techniques 96 Connected Components 96 Pixel Connectivity 96 Connected Component 96 Computer Science / Software Engineering Notes Network Connected Component Labelling 96 The two-pass algorithm 97 Lecture 5: Shap e descrip tio n an d mo dellin g 98 Extracting features from shapes represented by connected components 98 Borders 98 Inner Border 98 Outer Border 98 Two ways to describe shape 99 Region Description:Simple Scalar Shape Features 100 Area and Perimeter 100 Compactness 100 Centre of Mass 100 Irregularity / Dispersion 101 Moments 102 Standard Moments 102 Central Moments 102 Normalised Central Moments 103 Boundary Description 103 Chain Codes 103 Chain Code Invariance 103 Chain Code Advantages and Limitations 104 Fourier Descriptors 104 Region Adjacency Graphs 105 Active Shape Models and Constrained Local Models 105 Lecture 6: Lo cal in terest p o in ts 105 What makes a good interest point? 105 How to find interest points 106 The Harris and Stephens corner detector 107 Basic Idea 107 Harris & Stephens:Mathematics 107 Structure Tensor 108 Eigenvalues of the Structure Tensor 109 Harris & Stephens Response Function 109 Harris & Stephens Detector 110 Scale in Computer Vision 111 The problem of scale 111 Scale space theory 111 The Gaussian Scale Space 112 Nyquist-Shannon Sampling theorem 112 Gaussian Pyramid 113 Multi-scale Harris & Stephens 113 Computer Science / Software Engineering Notes Network Blob Detection Finally 113 Laplacian of Gaussian 113 Scale space LoG 114 Scale space DoG 115 DoG Pyramid 115 Lecture 7: Lo cal features an d matchin g 116 Local features and matching basics 116 Local Features 116 Why extract local features? 116 Example:Building a panorama 116 Problem 1: 116 Problem 2: 117 Two distinct types of matching problem 117 Narrow-baseline stereo 117 Wide-baseline stereo 117 Two distinct types of matching problem 118 Robust local description 118 Descriptor Requirements 118 Matching by correlation (template matching) 118 (Narrow baseline) template matching 118 Problems with wider baselines 120 Local Intensity Histograms 120 Use local histograms instead of pixel patches 120 Local histograms 120 Overcoming localisation sensitivity 121 Overcoming lack of illumination invariance 121 Local Gradient Histograms 121 Gradient Magnitudes and Directions 121 Gradient Histograms 121 Building gradient histograms 121 Rotation Invariance 121 The SIFT feature 122 Adding spatial awareness 122 SIFT Construction:sampling 123 SIFT Construction:weighting 123 SIFT Construction:binning 123 Matching SIFT features 124 Euclidean Matching 124 Improving matching performance 124 Lecture 8: C o n sisten t matchin g 124 Feature distinctiveness 124 Computer Science / Software Engineering Notes Network Constrained matching 125 Geometric Mappings 125 What are geometric transforms? 125 Point Transforms 125 The Affine Transform 125 Translation 126 Translation and Rotation 126 Scaling 126 Aspect Ratio 126 Shear 126 Degrees of Freedom 126 Affine Transform 129 Similarity Transform 130 More degrees of freedom 130 Homogeneous coordinates 131 The Planar Homography (Projective Transformation) 131 Recovering a geometric mapping 131 Simultaneous equations 131 Least-squares 132 Robust Estimation 132 Problem:Noisy data 133 Robust estimation techniques 133 RANSAC:RANdom SAmple Consensus 133 Further applications of robust local matching 134 Object recognition & AR 134 3D reconstruction 134 Problems with direct local feature matching 134 Local feature matching is slow! 134 Efficient Nearest Neighbour Search 134 K-D Trees 134 K-D Tree problems 135 Hashing 137 Sketching 138 Lecture 9: Image search an d B ags o f Visual Wo rds 138 Text Information Retrieval 138 The bag data structure 138 Bag of Words 138 Text processing (feature extraction) 139 The Vector-Space Model 139 Bag of Words Vectors 140 The Vector-space Model 140 Computer Science / Software Engineering Notes Network Searching the VSM 141 Recap:Cosine Similarity 141 Inverted Indexes 141 Computing the Cosine Similarity 142 Weighting the vectors 142 Possible weighting schemes 143 Vector Quantisation 143 Learning a Vector Quantiser 143 Vector Quantisation 143 Visual Words 144 SIFT Visual Words 144 Bags of Visual Words 144 Histograms of Bags of Visual Words 145 Visualising Visual Words 146 The effect of codebook size 146 Content-based Image Retrieval 147 BoVW Retrieval 147 Optimal codebook size 147 Problems with big codebooks 147 Overall process for building a BoVW retrieval system 148 Lecture 10: Image classificatio n an d auto -an n o tatio n 148 Multilabel classification 148 Object Detection / Localisation 149 Slide summary:Challenges in Computer Vision 149 Aside:Optimal codebook size 150 Another slide summary:Stuff 150 Dense Local Image Patches 150 Dense SIFT 150 Pyramid Dense SIFT 151 Spatial Pyramids 152 Developing and benchmarking a BoVW scene classifier 152 Evaluation Dataset 152 Building the BoVW 153 Training classifiers 153 Classifying the test set 153 Evaluating Performance 153 Lecture 11: To wards 3D visio n 153 Summary Summary 154 Pro grammin g fo r co mp uter visio n & o ther musin gs related to the co ursewo rk 155 Writing code for computer vision 155 Most vision algorithms are continuous 155 Computer Science / Software Engineering Notes Network Always work with floating point pixels 155 Guidelines for writing vision code 155 Convolution 155 Aside:phase and magnitude 156 Aside:Displaying FFTs 156 Template Convolution 157 What if you donâ€™t flip the kernel? 158 Ideal Low-Pass filter 158 Ideal Low-Pass filter - problems 159 Gaussian filters - why 160 Building Gaussian Filters 160 High-pass filters 161 Note - Donâ€™t do this! 161 High-pass filters have a mixture of negative and positive coefficients 162 Building hybrid images 162 â€¦is really simple 162 TL;D R 163 Part 1:Mark 164 Part 2:Jon 164 Lec 1 164 Lec 2 164 Lec 3 165 Lec 4 167 Lec 5 169 Lec 6 172 Lec 7 174 Lec 8 176 Lec 9 178 Lec 10 180 TL;DR By Mark Towers 182 Mark (edited by Joshua Gregory) 182 Jon 187 Notes intro â— Basically this is a summary of the slides, plus some extra explanation. â— Text which is coloured dark yellow:â€œlike thisâ€, is content from the slides I have decided to include as I think it is probably a bit important/interesting, even though there was no hand with bow for it: Computer Science / Software Engineering Notes Network Introduction Images consist of picture elements known as â€œpixelsâ€. ğ‘‚ ğ‘¥,ğ‘¦ = ğ‘“( ğ‘¥, ğ‘¦, ğ‘§, Î», ğ‘¡) â— (x,y) - location â— z - depth â— - colourÎ» â— T - time 2D Images are matrices of numbers. â— Grey level image â— 3D view â— Corresponding Matrix Point Operations: â— Recalculate point values â—‹ Modify Brightness â—‹ Find Intensity Group Operations: â— Process neighbourhoods â—‹ Image filtering â—‹ Edge detection Feature Extraction â— Finds shapes â—‹ Roads in remotely-sensed image â—‹ Artery in ultrasound image Applications of Computer Vision â¢ Image coding (MPEG/JPEG) â¢ Product inspection â¢ Robotics â¢ Modern cameras/phones â¢ Medical imaging â¢ Demography â¢ Biometrics Computer Science / Software Engineering Notes Network Part 1:Mark Lecture 1:Eye and Human Vision Is human vision a good model for computer vision? The human eye Diagram of the human eye. Light comes in through the lens and is focused onto the retina. These light impulses are then sent through the optic nerve. Cones are photopic (Coloured vision) and Rods are scotopic (Black and white) (Nixon made a mistake on his notes) - Thanks Tim :) Computer Science / Software Engineering Notes Network Optics Image is flipped, just like a camera. Our brain works this out and we perceive it the right way up. Computer Science / Software Engineering Notes Network Spectral responses The eye is most sensitive to green light, least to blue, but the point is that colours are at different response levels. Computer Science / Software Engineering Notes Network Mach bands Mach bands are an optical illusion whereby the contrast between edges of slightly differing shades of grey is exaggerated when they are touching. [Source:Wikipedia] This and many other optical illusions (edges, static, benhamâ€™s disk... you can look at in the slides) demonstrate that our eyes can actually be not so good as a vision system. Computer Science / Software Engineering Notes Network Neural processing Lecture 2:Image Formation What is inside an image? Decomposition You can decompose an image into its bits. â— For a greyscale image, each pixel in the image is usually represented by 8 bits;a byte. â— The number value ranges from 0-255, which represents all the greyscale colours from white to black. Computer Science / Software Engineering Notes Network â— When you decompose an image, you are only looking at the one bit in each pixel, and this is either a 0 or 1, so either black or white. â— So for bit 0, only the first bit of each pixel is being shown in the image. As you can see this bit represents the least amount of information and is very noisy. â— As you look at the images increasing in the bit position, you can see more information is represented. â— The main point here is that the image gets clearer as you go up to the most significant bit (bit 7). Resolution Images are also (obviously) less detailed at lower resolutions. Fourier Transform Any periodic function is the result of adding up sine and cosine waves of different frequencies. [This is a good visual intuitive:https://www.youtube.com/watch?v=spUNpyF58BY] â— is the Fourier transformğ¹ğ‘ ( Ï‰) â— denotes the Fourier transform process: â— is the angular frequency, measure in radians/s (where frequency is theÏ‰ Ï‰ = 2 Ï€ğ‘“ ğ‘“ reciprocal of time , )ğ‘¡ ğ‘“ = 1 /ğ‘¡ â— is the complex variable (usually )ğ‘— ğ‘— = âˆ’ 1 ğ‘– â— is a continuous signal (varying continuously with time)ğ‘( ğ‘¡) â— gives the frequency components inğ‘’ âˆ’ ğ‘—Ï‰ğ‘¡ = ğ‘ğ‘œğ‘ ( Ï‰ğ‘¡) âˆ’ ğ‘—ğ‘ ğ‘–ğ‘›( Ï‰ğ‘¡) ğ‘( ğ‘¡) Computer Science / Software Engineering Notes Network What the Fourier Transform actually does (To do:explain how a function that deals with (sound) waves can be used on images with pixels. If anyone reading can explain thatâ€™d be great) A:The fourier transform is just a technique for mapping signals that vary in one dimension to the frequency domain of that dimension. In the case of audio this means transforming air pressure from the time domain to the (temporal) frequency domain. If we consider the intensity of an image to be our signal, which varies in space (let's say along the x axis), then the FT will transform the signal into spatial frequency in the x dimension. Now, we can generalise the fourier transform to take signals that are defined in more than one mutually orthogonal dimensions, e.g. an image represented by pixel intensity defined in x and y. Now the FT transforms the image into the (spatial) frequency domain, defined in two dimensions (spatial frequency in x, and spatial frequency in the y direction). Hope that helps? - James â— Pulse Computer Science / Software Engineering Notes Network â— Use Fourier â— Evaluate integral â— And get result A pulse and its Fourier transform Computer Science / Software Engineering Notes Network Reconstructing signal from its Fourier Transform Computer Science / Software Engineering Notes Network Magnitude and phase of the Fourier transform of a pulse Re() is the real function and Im() is the imaginary function so for expression then5 + 2 ğ‘– and . In the complex plane, you can imagine theğ‘…ğ‘’( 5 + 2 ğ‘–) = 5 ğ¼ğ‘š( 5 + 2 ğ‘–) = 2 imaginary numbers as another number line with the real number as the other number. So to calculate the magnitude, it is Pythagorasâ€™ theorem of the two numbers while the angle is the angle of the two numbers. For more information, look up complex numbers. (Thanks Mark ;) ) The magnitude is the scalar amount of the frequency and the phase is its offset (from a regular sin wave) (-Alex) Computer Science / Software Engineering Notes Network Lecture 3:Image Sampling How is an image sampled and what does it imply? Aliasing in Sampled Imagery â— As you can see the left image looks clearer as it is at a higher resolution, and therefore literally has more information than lower resolutions. â— The low resolution image on the right is aliased, and appears a bit blurry to the left image. â— This is probably a result of a low sampling rate. Aliasing Aliasing is an effect that causes different signals to become indistinguishable when sampled. It also often refers to the distortion or artifact that results when a signal reconstructed from samples is different from the original continuous signal. [-Wiki] Computer Science / Software Engineering Notes Network Sampling Signals â— The original signal is a continuous function. We want to sample this, and have to do it digitally. â— If we sample at a good (high) rate, then we can capture a good representation of the signal. â— If we sample at a bad (low) rate, then this captured signal become aliased, and is not a good representation. â— As you can see, the signal wave is completely the wrong representation from the original image. Wheels Motion â— Have you ever seen a wheel (or propeller) turn sooooooooooo fast in a video (or even real life) that it looks like itâ€™s not turning at all!? â— Or even turning the wrong way!?!!??!?? â— This is due to the sampling rate the wheel is captured at. â— The wheel is spinning faster or slower to (a multiple) of the sampling rate. â— This can cause the illusion of standing or reverse turning. â— Look up some videos or something to see this in action. Computer Science / Software Engineering Notes Network â— The point is the sampling rate matters in vision. Sampling Theory â— Nyquistâ€™s sampling theorem is 1D. â— E.g. speech 6kHz, sample at 12 kHz. â— Video bandwidth (CCIR) is 5 MHz. â— Sampling at 10 MHz gave 576x576 images. â— Guideline:â€œtwo pixels for every pixel of interestâ€. Transform Pair from Sampled Pulse Computer Science / Software Engineering Notes Network Some stuff to do with the Fourier Transformâ€¦ (If anyone could explain what these graphs are demonstrating thatâ€™d be epic) A:Itâ€™s showing the effect of adding reconstructing the sampled signal from its frequency components. However, the images for (b) and (c) are the wrong way around. Essentially itâ€™s trying to demonstrate that as you add more frequency components from the transform of the sampled signal, the reconstruction becomes a better approximation of the sampled signal, until eventually (f) all the frequency components are present and the reconstruction is perfect [(a) and (f) are identical]. - James 2D Fourier Transform â— The Fourier Transform has a fo rward and in verse function. â— The fo rward transform: â—‹ Where there are â–  Two dimensions of space, x and y â–  Two dimensions of frequency, u and v â–  Image NxN pixels Px,y â— The in verse transform: â— â—‹ Lol Computer Science / Software Engineering Notes Network Reconstruction (non examinable) Shift Invariance â— Images can be shifted. â— What happens to the magnitude and phase of the fourier transform from shifted images. Computer Science / Software Engineering Notes Network â— This shows that the magnitude is not affected by shifting. â— But the phase is affected by shifting. â— What about for rotation? Rotation Invariance â— As you can see, the transform is just rotated too. Filtering â— Fourier gives access to frequency components. Computer Science / Software Engineering Notes Network â— â—‹ Original Image â— â—‹ Transforms Other Transforms Applications of 2D Fourier Transform â— Understanding and analysis Computer Science / Software Engineering Notes Network â— Speeding up algorithms â— Representation (invariance) â— Coding â— Recognition / understanding (e.g. texture) Lecture 4:Point Operators How many different operators are there which operate on image points? Image Histograms â— A histogram is a graph of frequency. â— In this context it is showing how many pixels have a certain brightness level. â— This data is a global feature. Brightening an Image â— An Image can be brightened by using this formula: â— â— Where: â—‹ N - new image â—‹ O - old image â—‹ k - gain â—‹ l - level â—‹ x,y - coordinates Computer Science / Software Engineering Notes Network Intensity Mappings Exponential/Logarithmic Point Operators (non examinable) Intensity Normalisation Computer Science / Software Engineering Notes Network â— â—‹ N - new image â—‹ O - old image â—‹ x,y - coordinates â—‹ Nmax - maximum input â—‹ Nmin - minimum input â—‹ Omax - maximum output â—‹ Omin - minimum output â— Avoids need for parameter choice Intensity normalisation and histogram equalisation Computer Science / Software Engineering Notes Network Histogram Equalisation â— N 2 points in the image;the sum of points per level is equal â— Cumulative histogram up to level p should be transformed to cover up to the level q â— Number of points per level in the output picture â— Cumulative histogram of the output picture â— Mapping for the output pixels at level q Applying intensity normalisation and histogram equalisation Computer Science / Software Engineering Notes Network Thresholding and eye image Thresholding:Manual vs Automatic Other thresholding includes Entropic thresholding and Optimal Thresholding Computer Science / Software Engineering Notes Network Lecture 5:Group Operators How do we combine points to make a new point in a new image? Template Convolution Iâ€™m sure everyone knows about this by nowâ€¦ This is when you take an original image and then apply a convolution template over every pixel to create a resultant pixel value. Computer Science / Software Engineering Notes Network (to do:more explanationâ€¦) â— Convolution is a system response â—‹ â— Template convolution includes coordinate inversion in x and in y â—‹ â— Inversion is not needed if the template is symmetric Computer Science / Software Engineering Notes Network 3x3 template and weighting coefficients Where wi,j are the weights and x(i), y(j) denote the position of the point that matches the weighting coefficient position 3x3 averaging operator Computer Science / Software Engineering Notes Network Illustrating the effect of window size Larger windows lead to more blur. Template convolution via the Fourier transform Allows for fast computation for template sizes >= 7x7 â— Template convolution:* â— Fourier transform of the picture: â— Fourier transform of the template: â— Point by point multiplication (.Ã—) Note:itâ€™s point by point! The equation is wrongly written in various places. Computer Science / Software Engineering Notes Network 2D Gaussian function â— Used to calculate template values â— Note compromise between variance ÏƒÂ² and window size â— Common choices â—‹ 5x5, 1.0 â—‹ 7x7, 1.2 â—‹ 9x9, 1.4 Computer Science / Software Engineering Notes Network 2D Gaussian template Applying Gaussian averaging Finding the median from a 3x3 template â— Preserves edges â— Removes salt and pepper noise Computer Science / Software Engineering Notes Network Newer stuff (non-examinable) Averaging which preserves regions Computer Science / Software Engineering Notes Network Applying non local means Even newer stuff:Image Ray Transform Use analogy to light to find shapes, removing the remainder Computer Science / Software Engineering Notes Network Applying Image Ray Transform Computer Science / Software Engineering Notes Network Comparing operators Lecture 6:Edge Detection What are edges and how do we find them? Edge detection â— Images have edges in them. â— Itâ€™s a very useful thing to find these edges in computer vision. â— There are a few different operators for edge detection of an image. â— It involves maths. Computer Science / Software Engineering Notes Network First order edge detection â— You can detect vertical and horizontal separately. â— â— Vertical edges, Ex â— Horizontal edges, Ey â— Vertical and horizontal edges: â— Template: â— Code: Computer Science / Software Engineering Notes Network Edge detection maths â— Taylor expansion for â— By rearrangement â— This is equivalent to â— Expand Templates for improved first order difference Computer Science / Software Engineering Notes Network Edge Detection in Vector Format Templates for Prewitt operator Applying the Prewitt Operator Computer Science / Software Engineering Notes Network Templates for Sobel operator â— The two kernels used for the 3 x 3 sobel operator. Applying Sobel operator Generalising Sobel â— Averaging â— Differencing Computer Science / Software Engineering Notes Network Generalised Sobel (non examinable) Lecture 7:Further Edge Detection What better ways are there to detect edges? Canny edge detection operator Formulated with three main objectives: â— Optimal detection with no spurious responses â— Good localisation with minimal distance between detected and true edge position â— Single response to eliminate multiple responses to a single edge. Computer Science / Software Engineering Notes Network Approximation 1. Use Gaussian smoothing 2. Use the Sobel operator (could combine with 1?) 3. Use non-maximal suppression 4. Threshold with hysteresis to connect edge points Interpolation in non-maximum suppression â— Sobel edge detection is first order (equivalent to differentiation) so gives us the change in x and the change in y, which we turn into a vector and use to find the gradient direction (the dotted line in the diagram above), which will intuitively be at right angles to the direction of the line (the non-dotted line above). â— In non-maximum suppression, we set all points that arenâ€™t a maximum along the line to zero (thereby thinning and sharpening the line). â—‹ This is done by taking a line of points along the gradient (dotted line) and setting all the points to zero, except those at a maximum. â—‹ Interpolation is used to figure out the values of points between pixels. â—‹ Alternatively, we can round the direction to the nearest horizontal/vertical/diagonal line (Thanks Samuel Collins) Computer Science / Software Engineering Notes Network Hysteresis thresholding transfer function To help better understand the function an example: â— The level is currently at black â— Brightness is increased until it reaches and passes the â€œupper switching thresholdâ€ â— The level now changes to white â— If the brightness is decreased and goes below the â€œupper switching thresholdâ€ it will not switch to black â— The brightness will have to decrease until it is below the â€œlower switching thresholdâ€, then it will switch back to black. Computer Science / Software Engineering Notes Network Action of non-maximum suppression and hysteresis thresholding Hysteresis thresholding vs uniform thresholding As you can see, hysteresis thresholding is arguably better. Computer Science / Software Engineering Notes Network Canny vs Sobel Computer Science / Software Engineering Notes Network First and second order edge detection Edge detection via the Laplacian operator Computer Science / Software Engineering Notes Network Mathbelts onâ€¦ (I think this is essentially just rearranging an equation) Shape of Laplacian of Gaussian operator Computer Science / Software Engineering Notes Network Zero crossing detection â— Basic - straight comparison â— Advanced: â— You could compare every point to try and find where a 1 switches to 0. â—‹ Thatâ€™s not that smart. â— So you average the 4 points in the corners â—‹ Then you get 4 summations â—‹ If one of those is positive and another is negative, then there is a zero crossing. Marr-Hildreth edge detection â— Application of LoG (or DoG) and zero-crossing detection. Computer Science / Software Engineering Notes Network Comparison of edge detection operators Newer stuff - interest detections (non-examinable) Computer Science / Software Engineering Notes Network Newer stuff - saliency Lecture 8:Finding Shapes How can we group points to find shapes? Feature extraction by thresholding â— Letâ€™s try to extract features from this image using thresholding. â— Low threshold doesn't look that good. â— Neither does the high threshold. â— In conclusion:we need to identify shape! Template Matching â— This is a technique for finding small parts of an image which match a template image. â— Intuitively simple â— Correlation and convolution â— Implementation via Fourier â— Relationship with matched filter, viz:optimality Computer Science / Software Engineering Notes Network â— The template is the silverstone sign. â— This can be found in the image with template matching â— The accumulator space is a thing Template matching in: In Noisy images â— Noise is an issue for template matching â— You want to create a matcher which is robust to noise Computer Science / Software Engineering Notes Network In Occluded Images Encore, Monsieur Fourier! (???) (non examinable) Applying Template Matching (non examinable) Here, have this pretty low res image of a reaaaaally realistic weapon identification system, which can detect weapons like guns and knives. (aka, this is an application of template matching.) Computer Science / Software Engineering Notes Network Windows 2000 anyone? Applying SIFT in ear biometrics (non examinable) â— Have you ever wanted to identify people by their ears? â— Well now you can! â— Do this stuff with the arrows and circles and stuff Hough Transform â— This is another feature extraction technique â— It can find imperfect instances of objects within a certain class of shapes by a voting procedure â— The voting procedure is carried out in a parameter space, from which object candidates are obtained as local maxima in a so-called accumulator space. Computer Science / Software Engineering Notes Network â— [-Wiki] â— Performance equivalent to template matching, but faster â— A line is points x,y gradient is m intercept is c. â—‹ ğ‘¦ = ğ‘šğ‘¥ + ğ‘ â— You can rearrange to get: â—‹ ğ‘ = âˆ’ ğ‘¥ğ‘š + ğ‘¦ â— In maths itâ€™s the principle of duality Go and read the following article about the Hough Transform: http://aishack.in/tutorials/hough-transform-basics/ Itâ€™s really good. Applying the Hough transform for lines Hough Transform for Lines â€¦ problems â— m, c tend to infinity â— Change the parameterisation â— Use foot of normal Ï = ğ‘¥ğ‘ğ‘œğ‘ Î¸ + ğ‘¦ğ‘ ğ‘–ğ‘›Î¸ â— Gives polar HT for lines Computer Science / Software Engineering Notes Network Images and accumulator space of polar Hough Transform Computer Science / Software Engineering Notes Network Applying Hough Transform Lecture 9:Finding More Shapes How can we go from conic sections to general shapes? Hough Transform for Circles â— Again, itâ€™s duality: â— Equation of a circle:( ğ‘¥ âˆ’ ğ‘¥ 0 ) 2 + ( ğ‘¦ âˆ’ ğ‘¦ 0 ) 2 = ğ‘Ÿ 2 Points Parameters Radius x,y x0,y0 r x0,y0 x,y r Computer Science / Software Engineering Notes Network Circle Voting and Accumulator Space Like before, but this time for circles. Speeding it up â— Now itâ€™s a 3D accumulator, fast algorithms are available â— E.g. by differentiation: ğ‘‘ğ‘¦ ğ‘‘ğ‘¦ = âˆ’ ( ğ‘¥âˆ’ ğ‘¥ 0 ) ( ğ‘¦âˆ’ ğ‘¦ 0 ) â— SO edge gradient direction can be used, e.g. 2D accumulator by: ( ğ‘‘ğ‘¦ ğ‘‘ğ‘¥ ) 2 ( ğ‘¦ âˆ’ ğ‘¦0 ) 2 + ( ğ‘¦ âˆ’ ğ‘¦0 ) 2 = ğ‘Ÿ2 Computer Science / Software Engineering Notes Network Applying the HT for circles This can be used for detecting the shapes of an eye and iris. Integrodifferential operator? (non examinable) https://stackoverflow.com/questions/27058057/comparing-irises-images-with-opencv Looks cool??? Arbitrary Shapes â— Use Generalised HT â— Form (discrete) look-up-table (R-table) â— Vote via look-up-table Computer Science / Software Engineering Notes Network â— Orientation? Rotate R-table voting â— Scale? Scale R-table voting â— Inherent problems with discretisation (process of transferring continuous functions to discrete) R-table Construction Pick a reference point in the image. For each boundary point x, compute \\Phi(x) - the gradient direction. r is the distance from the reference point to the boundary point (radial distance), and \\alpha is the angle. The table then stores each (r, \\alpha) pair with the corresponding \\Phi. (Thanks Bradley Garrod) Active Contours (non examinable) â— For unknown arbitrary shapes:extract by evolution â— Elastic band analogy â— Balloon analogy â— Discrete vs. continuous â— Volcanoes? ğŸŒ‹ Computer Science / Software Engineering Notes Network Geometric active contours (non examinable) Coupleâ€™a hippos ğŸ¦›ğŸ¦› Parts-based shape modelling (non examinable) This guy donâ€™t look so good Computer Science / Software Engineering Notes Network Symmetry yrtemmyS (non examinable) SPooky Lecture 10 Applications/Deep Learning Where is feature extraction used these days? This lecture is mostly extra (non examinable) stuff, so Iâ€™m only gonna cover the hand with bow slides. If you want a look then jump to the slides: http://comp3204.ecs.soton.ac.uk/mark/Lecture%2010.pdf Where is computer vision used? What you see depends on the viewpoint you take Computer Science / Software Engineering Notes NetworkComputer Science / Software Engineering Notes Network Deep Learning Conclusions â— Computer vision is changing the way we live â— Computer vision uses modern hardware and modern cameras to achieve what we understand by â€œsightâ€ â— No technique is a panacea (a solution for all difficulties):many alternatives exist â— Computer vision is larger than this course Computer Science / Software Engineering Notes Network Part 2:Jon Note:Jon has done some really good handout summaries which have basically done my job for me, they can be found here:http://comp3204.ecs.soton.ac.uk/part2.html Highly recommend reading these probably. But I will still go through the slides and make notes of hand with bow slides :) Lecture 1:Building machines that see Key terms in designing Computer Vision systems 1. Robust 2. Repeatable 3. Invariant 4. Constraints â— You want your system to be robust and repeatable. â— You design your system to be invariant. â— You apply constraints to make it work. Robustness â— The vision system must be ro bust to changes in its environment â—‹ i.e. changes in lighting;angle or position of the camera;etc Repeatability â— Rep eatability is a measure of robustness â— Repeatability means that the system must work the same over and over, regardless of environmental changes Invariance â— In varian ce to environmental factor helps achieve robustness and repeatability â—‹ Hardware and software can be designed to be invariant to certain environmental changes â–  e.g. you could design an algorithm to be invariant to illumination changesâ€¦ Constraints â— C o n strain ts are what you apply to the hardware, software and wetware (human brains in the system) to make sure your computer vision system works in a repeatable, robust fashion. Computer Science / Software Engineering Notes Network â—‹ e.g. you constrain the system by putting it in a box so there canâ€™t be any illumination changes Constraints in Industrial Vision Software Constraints â— Really simp le, but in credibly fast algorithms â—‹ Hough Transform is popular, but note that it isnâ€™t all that robust without physical constraints â–  Actually, the same is true of most algorithms/techniques used in industrial vision â—‹ Intelligent use of colourâ€¦ Colour-Spaces (non examinable?) Even though there is no hand with bow for these slides, they seem useful. â— There are many different ways of numerically representing colour â—‹ A single representation of all possible colours is called a colour-space â—‹ Itâ€™s generally possible to convert to one colour-space to another by applying a mapping (in the form of a set of equations or an algorithm) RGB Colour-space â— Most Physical image sensors capture RGB â—‹ By far the most widely known space â—‹ RGB â€œcouplesâ€ brightness (luminance) with each channel, meaning that the illumination invariance is difficult â— HSV Colour-space â— Hue, Saturation, Value is another colour-space â—‹ Hue encodes the pure colour as an angle â–  Red == 0Â° == 360Â° Computer Science / Software Engineering Notes Network â—‹ Saturation is how vibrant the colour is â—‹ And the Value encodes brightness â— A simple way of achieving invariance to lightning is to use just the H or H & S components LAB Colour-space Lab color space is more perceptually lin ear than other color spaces. Perceptually linear means that a change of the same amount in a color value should produce a change of about the same visual importance. It is important especially when you try to measure the perceptual difference of two colors. (Source) â— Can also be made invariant to lighting by only using the AB components. Physical Constraints â— Industrial vision is usually solved by applying simple computer vision algorithms, and lots of physical constraints: â—‹ Environment:lighting, enclosure, mounting â—‹ Acquisition hardware:expensive camera, optics, filters Vision in the wild â— So, what about vision systems in the wild, like ANPR (Automatic Number-Plate Recognition) cameras, or recognition apps for mobile phones? â—‹ Apply as many hardware and wetware constraints as possible, and let the software take up the slack â—‹ Colour information often less important than luminance Lecture 2:Machine learning for pattern recognition Feature Spaces Many computer vision applications involving machine learning take the following form Computer Science / Software Engineering Notes Network 1. This is where cool image processing happens 2. Feature extractors make feature vectors from images 3. Machine learning system uses featurevectors to make intelligent decisions Key terminology â— Featurevecto r:a mathematical vector â—‹ Just a list of (usually Real) numbers â—‹ Has a fixed number of elemen ts in it â–  The number of elements is the dimen sio n ality of the vector â—‹ Represents a p o in t in a featuresp ace or equally a directio n in the featurespace â—‹ The dimen sio n ality o f a featuresp ace is the dimensionality of every vector within it â–  Vectors of differing dimensionality canâ€™t exist in the same featurespace Density and Similarity Distance in featurespace â— Feature extractors are often defined so that they produce vectors that are close together for similar inputs â—‹ Closeness of two vectors can be computed in the feature space by measuring the distance between the vectors. Computer Science / Software Engineering Notes Network Cats are a close distance apart, and are further away from the cluster of dogs in this feature space. Euclidean distance (L2 distance) â— L2 distance is the most intuitive distanceâ€¦ â—‹ The straight-line distance between two points â—‹ Computed via an extension of Pythagoras theorem to n dimensions: Equation Computer Science / Software Engineering Notes Network The straight-line;â€œEuclidean distanceâ€ Manhattan/Taxicab distance (L1 distance) â— L1 distance is computed along paths parallel to the axes of the space: Equation Essentially just like you are taking a taxi cab around the grid like streets of manhattan Cosine Similarity â— Cosine similarity measures the cosine of the angle between two vectors â—‹ It is n o t a distan ce! â— Useful if you donâ€™t care about the relative length of the vectors Equation Computer Science / Software Engineering Notes Network The angle between the two points from the origin Choosing good featurevector representations for machine learning â— Choose features which allow to distinguish objects or classes of interest â—‹ Similar within classes â—‹ Different between classes â— Keep number of features small â—‹ Machine-learning can ge t more difficult as dimensionality of featurespace gets large Supervised Machine Learning:Classification â— C lassificatio n is the process of assigning a class label to an object (typically represented by a vector in a feature space). â— A sup ervised machin e-learn in g algo rithm uses a set of pre-labelled training data to learn how to assign class labels to vectors (and the corresponding objects). â—‹ A bin ary classifier only has two classes â—‹ A multiclass classifier has many classes Linear Classifiers Linear classifiers try to learn a hyp erp lan e that separates two classes in featurespace with min imum erro r Computer Science / Software Engineering Notes Network There can be lots of hyperplanes to choose from;differing classification algorithms apply differing constraints when learning the classifier. To classify a new image, you just need to check what side of the hyperplane it is on. Non-linear binary classifiers Linear classifiers work best when the data is linearly separable. Like this: Computer Science / Software Engineering Notes Network But what if the data is like this: There is no hope for a linear classifier! ğŸ˜­ Non-linear binary classifiers, such as Kern el sup p o rt Vecto r Machin es learn non-linear decision boundaries. (basically a curved graph separates the data instead of a straight one) Computer Science / Software Engineering Notes Network However, you have to be careful, you might lose generality by overfitting: What class would the blue question mark actually belong to? Multiclass classifiers:KNN Assign class of unknown point based on majority class of closest K neighbours in featurespace. Computer Science / Software Engineering Notes Network KNN Problems (non examinable?) â— Computationally expensive if there are: â—‹ Lots of training examples â—‹ Many dimensions Unsupervised Machine Learning:Clustering â— Clustering aims to group data without any prior knowledge of what the groups should look like or contain. â— In terms of featurevectors, items with similar vectors should be grouped together by a clustering operation. â— Some clustering operations create overlapping groups;for now weâ€™re only interested in disjoint clustering methods that assign an item to a single group. K-Means Clustering StatQuest:K-means clustering (good video to explain this!!!!!!!!!!!!!!!!) â— K-Means is a classic featurespace clustering algorithm for grouping data in K groups with each group represented by a centroid: â— Pseudo code: â—‹ T h e v a l u e o f K i s c h o s e n â—‹ K i n i t i a l c l u s t e r c e n t r e s a r e c h o s e n Computer Science / Software Engineering Notes Network â—‹ T h e f o l l o w i n g p r o c e s s i s p e r f o r m e d i t e r a t i v e l y u n t i l t h e c e n t r o i d s d o n â€™ t m o v e b e t w e e n i t e r a t i o n s : â–  E a c h p o i n t i s a s s i g n e d t o i t s c l o s e s t c e n t r o i d â–  T h e c e n t r o i d i s r e c o m p u t e d a s t h e m e a n o f a l l t h e p o i n t s a s s i g n e d t o i t . I f t h e c e n t r o i d h a s n o p o i n t s a s s i g n e d i t i s r a n d o m l y r e - i n i t i a l i s e d t o a n e w p o i n t . â—‹ T h e f i n a l c l u s t e r s a r e c r e a t e d b y a s s i g n i n g a l l p o i n t s t o t h e i r n e a r e s t c e n t r o i d . Lecture 3:Covariance and Principal Components Random Variables and Expected Values â— Random variables â—‹ Variable that takes on different values due to chance â— Expected values â—‹ The expected value (denoted E[X]) is the most likely value a random variable will take. Variance â— Variance (Ïƒ 2) is the mean squared difference from the mean (Î¼). â— Itâ€™s a measure of how spread-out the data is. Equation Technically itâ€™s E[(X - E[X]) 2] Covariance â— Covariance (Ïƒ(x,y)) measures how two variables change together â— The variance is the covariance when the two variables are the same (Ïƒ(x,y)=Ïƒ 2(x)) â— A covariance of 0 means the variables are uncorrelated (Covariance is related to Correlation thoughâ€¦) Computer Science / Software Engineering Notes Network Equation Technically itâ€™s E[(x - E[x]) (y - E[y]) ] Covariance Matrix â— A covariance matrix encodes how all possible pairs of dimensions in an n-dimensional dataset vary together. The covariance matrix is a sq uare symmetric matrix, as you can see by the symmetry in the example above. Mean Centring â— Mean Centring is the process of computing the mean (across each dimension independently) of a set of vectors, and then subtracting the mean vector from every vector in the set. â—‹ All the vectors will be translated so their average positions is the origin. Computer Science / Software Engineering Notes Network From top image to bottom image;mean centered around the origin. Covariance matrix again So then this means that The covariance matrix is directly proportional to Z transposed, multiplied by Z, where Z is the matrix formed by the mean-centred vectors (each row of matrix Z is one mean-centred vector) - Hope this helps :) - Lorena Computer Science / Software Engineering Notes Network Principal axes of variation Basis â— A basis is a set of n lin early in dep en den t (remember all the way back to first year Foundations guys!) vectors in an n dimensional space â—‹ The vectors are o rtho go n al (all right-angles to each other) â—‹ They form a â€œcoordinate systemâ€ â—‹ There are an infinite number of possible bases The first principal axis â— For a given set of n dimensional data, the first p rin cip al axis (or just p rin cip al axis) is the vector that describes the direction of greatest variance. â— (Big turquoise arrow pointing up and to the right in the image below) The second principal axis â— The seco n d p rin cip al axis is a vector in the direction of the greatest variance orthogonal (perpendicular) to the first major axis. â— (Small lilac arrow pointing up to the left in the image below) Computer Science / Software Engineering Notes Network The third principal axis â— In a space with 3 or more dimensions, the third principal axis is the direction of greatest variance orthogonal to both the first and second principal axes. â—‹ The fourthâ€¦ and so onâ€¦ â—‹ The set of n p rin cip al axes of an n dimensional space are a basis. Eigenvectors and Eigenvalues Important Equation â— A = n x n square matrix â— Ï… = n dimensional vector, known as the eigen vecto r â— Î» = scalar values, known as an eigen value Properties â— There are at most n eigenvector-eigenvalue pairs. â— If A is symmetric, then the set of eigenvectors is o rtho go n al â— If A is a co varian ce matrix, then the eigenvectors are the p rin cip al axes â— The eigenvalues are proportional to the variance of the data along each eigenvector Computer Science / Software Engineering Notes Network â— The eigenvector corresponding to the largest eigenvalue is the first principal component. Finding Values â— For small matrices (nâ‰¤4) there are algebraic solutions to finding all the eigenvector-eigenvalues pairs â— For larger matrices, numerical solutions to the Eigen deco mp o sitio n must be sought. Eigendecomposition Eigendecomposition takes a matrix and represents it in terms of its eigenvalues and eigenvectors. â— Columns of Q are the eigenvectors â— Diagonal eigenvalue matrix (Î›ii = Î»i) â— If A is real symmetric (i.e. a covariance matrix), then Q -1 = Q T (i.e. eigenvectors are orthogonal), so: â— Summary The Eigendecomposition of a covariance matrix A : Gives you the principal axes and their relative magnitudes. Ordering â— Standard Eigendecomposition implementations will order the eigenvectors (columns of Q) such that the eigenvalues (in the diagonal of Î›) are sorted in order of decreasing value. â—‹ Some solvers are optimised to only find the top k eigenvalues and corresponding eigenvectors, rather than all of them. Computer Science / Software Engineering Notes Network Principal Component Analysis Linear Transform â— A linear transform W projects data from one space into another: â—‹ T = ZW â–  Original data stored in the rows of Z â—‹ T can have fewer dimensions than Z Linear Transforms â— The effects of a linear transform can be reversed if W is in vertible: â—‹ Z = TW -1 â–  A lossy process if the dimensionality of the spaces is different Computer Science / Software Engineering Notes Network PCA â— PCA is an Ortho go n al Lin ear Tran sfo rm that maps data from its original space to a space defined by the principal axes of the data. â—‹ The transform matrix W is just the eigenvector matrix Q from the Eigendecomposition of the covariance matrix of the data. â—‹ Dimensionality reduction can be achieved by removing the eigenvectors with low eigenvalues from Q (i.e. keeping the first L columns of Q assuming the eigenvectors are sorted by decreasing eigenvalue). PCA Algorithm 1 . M e a n - c e n t r e t h e d a t a v e c t o r s 2 . F o r m t h e v e c t o r s i n t o a m a t r i x Z , s u c h t h a t e a c h r o w c o r r e s p o n d s t o a v e c t o r 3 . P e r f o r m t h e E i g e n d e c o m p o s i t i o n o f t h e m a t r i x Z T Z , t o r e c o v e r t h e e i g e n m a t r i x Q a n d d i a g o n a l e i g e n v a l u e m a t r i x Î› : Z T Z = Q Î› Q T 4 . S o r t t h e c o l u m n s o f Q a n d c o r r e s p o n d i n g d i a g o n a l v a l u e s o f Î› s o t h a t t h e e i g e n v a l u e s a r e d e c r e a s i n g 5 . S e l e c t t h e L l a r g e s t e i g e n v e c t o r s o f Q ( t h e f i r s t L c o l u m n s ) t o c r e a t e t h e t r a n s f o r m m a t r i x Q L Computer Science / Software Engineering Notes Network 6 . P r o j e c t t h e o r i g i n a l v e c t o r s i n t o a l o w e r d i m e n s i o n a l s p a c e , T L : T L = Z Q L Eigenfaces Spooky 0_0 Making Invariant â— Require (almost) the same object pose across images (i.e. full frontal faces) â— Align (rotate, scale and translate) the images so that a common feature is in the same place (i.e. the eyes in a set of face images) â— Make all the aligned images the same size â— (optional) Normalise (or perhaps histogram equalise) the images so they are invariant to global intensity changes Problems â— Featurevectors are huge â—‹ If the images are 100x200 pixels, the vector has 20000 dimensions â—‹ Thatâ€™s not really practicalâ€¦ â—‹ Also, the vectors are still highly susceptible to imaging noise and variations due to slight misalignments. Potential Solutionâ€¦ Apply PCA â— PCA can be used to reduce the dimensionality â—‹ Smaller number of dimensions allows greater robustness to noise and mis-alignment â–  There are fewer degrees of freedom, so noise/misalignment has much less effect â–  And the dominant features are captured Computer Science / Software Engineering Notes Network â—‹ Fewer dimensions makes applying machine-learning much more tractable Lecture 4:Types of image feature and segmentation Image Feature Morphology â— There are 4 main ways of extracting features from an image: â—‹ Global â—‹ Grid/Block-based â—‹ Region based â—‹ Local Global Features â— A Glo bal Feature is extracted from the contents of an entire image. Grid or Block-based Features â— Multiple features are extracted;one per block Computer Science / Software Engineering Notes Network Region-based Features â— Multiple features are extracted;one per region Local Features â— Multiple features are extracted;one per local interest point Computer Science / Software Engineering Notes Network Global Features Image Histograms â— Simple global features can be computed from the average of the colour bands of the imageâ€™s histogram. â—‹ This wasnâ€™t particularly robust, and couldnâ€™t deal well with multiple colours in the image. â— A more common approach to computing a global image description is to compute a histogram of the pixel values. Joint-colour histogram â— A joint colour histogram measures the number of times each colour appears in an image. â—‹ These are different to histograms in image editing programs with compute separate histograms for each channel. â—‹ The colour space is quantised into bins, and we accumulate the number of pixels in each bin. â–  Technically, itâ€™s a multidimensional histogram, but we flatten it (unwrap) to make it a feature vector. â— Normalisation (i.e. by the number of pixels) allows the histogram to be invariant to image size. â— Choice of colour-space can make it invariant to uniform lighting changes (e.g. H-S histogram) â— Invariant to rotation â— But vastly different images can have the same histogram! Like these two below: Computer Science / Software Engineering Notes Network â— Cool, right? Even though itâ€™s, uh, bad;not invariant to this kinda problem lol Image Segmentation What is segmentation? â— The first part in the process of creating region-based descriptionsâ€¦ â—‹ The process of partitioning the image into sets of pixels often called segmen ts. â—‹ Pixels within a segment typically share certain visual characteristics. Global Binary Thresholding â— Thresholding is the simplest form of segmentation â—‹ Turns grey level images into binary (2 segments) by assigning all pixels with a value less than a predetermined threshold to one segment, and all other pixels to the other. â—‹ Really fast â—‹ Required a manually set static threshold â–  Not robust to lightning changes â–  Can work well in applications with lots of physical constraints (lighting control and / or high-contrast objects) Otsuâ€™s thresholding method â— Otsuâ€™s method (named after Nobuyuki Otsu) provides a way to automatically find the threshold. â— Assume there are two classes (i.e. foreground & background) â—‹ The histogram must have two peaks â— Exhaustively search for the threshold that maximises interclass variance. Computer Science / Software Engineering Notes Network More detailed look over at wikipedia! Adaptive / local thresholding â— Local (or Adaptive) thresholding operators compute a different threshold value for every pixel in an image based on the surrounding pixels. â—‹ Usually a square or rectangular window around the current pixel is used to define the neighbours Mean adaptive thresholding Set the current pixel to 0 if its value is less than the mean of its neighbours plus a co n stan t value;otherwise set to 1. â— Parameters: â—‹ Size of window â—‹ Constant offset value â— Good invariance to uneven lighting / contrast â— Butâ€¦ â—‹ Computationally expensive (at least compared to global methods) â—‹ Can be difficult to choose the window size â–  If the object being imaged can appear at different distance to the camera then it could breakâ€¦ Segmentation with K-Means â— K-Means clustering also provides a simple method for performing segmentation: â—‹ Cluster the colour vectors (i.e. [r, g, b]) of all the pixels, and then assign each pixel to a segment based on the closest cluster centroid. Computer Science / Software Engineering Notes Network â—‹ Works best if the colour-space and distance function are compatible â–  E.g. Lab colour-space is designed so that Euclidean distances are proportional to perceptual colour differences â— NaÃ¯ve approach to segmentation using k-means doesnâ€™t attempt to preserve continuity of segments â—‹ Might end up with single pixels assigned to a segment, far away from other pixels in that segment. â— Can also encode spatial position in the vectors being clustered: [r, g, b, x, y] â—‹ Normalise x and y by the width and height of the image to take away the effect of different images sizes â—‹ Scale x and y so they have more or less effect than the colour components Advanced segmentation techniques â— Lots of ongoing research into better segmentation techniques: â—‹ Techniques that can automatically determine the number of segments â—‹ â€œSemantic segmentationâ€ techniques that try to create segments that fit the objects in the scene based on training examples Connected Components Pixel Connectivity â— A pixel is said to be connected with another if they are spatially adjacent to each other. â—‹ Two standard ways of defining this adjacency: â–  4-connectivity â–  8-connectivity (like minesweeper!) Connected Component A connected component is a set of pixels in which every pixel is connected either directly or through any connected path of pixels from the set. Computer Science / Software Engineering Notes Network Connected Component Labelling â— Connected Component Labelling is the process of finding all the connected components within a binary (segmented) image. â—‹ Each connected segment is identified as a connected component. â— Lots of different algorithms to perform connected component labelling â—‹ Different performance tradeoffs (memory verses time) The two-pass algorithm 1 . O n t h e f i r s t p a s s : a . I t e r a t e t h r o u g h e a c h e l e m e n t o f t h e d a t a b y c o l u m n , t h e n b y r o w ( R a s t e r S c a n n i n g ) i . I f t h e e l e m e n t i s n o t t h e b a c k g r o u n d 1 . G e t t h e n e i g h b o u r i n g e l e m e n t s o f t h e c u r r e n t e l e m e n t 2 . I f t h e r e a r e n o n e i g h b o u r s , u n i q u e l y l a b e l t h e c u r r e n t e l e m e n t a n d c o n t i n u e 3 . O t h e r w i s e , f i n d t h e n e i g h b o u r w i t h t h e s m a l l e s t l a b e l a n d a s s i g n i t t o t h e c u r r e n t e l e m e n t 4 . S t o r e t h e e q u i v a l e n c e b e t w e e n n e i g h b o u r i n g l a b e l s 2 . O n t h e s e c o n d p a s s : a . I t e r a t e t h r o u g h e a c h e l e m e n t o f t h e d a t a b y c o l u m n , t h e n b y r o w i . I f t h e e l e m e n t i s n o t t h e b a c k g r o u n d Computer Science / Software Engineering Notes Network 1 . R e l a b e l t h e e l e m e n t w i t h t h e l o w e s t e q u i v a l e n t l a b e l Lecture 5:Shape description and modelling Extracting features from shapes represented by connected components Borders â— There are 2 types of pixel borders:in n er and o uter. Say you have this pixel shape: Inner Border A border made up of only pixels from the shape;the outermost pixels within the shape. Computer Science / Software Engineering Notes Network Outer Border A border where the outline of pixels outside the shape make up the border. Two ways to describe shape 1. Region Description 2. Boundary Description Computer Science / Software Engineering Notes Network Region Description:Simple Scalar Shape Features Area and Perimeter Compactness â— Compactness measures how tightly packed the pixels in the component are. â— Itâ€™s often computed as the weighted ratio of area to perimeter squared: â— Examples: Computer Science / Software Engineering Notes Network Centre of Mass Irregularity / Dispersion A measure of how â€œspread-outâ€ the shape is Computer Science / Software Engineering Notes Network Moments Standard Moments â— Moments describe the distribution of pixels in a shape. â—‹ Moments can be computed for any grey-level image. For the purposes of describing shape, weâ€™ll just focus on moments of a connected component â—‹ Standard two-dimensional Cartesian moment of an image, with order p and q and I(s) as the pixel intensity, it is defined as: â—‹ â—‹ In the case of a connected component, this simplifies to: â—‹ â–  The zero order moment of a connected component m00 is just the area of the component. The centre of mass is (centroid): â–  Central Moments â— Standard 2D moments can be used as shape descriptors â—‹ But, theyâ€™re not invariant to translation, rotation and scaling Computer Science / Software Engineering Notes Network â— C en tral Mo men ts are computed about the centroid of the shape, and are thus translation invariant: â— Note:Î¼01 and Î¼10 are always 0, so have no descriptive power Normalised Central Moments â— No rmalised C en tral Mo men ts are both scale and translation invariant Boundary Description Chain Codes â— Simple way of encoding a boundary. â—‹ Walk around the boundary and encode the direction you take on each step as a number. â–  Some direction examples are shown below left. â—‹ Then cyclically shift the code so it forms the smallest possible integer value (making it invariant to the starting point) Computer Science / Software Engineering Notes Network Chain Code Invariance â— Can be made rotation invariant: â—‹ Encode the differences in direction rather than absolute values. â— Can be made scale invariant: â—‹ Resample the component to a fixed size â–  Doesnâ€™t work well in practice Chain Code Advantages and Limitations â— Can be used for computing perimeter area, moments, etc. â—‹ Perimeter for and 8-connected chain code is N(even numbers in code) + âˆš2N(odd numbers in code) â— Practically speaking, not so good for shape matching â—‹ Problems with noise, resampling effects, etc â—‹ Difficult to find good similarity/distance measures Fourier Descriptors â— The Fourier transform can be used to encode shape information by decomposing the boundary into a (small) set of frequency components. â—‹ There are two main steps to consider: â–  Defining a representation of a curve (the boundary) â–  Expanding the representation using Fourier theory Computer Science / Software Engineering Notes Network â—‹ By choosing these steps carefully it is possible to create rotation, translation and scale invariant boundary descriptions that can be used for recognition, etc. Region Adjacency Graphs â— Build a graph from a set of connected components â—‹ Each node corresponds to a component â—‹ Nodes connected if they share a border â— Can easily detect patterns in the graph â—‹ E.g. â€œa node with one child with four childrenâ€ â— Invariant to non-linear distortions, but not to occlusion. Theres some more stuff in the slides which isnâ€™t hand with bow-ed, go have a look Slides 40-47:http://comp3204.ecs.soton.ac.uk/lectures/pdf/L5-shapedescription.pdf Active Shape Models and Constrained Local Models â— ASMs/CLMs extend a PDM (Point Distribution Model) by also learning local appearance around each point â—‹ Typically just as an image template. â— Using a constrained optimisation algorithm, the shape can be optimally fitted to an image â—‹ Constraints: â–  Plausible shape â–  Good template matching Lecture 6:Local interest points What makes a good interest point? â— Invariance to brightness change (local changes as well as global ones) â— Sufficient texture variation in the local neighbourhood Computer Science / Software Engineering Notes Network â—‹ But not too much! â— Invariance to changes between the angle / position of the scene to the camera How to find interest points â— There are lots of different types of interest point types to choose from â—‹ Weâ€™ll focus on two specific types and look in detail at common detection algorithms: â–  Corner detection - Harris and stephens â–  â–  Blob Detection - Difference-of-Gaussian Extrema â–  Computer Science / Software Engineering Notes Network The Harris and Stephens corner detector Basic Idea â— Search for corners by looking through a small window â— Shifting that window by a small amount in any direction should give a large change in intensity Harris & Stephens:Mathematics Weighted average change in intensity between a window and a shifted version [by (Î”x,Î”y)] of that window: Computer Science / Software Engineering Notes Network â— The Taylor expansion allows us to approximate the shifted intensity. â— Taking the first order terms we get this: â— â— Substituting and simplifying gives: â— â— Bruh. Computer Science / Software Engineering Notes Network Structure Tensor â— The sq uare symmetric matrix M is called the Structure Tensor or the Second Moment matrix â— â— It concisely encodes how the local shape intensity function of the window changes with small shifts Eigenvalues of the Structure Tensor â— Think back to covariance matricesâ€¦ â—‹ As with the 2D covariance matrix, the structure tensor describes and ellipse: x TMx=c (this is a quadratic form) â—‹ The eigenvalues and vectors tell us the rates of change and their respective directions Computer Science / Software Engineering Notes Network Harris & Stephens Response Function â— Rather than compute the eigenvalues directly, Harris and Stephens defined a corner response function in terms of the determinant and trace of M: â— â— Smart. Computer Science / Software Engineering Notes Network Harris & Stephens Detector â— Simple algorithm: â—‹ Take all points with the response value above a threshold â—‹ Keep only the points that are local maxima (i.e. where the current response is bigger than the 8 neighbouring pixels) Scale in Computer Vision The problem of scale â— As an object moves closer to the camera it gets larger with more detailâ€¦ as it moves further away it gets smaller and loses detailâ€¦ â— If youâ€™re using a technique that uses a fixed size processing window (e.g. Harris corners, or indeed anything that involves a fixed kernel) then this is a bit of a problem! Scale space theory â— Scale space theory is a formal framework for handling the scale problem. Computer Science / Software Engineering Notes Network â—‹ Represents the image by a series of increasingly smoothed / blurred images parameterised by a scale parameter t. â—‹ t represents the amount of smoothing. â—‹ Key n o tio n : Image structures smaller than âˆšt have been smoothed away at scale t. The Gaussian Scale Space â— Many possible types of scale space are possible (depending on the smoothing function), but only the Gaussian function has the desired properties for image representation. â—‹ These provable properties are called the â€œscale space axiomsâ€. Formally, Gaussian scale space is defined as: Where t â‰¥ 0 and, Normally, only a fixed set of values of t are used - itâ€™s common to use integer powers of 2 or âˆš2 Computer Science / Software Engineering Notes Network Nyquist-Shannon Sampling theorem If a fun ctio n x(t) co n tain s n o freq uen cies higher than B hertz, it is co mp letely determin ed by givin g its o rdin ates at a series o f p o in ts sp aced 1/(2B ) seco n ds ap art. ...so, if you filter the signal with a low-pass filter that halves the frequency content, you can also half the sampling rate without loss of informationâ€¦ Gaussian Pyramid â— Every time you double t in scale space, you can half the image size without loss off information! â—‹ Leads to a much more efficient representation â–  Faster processing â–  Less memory Multi-scale Harris & Stephens â— Extending the Harris and Stephens detector to work across scales is easyâ€¦ â—‹ We define a Gaussian scale space with a fixed set of scales and compute the corner response function at every pixel of each scale and keep only those with a response above a certain threshold. Blob Detection Finally Laplacian of Gaussian â— Recall that the LoG is the second derivative of a Gaussian Computer Science / Software Engineering Notes Network â—‹ Used in the Marr-Hildreth edge detector â–  Zero crossing of LoG convolution â— By finding local minima or maxima, you get a blob detector! Scale space LoG â— Normalised scale space LoG defined as: â— By finding extrema of this function in scale space, you can find blobs at their representative scale (~âˆš2t ) â—‹ Just need to look at the neighbouring pixels! Computer Science / Software Engineering Notes Network Very useful p ro p erty: if a blob is detected at (x0, y0 ;t0) in an image, then under a scaling of that image by a factor s, the same blob would be detected at (sx0, sy0 ;s 2t0) in the scaled image. Scale space DoG â— In practice itâ€™s computationally expensive to build a LoG scale space. â— But, the following approximation can be made: â— This is called a Difference-of-Gaussians (DoG) â—‹ Implies that the LoG scale space can be built from subtracting adjacent scales of a Gaussian scale space DoG Pyramid â— Of course, for efficiency you can also build a DoG pyramid â—‹ An oversampled pyramid as there are multiple images between a doubling of scale. â—‹ Images between a doubling of scale are an octave. Computer Science / Software Engineering Notes Network Lecture 7:Local features and matching Local features and matching basics Local Features Multiple features are extracted;one per local interest point Why extract local features? â— Feature points are used for: â—‹ Image align men t â—‹ Camera pose estimate & Camera calibration â—‹ 3D reconstruction â—‹ Motion tracking â—‹ Object reco gn itio n â—‹ In dexin g an d database retrieval â—‹ Robot navigation â—‹ â€¦ Example:Building a panorama â— You need to match and align the images â— Detect feature points in both images â— Find corresponding pairs â— Use the pairs to align the images Problem 1: â— Detect the same points independently in both images â— We need a rep eatable detector Computer Science / Software Engineering Notes Network Problem 2: â— For each point correctly recognise the corresponding one â— We need an in varian t, ro bust and distin ctive descriptor Two distinct types of matching problem â— In stereo vision (for 3D reconstruction) there are two important concepts related to matching: â—‹ Narrow-baseline stereo â—‹ Wide-baseline stereo Narrow-baseline stereo â— This is where the two images are very similar - the local features have only moved by a few pixels. â—‹ Typically the images are from similar points in time (Notice how the red-circled background object only appears in the second image;so these two images have a slight difference to them) Wide-baseline stereo â— This is where the difference in views is much bigger. Computer Science / Software Engineering Notes Network (I donâ€™t think I need to circle the whole image lol) Two distinct types of matching problem â— These concepts extend to general matching: â—‹ The techniques for narrow-baseline stereo are applicable to tracking where the object doesnâ€™t move too much between frames â—‹ The techniques for wide-baseline stereo are applicable to generic matching tasks (object recognition, panoramas, etc.). Robust local description Descriptor Requirements â— Dependent on task: â—‹ Narrow baseline: â–  Robustness to rotation and lighting is not so important. â–  Descriptiveness can be reduced as the search is over a smaller area. â—‹ Wide baseline â–  Need to be robust to intensity change, invariant to rotation. â–  Need to be highly descriptive to avoid mismatches (but not so distinctive that you canâ€™t find any matches!) â–  Robust to small localisation errors of the interest point â— The descriptor should not change too much if we move it by a few pixels, but to change more rapidly once we move further away. â–  Lol Computer Science / Software Engineering Notes Network Matching by correlation (template matching) (Narrow baseline) template matching â— Interest points in two images with a slight change in position â— Local search windows, based on the interest point in the first image â— The template can then be matched against target interest points in the second image Computer Science / Software Engineering Notes Network Problems with wider baselines â— Not robust to rotation â— Sensitive to localisation of interest point â—‹ (although not such a problem with a small search window) â— With wider baselines you canâ€™t assume a search area â—‹ Need to consider all the interest points in the second image â–  More likely to mismatch :( Local Intensity Histograms Use local histograms instead of pixel patches â— Describe the region around each interest point with a pixel histogram â— Match each interest point in the first image to the most similar point in the second image (i.e. in terms of Euclidean distance [or other measure]between the histograms) Local histograms â— Problems: â—‹ Not necessarily very distinctive â–  Many interest points likely to have similar distribution of grey-values â—‹ Not rotation invariant if the sampling window is square or rectangular â–  Can be overcome using a circular window â—‹ Not invariant to illumination changes â—‹ Sensitive to interest point localisation Computer Science / Software Engineering Notes Network Overcoming localisation sensitivity â— Want to allow the window around the interest point to move a few pixels in any direction without changing the descriptor â—‹ Apply a weighting so that pixels near the edge of the sampling patch have less of an effect, and those near the interest point have a greater effect â–  Common to use Gaussian weightin g centred on the interest point for this. Overcoming lack of illumination invariance â— Illumination invariance potentially achievable by normalising or equalising the pixel patches before constructing the histogram â— ...but there is another alternative!................. Iâ€™m not going to tell youâ€¦â€¦. Jokingâ€¦â€¦. Local Gradient Histograms Gradient Magnitudes and Directions â— From the partial derivatives of an image (e.g. from applying convolution with Sobel), it is easy to compute the gradient orientations / directions and magnitudes Gradient Histograms â— Instead of building histograms of the raw pixel values we could instead build histograms that encode the gradient magnitude and direction for each pixel in the sampling patch. â—‹ Gradient magnitudes (and directions) are in varian t to brightn ess change! â—‹ The gradient magnitude and direction histogram is also mo re distin ctive. Building gradient histograms â— Quantise the directions (0Â°-360Â°) into a number of bins â—‹ Usually around 8 bins â— For each pixel in the sampling patch, accumulate the gradient magnitude of that pixel in the respective orientation bin Computer Science / Software Engineering Notes Network Rotation Invariance â— Gradient histograms are not naturally rotation invariant â—‹ But, can be made invariant by finding the dominant orientation and cyclically shifting the histogram so the dominant orientation is in the first bin. The SIFT feature Adding spatial awareness â— The SIFT (Scale Invariant Feature Transformer) feature is widely used â—‹ Builds on the idea of a local gradient histogram by incorporating spatial binning, which in essence creates multiple gradient histograms about the interest point and appends them all together into a longer feature. â—‹ Standard SIFT geometry appends a spatial 4x4 grid of histograms with 8 orientations Computer Science / Software Engineering Notes Network â–  Leading to a 128-dimensional feature which is highly discrimin ative and ro bust! SIFT Construction:sampling SIFT Construction:weighting Computer Science / Software Engineering Notes Network SIFT Construction:binning Matching SIFT features Euclidean Matching â— SImplest way to match SIFT features is to take each feature in turn from the first image and find the most similar in the second image â—‹ Threshold can be used to reject poor matches â—‹ Unfortunately, doesn't work that well and results in lots of mismatches. Improving matching performance â— A better solution is to take each feature from the first image, and find the two closest features in the second image. â—‹ Only form a match if the ratio of distances between the closest and second closest matches is less than a threshold. â–  Typically set at 0.8, meaning that the distance to the closest feature must be at least 80% of the second closest. â—‹ This leads to a much more robust matching strategy. Lecture 8:Consistent matching Feature distinctiveness â— Even though the most advanced local features can be prone to being mismatched. Computer Science / Software Engineering Notes Network â—‹ There is always a tradeoff in feature distinctiveness. â—‹ If itâ€™s too distinctive it will not match subtle variations due to noise of imaging conditions. â—‹ If itâ€™s not distinctive enough it will match everything. Constrained matching â— Assume we are given a number of correspondences between the interest points in a pair of images â—‹ Is it possible to estimate which of those correspondences are in liers (correct) or o utliers (incorrect/mismatches)? â—‹ What assump tio n s do we have to make? â— By assuming a geometric mapping between the two scenes, can we recover that mapping and eliminate the mismatches? Geometric Mappings What are geometric transforms? â— In general, a geometric mapping can be thought of as a transform function that maps the x, y coordinates of points in one image to another. Go have a look at some on Wikipedia here: https://en.wikipedia.org/wiki/Geometric_transformation Point Transforms Weâ€™re interested in transforms that take the following form:x â€™ = T x The Affine Transform The affine transform is defined as Computer Science / Software Engineering Notes Network Itâ€™s more convenient to write this as a single transform matrix by adding an extra dimension to each vector: Translation Computer Science / Software Engineering Notes Network Translation and Rotation Scaling Computer Science / Software Engineering Notes Network Aspect Ratio Computer Science / Software Engineering Notes Network Shear Degrees of Freedom Affine Transform 6 DoF:translation + rotation + scale + aspect ratio + shear Computer Science / Software Engineering Notes Network Similarity Transform 4 DoF:translation+rotation+scale More degrees of freedom Normalise by w so that the transformed vector is [â€¢,â€¢,1] Computer Science / Software Engineering Notes Network Homogeneous coordinates The Planar Homography (Projective Transformation) Recovering a geometric mapping Simultaneous equations â— It is possible to estimate a transform matrix from a set of point matches by solving a set of simultaneous equations Computer Science / Software Engineering Notes Network â—‹ Need at least 4 point matches to solve a Homography or 3 to solve an affine transform â— The actual solution technique isnâ€™t importantâ€¦ â—‹ It is important to note that in the presence of noise, and with potentially more matches than required, that we have to solve an overdetermined system â—‹ We need to seek the minimum error or least-sq uares solution Least-squares Computer Science / Software Engineering Notes Network Robust Estimation Problem:Noisy data â— Need a way to deal with estimating a model (i.e. a transform matrix) in the presence of high amounts of noise (i.e. mis-matches) â—‹ Least-squares will be highly suboptimal, and probably find a very bad solution. â—‹ Ideally, we want to identify the correct data (the inliers) and the bad data (the outliers) â–  Then estimate the model using only the good data. Robust estimation techniques â— The problem of learning a model in the presence of inliers and outliers comes under an area of mathematics called ro bust estimatio n or ro bust mo del fittin g â—‹ There are a number of different possible techniques RANSAC:RANdom SAmple Consensus This is an iterative method to estimate parameters of a mathematical model for a set of observed data that contains outliers. A s s u m e : M d a t a i t e m s r e q u i r e d t o e s t i m a t e m o d e l T N d a t a i t e m s i n t o t a l A l g o r i t h m : 1 . ) S e l e c t M d a t a i t e m s a t r a n d o m 2 . ) E s t i m a t e m o d e l T Computer Science / Software Engineering Notes Network 3 . ) F i n d h o w m a n y o f t h e N d a t a i t e m s f i t T w i t h i n t o l e r a n c e t o l , c a l l t h i s K ( i . e . c o m p u t e h o w m a n y t i m e s t h e a b s o l u t e r e s i d u a l i s l e s s t h a n t o l ) . T h e p o i n t s t h a t h a v e a n a b s o l u t e r e s i d u a l l e s s t h a n t o l a r e t h e i n l i e r s ; t h e o t h e r p o i n t s a r e t h e o u t l i e r s . 4 . ) I f K i s l a r g e e n o u g h , e i t h e r a c c e p t T , o r c o m p u t e t h e l e a s t - s q u a r e s e s t i m a t e u s i n g a l l i n l i e r s , a n d e x i t w i t h s u c c e s s . 5 . ) R e p e a t s t e p s 1 . . 4 n I t e r a t i o n s t i m e s 6 . ) F a i l - n o g o o d T f i t o f d a t a Further applications of robust local matching Object recognition & AR â— Object recognition â—‹ Image of object is matched against scene, and recognised if there is a consistent match â— Augmented reality â—‹ Data can be added to a scene on the basis of a match 3D reconstruction â— Itâ€™s possible to estimate depth, and ultimately build a complete 3d scene from sets of point correspondences formed from matching local features Problems with direct local feature matching Local feature matching is slow! â— Typical image (800x600) might have ~2000 DoG Interest points/SIFT descriptors â—‹ Each SIFT descriptor is 128 dimensions â—‹ Now assume you want to match a query image against a database of imageâ€¦ â–  The distance between every query feature and every other feature needs to be calculated â–  Can this be optimised somehow? Efficient Nearest Neighbour Search â— How can we q uickly find the nearest neighbour to a query point in a high dimensional space? â—‹ Index the points in some kind of tree structure? â—‹ Hash the points? â—‹ Quantise the space? Computer Science / Software Engineering Notes Network K-D Trees â— Binary tree structure that partitions the space along axis-aligned hyperplanes â—‹ Typically take each dimension in turn and splits on the median of the points in the enclosing partition. â—‹ Stop after a certain depth, or when the number of points in a leaf is less than a threshold â— Search by walking down the tree until a leaf is hit, and then brute-force search to find the best in the leaf. â—‹ This is not guaranteed to be the best thoughâ€¦ â—‹ To have to walk back up the tree and see if there are any better matches, and only stop once the root is reached. â—‹ (note you donâ€™t have to check a subtree if itâ€™s clear that all points in that subtree are further than the current best.) Computer Science / Software Engineering Notes NetworkComputer Science / Software Engineering Notes Network K-D Tree problems â— Doesnâ€™t scale well to high dimensions â—‹ You tend to end up needing to search most of the tree â— There are approximate versions that wonâ€™t necessarily return the exact answer that do scale (if you donâ€™t mind the potential for mismatch) Computer Science / Software Engineering Notes Network Hashing â— Locality Sensitive Hashing (LSH) creates hash codes for vectors such that similar vectors have similar hash codes! Sketching â— A technique called sketching concatenates binary hashes into a bit string. â—‹ With the correct LSH function, the Hamming distance between a pair of sketches is proportional to the Euclidean distance between the original vectors â—‹ Can easily compress SIFT features to 128 bits â–  Hamming distance computation is cheap â— Lookup tables and bitwise operations Lecture 9:Image search and Bags of Visual Words Text Information Retrieval The bag data structure â— A bag is an un o rdered data like a set, but which unlike a set allows elements to be in serted multip le times. â—‹ Sometimes called a multiset or a counted set Bag of Words â— Say you have a document with this content: â—‹ â€œt h e q u i c k b r o w n f o x j u m p e d o v e r t h e l a z y d o g â€ â— A bag of words describing this document would be: Computer Science / Software Engineering Notes Network â—‹ Text processing (feature extraction) The Vector-Space Model â— Conceptually simple: â—‹ Model each document by a vector â—‹ Model each query by a vector â—‹ Assumption:documents that are â€œclose togetherâ€ in space are similar in meaning. â—‹ Use standard similarity measures to rank each document to a query in terms of decreasing similarity Computer Science / Software Engineering Notes Network Bag of Words Vectors â— The lexicon or vocabulary is the set of all (processed) words across all documents known to the system. â— We can create vectors for each document with as many dimensions as there are words in the lexicon â—‹ Each word in the documentâ€™s bag of words contributes to a count to the corresponding element of the vector for that word. â–  In essence, each vector is a histogram of the word occurrences in the respective document. â–  Vecto rs will have a very high n umber o f dimen sio n s, but will be very sp arse. The Vector-space Model Computer Science / Software Engineering Notes Network Searching the VSM Recap:Cosine Similarity Computer Science / Software Engineering Notes Network Inverted Indexes â— A map of words to lists of postings Aardvark [doc3:4] Astronomy [doc1:2] Diet [doc2:9;doc3:8] ... Movie [doc2:10] Star [doc:13;doc2:4] Telescope [doc1:15] â— A p o stin g is a pair formed by a do cumen t ID and the n umber o f times the specific word appeared in that document â— So for the first entry:Aardvark appeared 4 times in document 3. Computing the Cosine Similarity â— For each word in the query, lookup the relevant posting list and accumulate similarities for only the documents seen in those postings lists â—‹ Much more efficient than fully comparing vectorsâ€¦ Computer Science / Software Engineering Notes Network Weighting the vectors â— The number of times a word occurs in a document reflects the importance of that word in the document. â— Intuitions: â—‹ A term that appears in many documents is not important:e.g., the, going come,... â—‹ If a term is frequent in a document and rare across other documents, it is probably important in that document. Possible weighting schemes â— Binary weights â—‹ Only presence (1) or absence (0) of a term recorded in vector. â— Raw frequency â—‹ Frequency of occurrence of term in document included in vector. â— TF / IDF â—‹ Term frequency is the frequency count of a term in a document. â—‹ Inverse document frequency (idf) provides high values for rare words and low values for common words. Vector Quantisation Learning a Vector Quantiser â— Vector quantisation is a lossy data compression technique. â— Given a set of vectors, a technique like K-Means clustering can be used to learn a fixed size set of representative vectors. â—‹ The representatives are the mean vector of each cluster in k-means. â—‹ The set of representation vectors is called a co debo o k Computer Science / Software Engineering Notes Network Vector Quantisation â— Vector quantisation is achieved by representing a vector by another approximate vector, which is drawn from a pool of representative vectors. â—‹ Each input vector is assigned to the â€œclosestâ€ vector from the pool. Visual Words SIFT Visual Words â— We can vector quantise SIFT descriptors (or any other local feature) â—‹ Each descriptor is replaced by a representative vector known as a visual wo rd â–  In essence the visual word describes a small image patch with a certain pattern of pixels â–  In many ways the process of applying vector quantisation to local features is analogous to the process of stemming words. â—‹ The codebook is the visual equivalent of a lexicon or vocabulary. Bags of Visual Words â— Once weâ€™ve quantised the local features into visual words, they can be put into a bag. â—‹ This is a B ag o f Visual Wo rds (B o VW) â—‹ Weâ€™re basically ignoring where in the image the local features came from (including ignoring scale) So why are we doing all this word stuff for a module about vision?.... Computer Science / Software Engineering Notes Network Histograms of Bags of Visual Words â— Like in the case of text, once we have a BoVW and knowledge of the complete vocabulary (the codebook) we can build histograms of visual word occurrences! â—‹ This is niceâ€¦ it gives us a way of aggregating a variable number of local descriptors into a fixed length vector. â–  Useful for machine learning â–  But also allows us to apply techniques for text retrieval to images Computer Science / Software Engineering Notes Network Visualising Visual Words The effect of codebook size â— There is one key p arameter in building visual word representations - the size o f the vo cabulary. â—‹ Too small, and all vectors look the same â–  Not distinctive â—‹ Too big, and the same visual words might never appear across images â–  Too distinctive Computer Science / Software Engineering Notes Network Content-based Image Retrieval BoVW Retrieval â— With the visual word representation, everything used for text retrieval can be applied directly to images â—‹ Vector space model â—‹ Cosine similarity â—‹ Weighting schemes â—‹ Inverted index Optimal codebook size â— Inverted index only gives a performance gain if the vectors are sparse (you donâ€™t want to end up explicitly scoring all documents) â— Visual words also need to be sufficiently distinctive to minimise mismatching â—‹ Implies a very big codebook â–  Modern research systems often use 1 million or more visual words for SIFT vectors Problems with big codebooks â— Thereâ€™s a slight problemâ€¦ â—‹ Need to use k-means to learn 1 million clusters in 128 dimensions from 10â€™s of millions of features Computer Science / Software Engineering Notes Network â–  Non-trivial! â–  Vector quantisation has the same problems â— Have to use approximate methods, like approximate k-d trees Overall process for building a BoVW retrieval system 1. Collect the corpus of images that are to be indexed and made searchable 2. Extract local features from each image 3. Learn a large codebook from (a sample of) the features 4. Vector quantise the features, and build BoVW representations for each image 5. Construct an inverted index with the BoVW representations Lecture 10:Image classification and auto-annotation Multilabel classification â— Oh my gosh, this picture has a cat in it, so it must be classified as cat. â— But wait, it has a dog in it as well, so itâ€™s classified as a dog. â— AAHHHH there is both a cat and a dog in the image AAAAAâ€¦. â— Waitâ€¦ â— Why canâ€™t we just classify it as both? â— In the context of images often called Automatic Annotation Computer Science / Software Engineering Notes Network Object Detection / Localisation Slide summary:Challenges in Computer Vision This is a summary of slides slides 7-16, go look in the slides if youâ€™re interested http://comp3204.ecs.soton.ac.uk/lectures/pdf/L10-classification.pdf â— Object Recognition in natural scenes â— Scene/Activity Classification â— Automatic Annotation (itâ€™s not that good sometimes) â— The fundamental problem of computer vision:The Seman tic Gap â— In an image for computer vision there are: â—‹ Semantics â—‹ Object Labels â—‹ Objects â—‹ Descriptors â—‹ Raw media â— Upside-down cars lol â— History: â—‹ 1999 - SIFT matching â–  Very powerful, but computationally demanding â—‹ 2001 - Cascades of Haar-like features â–  Very popular for face detection â—‹ 2006 - SURF matching â–  Combined ideas from SIFT and the integral images used for computing Haar-like features â— Interest in auto-annotation grew from the late 90s Bags of â€œVisual Wordsâ€ were rather important! Computer Science / Software Engineering Notes Network Aside:Optimal codebook size â— The codebook vocabulary needs to be much smaller than for doing image search(ing?) â—‹ In general, machine-learning techniques need much smaller vectors (for both performance and effectiveness) â—‹ The visual words can be allowed to be less distinctive, allowing a little more variation between matching features. â—‹ Typically, the number of visual words might be as small as a few hundred, and up to a few thousand. Another slide summary:Stuff Slides 18-26 â— Machine Translation (2002) â—‹ Identifying areas in an image, using visual words! â— Semantic Spaces â—‹ SIngular Value Decomposition â—‹ Probabilistic Latent Factor Models â—‹ Non-negative Matrix Factorisation â—‹ Visual words again! â— Research focus shifted a little to use of bigger datasets in the mid-late 2000s. â— Interest in simpler (but more scalable) classifiers grew â— Classifying with BoVW â—‹ BoVW histogram representations are incredibly useful for image classification and object detection â—‹ Commonly used with fast linear classifiers and SVMs â— Over time the features used to create BoVW representations have improved â— Early global colour visual terms â—‹ Consider each pixel as a visual word based on the quantisation of its colour to a discrete set of values. â—‹ The BoVW Histogram is just a joint colour histogram that we saw earlier â—‹ Sunset colours :o â— Visual words from regions/segments â—‹ Car in the night sky ğŸ˜Œâ­ â— Visual words from interest points â—‹ Salient region detection â—‹ Local Descriptors â—‹ Vector Quantisation â—‹ Vocabulary of visual terms learnt through hierarchical k-means â—‹ Word Occurrence Vectors â— Local features extracted around interest points work okay for classification, but there are more recent strategies that can work betterâ€¦ Computer Science / Software Engineering Notes Network â—‹ densely sampled features Dense Local Image Patches Dense SIFT â— Rather than extracting your SIFT features at DoG interest points, you could extract them across a dense grid - this gives much more coverage of the entire image. Pyramid Dense SIFT â— For even better performance and coverage, you can sample in a Gaussian pyramid â—‹ Note that the sampling region is a fixed size, so at higher scales you sample more content Computer Science / Software Engineering Notes Network Egyptian Rhinoceros Spatial Pyramids Developing and benchmarking a BoVW scene classifier Evaluation Dataset â— Common for academic research to use standardised datasets for developing scene classifiers and comparing results â—‹ Datasets are usually split into labelled â€œtrainingâ€ and â€œtestâ€ sets. Computer Science / Software Engineering Notes Network â–  Only the training set can be used to train the classifier â–  Sometimes the test set labels are withheld completely to ensure there is no cheating Building the BoVW â— Firstly the raw features need to be extracted from the training images â— Then (if necessary) learn a codebook from these features â—‹ i.e. using k-means on the raw features â–  Might be a uniform random sample of all the features rather than all of them â— Apply (vector) quantisation to the raw features and count the number of occurrences to build histograms for each image Training classifiers â— Classifiers can be trained using the histograms. â—‹ e.g. OvR linear classifiers with a kernel map. â—‹ You might train on a subset of the training data â–  And use the remaining data to â€œvalidateâ€ and optimise parameters. â–  Once youâ€™ve chosen the optimal parameters you can then re-train using the optimal values. Classifying the test set â— Youâ€™re now in a position to apply the classifiers to the test data: â—‹ Extract the features â—‹ Quantise the features (using the codebook developed from the training set!) â—‹ COmpute the occurrence histograms â—‹ Use the classifiers to find the most likely class Evaluating Performance â— Lots of ways to evaluate the performance of classification on the test (and validation) set.â—‹ Conceptually the simplest summary measure is probably average precision â—‹ This is literally the proportion of number of correct classifications to the total number of predictions Phew, I think weâ€™ve covered all the examinable content. Iâ€™ll summarise the bonus slides just for completeness. Computer Science / Software Engineering Notes Network Lecture 11:Towards 3D vision Look at the slides for pictures :) http://comp3204.ecs.soton.ac.uk/lectures/pdf/L11-towards3d.pdf â— Applications â—‹ Architecture â—‹ Urban Planning â—‹ Virtual Tourism â—‹ Clothing & body measurement â—‹ Art â—‹ SLAM (Simultaneous localization and mapping) â—‹ Cultural Heritage â—‹ Forensics â—‹ Surveillance â—‹ Motion Capture (Films & Games) â— Cameras â—‹ Camera Geometry â—‹ Camera Calibration â— Measuring Depth â— Narrow Baseline Stereo â—‹ Stereo Camera â—‹ Epipolar geometry â—‹ Dense narrow-baseline stereo â— Wide Baseline Stereo â—‹ Multiple images can be used to jointly infer 3D structure, and the camera pose and intrinsics of each camera â—‹ Point matches (i.e. SIFT) are used as the basis for triangulating 3D points from the 2D images â—‹ Reconstructing Venice â— Monocular Vision â— Shadow Scanner â— Structured Light Imaging â—‹ Time of Flight Imaging â— Non-visible techniques â—‹ LIDAR â—‹ PrimeSense (Kinect) Summary Summary â— 3D computer vision has lots of practical applications â— Camera models give a mathematical description of how a pixel in a 2D image is related to a point in a 3D scene â—‹ Camera calibration can be used to find the parameters of a camera â— Multiple views of a scene can be used to infer depth Computer Science / Software Engineering Notes Network â— There are lots of other techniques for capturing depth that only require a single sensor Programming for computer vision & other musings related to the coursework Writing code for computer vision â— Images usually stored as arrays of integers â—‹ Typically 8-bits per pixel per channel â–  12-16 bit increasingly common (e.g. HDR imaging) â–  Uses unsigned pixel values â—‹ Compressed using a variety of techniques â–  Lossy or lossless Most vision algorithms are continuous â— E.g. convolution with a continuous function (i.e. Gaussian) â— If we were writing the next Adobe Photoshop, it would be important that we kept out images in a similar format (integer pixels, same number of bits) â—‹ We would essentially round pixel values to the closest integer and clip those out of range â— For vision applications we donâ€™t want to do this as weâ€™ll lose precision Always work with floating point pixels â— Unless theyâ€™ve been specifically optimised for integer math, all vision algorithms should use floating point pixel values â—‹ Ensure the best possible discretisation from operations involving continuous functions â–  Higher effective bit depth (32/64 bits per pixel per band) â–  Ability to deal with negative values â— Turns out to be very important for convolution! â–  Ability to deal with numbers outside of the normal range â— Just because a pixel has a grey level of 1.1 doesnâ€™t mean itâ€™s invalid, just that itâ€™s too bright to be displayed in the normal colour gamut. Guidelines for writing vision code â— Convert any images to float types immediately once youâ€™ve read them â— Donâ€™t convert them back to integer types until you need to (i.e. for display or saving) â—‹ Be mindful that a meaningful conversion might not just involve rounding if you want to preserve the data. Computer Science / Software Engineering Notes Network Convolution We probably do need to know how to do convolution so... â— Convolution is an element-wise multiplication in the Fourier domain (c.f. Convolution Theorem) â— fï¹¡g = ifft(fft(f) . fft(g)) â— Whilst S and F might only contain real numbers, the FFTs are complex (real + imagj) â—‹ Need to do complex multiplication! â— Aside:phase and magnitude â— Given a complex number (n = real + imagj) from an FFT we can compute its p hase and magn itude â—‹ phase = atan2(imag, real) â—‹ magnitude = sqrt(real*real + imag*imag) â— We might perform this transformation to display the FFT as it conceptually helps us understand what the FFT is doing â— We canâ€™t use this representation to perform convolution however (need to transform back to complex form first) Aside:Displaying FFTs â— FFTs are often re-ordered so that the DC component (0- frequency) component is in the centre: Computer Science / Software Engineering Notes Network Template Convolution â— In the time domain, convolution is: â— No tice that the image o r kern el is â€œflip p edâ€ in time â—‹ Also notice that the is no normalisation or similar i n t k h = k e r n e l . h e i g h t ; i n t k w = k e r n e l . w i d t h ; i n t h h = k h / 2 ; i n t h w = k w / 2 ; I m a g e c l o n e = n e w I m a g e ( i m a g e . w i d t h , i m a g e . h e i g h t ) ; f o r ( i n t y = h h ; y < i m a g e . h e i g h t - ( k h - h h ) ; y + + ) { f o r ( i n t x = h w ; x < i m a g e . w i d t h - ( k w - h w ) ; x + + ) { f l o a t s u m = 0 ; f o r ( i n t j = 0 , j j = k h - 1 ; j < k h ; j + + , j j - - ) { f o r ( i n t i = 0 , i i = k w - 1 ; i < k w ; i + + , i i - - ) { i n t r x = x + i - h w ; i n t r y = y + j - h h ; s u m + = Computer Science / Software Engineering Notes Network i m a g e . p i x e l s [ r y ] [ r x ] * k e r n e l . p i x e l s [ j j ] [ i i ] ; } }c l o n e . p i x e l s [ y ] [ x ] = s u m ; } }Formatting bruh What if you donâ€™t flip the kernel? â— Obviously if the kernel is symmetric, there is no difference â— However, youâ€™re actually not computing convolution, but another operation called cross-correlation â— * represents the complex conjugate â— (you can compute this with the multiplication of the FFTs just like convolution: iFFT(FFT(f)* . FFT(g)) Ideal Low-Pass filter â— â€œIdealâ€ low pass filter removes all frequencies above a cutoff Computer Science / Software Engineering Notes Network Ideal Low-Pass filter - problems Computer Science / Software Engineering Notes Network Gaussian filters - why Building Gaussian Filters Computer Science / Software Engineering Notes Network High-pass filters â— â€œTo obtain a high-pass filtered image, subtract a lowpass filtered image from the image itselfâ€ â—‹ ILP = Iï¹¡G â—‹ IHP = I-ILP â—‹ IHP = I - Iï¹¡G â—‹ IHP = Iï¹¡Î´ - Iï¹¡G â—‹ IHP = Iï¹¡(Î´ - G) Note - Donâ€™t do this! â— IHP = Iï¹¡(Î´ - G) is not the same as IHP = Iï¹¡(1 - G) Computer Science / Software Engineering Notes Network High-pass filters have a mixture of negative and positive coefficients â— â€¦that means the resultant image will also have positive and negative pixels â—‹ this is important - for example it can tell us about the direction of edges: â–  [-0.5, 0.5]kernel â— (remember convolution means kernel flipped) â— +values in the output image mean edge from right to left â— -values in output image mean edge from left to right â— Convolution implementation MUST NOT: â—‹ Normalise â—‹ result in unsigned types Building hybrid images â€¦is really simple â— Add the low pass and high-pass images together â— Donâ€™t: â—‹ average the two images â—‹ do a weighted combination of the two images â— just add them (and clip if necessary) Happy Revising! Computer Science / Software Engineering Notes Network (not my hybrid, add a suggestion if you know whose it is) (- Thanks Matthew Barnes, god of notes (this is not a real hybrid lol)) Go look at the bonus lecture yourself: http://comp3204.ecs.soton.ac.uk/lectures/pdf/VisionRetrospective.pdf Computer Science / Software Engineering Notes Network TL;DR The TL;DRs are TL;DRs themselves;so much content The TL;DRs are actually pretty long, thereâ€™s just a lot of important content. Part 1:Mark See collab TL;DR below Jonâ€™s part. Mark (formatted by Joshua Gregory) Part 2:Jon Summaries from end of slides and hand-out notes. Lec 1 A computer vision system is not just software:it is any hardware, so ftware and wetware (i.e. humans) that make up the complete system. To engineer a computer vision system, you need to think about how you can achieve the required level of ro bustn ess by co n strain in g the problem at hand and incorporating sufficient in varian ce to potentially changing environmental factors. â— Ro bust and rep eatable computer vision is achieved through engineered in varian ce and applied co n strain ts. â— C o lo ur-sp aces â—‹ RGB â—‹ HSV (Hue, Saturation, Value) Lec 2 Machin e learn in g is a fundamental part of high-level computer vision (interpreting what is seen, versus the lo w-level, which is all about processing the image). The standard computer vision pipeline goes from the image, through a process of feature extractio n , to a p attern reco gn itio n co mp o n en t that makes inferences. Machine learning is a standard way of train in g the pattern recognition system. â— Extractin g features is a key part of computer vision â—‹ Typically, these are n umerical vecto rs that can be used with machine-learning techniques. â—‹ Featurevecto rs can be compared by measuring distan ce. â— C lassificatio n learns what class to assign a feature to. â— C lusterin g groups similar features. Computer Science / Software Engineering Notes Network â— Sometimes youâ€™ll need to figure out the Euclidean distance between 2 points in the feature space. â—‹ Point p =(p1, p2, â€¦, pn) and q =(q1, q2, â€¦, qn) â— Cosine similarity another measure for vectors. It is not a distance measure. â— Similarity = 1 if vectors same direction;decreases as the angle increases. â— Classification:assign class labels, bin ary or multiclass. â— ML algorithm learns on p re-labelled train in g data. â— Hyp erp lan e through the feature space, sep arates the classes. â— Linear classifiers:think of it as the hyperplane is a straight lin e in the vector space. â—‹ Linear classifiers mo re efficien t than KNN, donâ€™t need training data and only need to check side of hyperplane for unlabelled point. â— Non-linear classifier:Sup p o rt Vecto r Machin e (SVM) â— K-n earest-n eighbo urs (KNN) â—‹ Find class of unlabelled vector by finding majority class of closest K training points. â—‹ Can become computationally exp en sive when lots of training example or large dimensions. â— K-Mean s â—‹ Algorithm to cluster data in feature space. â—‹ Clusters represented by cen tro ids. â—‹ Algorithm: â–  K value set before â–  Randomly chose centroids â–  Each point assigned closest centroid â–  Centroid recomputed as mean â–  If Centroid has no points assigned, randomly re-initialise â–  This is done iteratively until the centroids donâ€™t move. â—‹ K-mean s always co n verges, but n o t n ecessarily to mo st o p timal so lutio n . Computer Science / Software Engineering Notes Network Lec 3 Understanding the shap e of data in a feature space is important to effectively usin g it. In addition, by understanding the distributio n of really highly dimensional data, it is possible to determine the most important modes of variatio n of that data, and thus represent the data in a space with many fewer dimen sio n s. â— C o varian ce measures the â€œshapeâ€ of data by measuring how different dimensions change together. â— The p rin cip le axes are a basis, aligned such they describe most directions of greatest varian ce. â— The Eigen deco mp o sitio n of the covariance matrix produces pairs of eigen vecto rs (corresponding to the principal axes) and eigen values (proportional to the variance along the respective axis). â— PC A (Principal Component Analysis) aligns data with its principal axes, and allows dimen sio n ality reductio n by discounting axes with low variance. â— Eigen faces applies PCA to vectors made from pixel values to make robust low-dimensional image descriptors. â— Varian ce - how â€œspread-outâ€ the data is from the mean â—‹ n data points:xi ... â—‹ Î¼ mean â— C o varian ce - how 2 variables change together. â— Variance = covariance when 2 variables the same. â— Variable un co rrelated when covariance = 0. â— C o varian ce matrix - square symmetric matrix containing all the covariance values. â— Prin cip al axes o f variatio n - linearly independent vectors, orthogonal, act as basis â—‹ Major axis data most spread â—‹ Minor axis perpendicular to major Computer Science / Software Engineering Notes Network â— Eigen deco mp o sitio n o f co varian ce matrix â—‹ A is square matrix â—‹ v is non-zero eigenvector â—‹ Î» is eigenvalue â— If matrix A is covariance matrix, then eigen vecto rs are p rin cip al co mp o n en ts. â—‹ Vector with largest eigenvalue is first principal axis and so onâ€¦ â— The Eigen deco mp o sitio n is thus a way o f fin din g the p rin cip al axes â— PCA projects data in an original space to a new space defined by the basis of principal axes. â— Reduce the dimen sio n ality of the data, get rid of axes with small variance. PCA steps summary: 1. Mean-centre the data vectors 2. Form vectors in matrix Z, so each row corresponds to vector 3. Do Eigendecomposition of matrix Z TZ, to get eigenvector matrix Q and diagonal eigenvalue matrix Î›: a. 4. Sort columns of Q and values of Î› to decreasing order. 5. Select L largest eigenvectors of Q (first L columns) to make transform matrix QL. 6. Project original vectors into lower dimensional space, TL: a. â— Eigen faces - face recognition, PCA to features, represent images far fewer dimensions. â—‹ All images need to be same size, aligned â—‹ Also generative model Overall ap p ro ach Training images: 1. Images flattened to vectors 2. Mean vector computed and stored 3. Vectors mean centred 4. PCA applied, project to lower dimensional space. Transform matrix stored. 5. Low dimensional vectors used as training data for classifier a. KNN with distance threshold Testing image: 1. Image flattened to vector, mean vector subtracted 2. Vector projected by PCA basis (transform matrix) to lower dimensional space 3. Vector given to classifier, generates class label Computer Science / Software Engineering Notes Network Lec 4 Features that can be extracted from images fall into fo ur main categories. Glo bal features are extracted from the en tire image. Regio n -based features are extracted from regio n s of the image called co n n ected co mp o n en ts. Detecting connected components requires that you first apply a process called segmen tatio n to partition the image into multiple segments. Image features can be categorised into one of four main categories: 1. Global 2. Grid-based (block-based) 3. Region-based 4. Local, â€œinterest pointsâ€ â— A common type of global feature is a glo bal co lo ur histo gram. â— Region-based methods need regio n s to be detected - this process is called segmen tatio n ... many different approaches â— Connected components are segmen ts in which the pixels are all reachable by a connected path. â— Feature morphology refers to the form or shape of a feature â— Jo in t co lo ur histo grams - (global) accumulate binned pixel values. â— No rmalisatio n of histogram for image of different sizes. â— Colour space important. â— Segmen tatio n - (region) set of pixels share certain visual characteristics. â— Thresho ldin g simplest form, grey image â†’ binary image. â—‹ Work well when constrained, designed to stand out (QR codes) â— Manual thresholding value, or ... â— Otsuâ€™s thresho ldin g metho d - assumes 2 classes, bi-modal histogram. â—‹ Combined spread of 2 classes minimal (intra-class variance) â— A dap tive thresho ldin g:e.g. sets pixel to bg (background) if the value less than mean of neighbours plus offset, fg foreground otherwise. â— K-Mean s clusterin g for segmentation. â— C o n n ected C o mp o n en ts - segment where all pixels reachable from other (adjacently) Computer Science / Software Engineering Notes Network â—‹ â— C o n n ected-co mp o n en t labellin g - binary image â†’ set of connected components Two -p ass algorithm: First pass 1. Raster scan data 2. If element not bg, then get neighbour elements 3. No neighbours? Uniquely label current element 4. Otherwise find neighbour with smallest label, assign current element 5. Store equivalence between neighbouring labels Second pass 1. Raster scan again 2. If element not bg, then relabel element with lowest equivalent label Lec 5 Basic shap e descrip tio n involves extracting characteristic features that describe the shape of a co n n ected co mp o n en t. Shape descriptors can be used together with machin e learn in g, or man ually defin ed mo dels to classify shapes into different classes;a common example of this is classifying shapes into letters of the alp habet in order to achieve optical character recognition. Statistical shape models describe how the shape of an object (or set of objects) can change (through internal movement and/or through changes in pose). In addition to describing shapes, statistical shape models can be used to find instances of a shape in an image. â— Many different ways to describe the shap e of co n n ected co mp o n en ts, the choice depends on req uired in varian ce â— Multip le shap es can efficiently be represented by a RA G, very ro bust â— Point distribution models apply PC A to x-y coordinate pairs across multiple images to produce a lo w-dimen sio n al p arametric mo del â—‹ A SMs/C LMs also model local appearance, and can use this to optimise the fit of the model parameter to match an image. â— Scalar features (specifically of connected components) â—‹ A rea â—‹ Perimeter length â–  Inner border â–  Outer border Computer Science / Software Engineering Notes Network â–  Approximated by: â—‹ C o mp actn ess - ratio of area to perimeter squared: â–  â—‹ Irregularity (disp ersio n ) - ratio of major chord length to area. â–  â–  x and y bar are coordinates of centroid. â— Mo men ts - describe distribution of pixels in a shape (connected component) â— 2D Cartesian moment, order p and q: â—‹ â— Connected component simplifies to: â—‹ â— Zero o rder mo men t of connected component m00 is just the area. â— Centre of mass is (centroid): â—‹ â— C en tral Mo men ts - translation invariant, compute about the centroid â—‹ â— Î¼10 and Î¼10 both equal 0, so give no information, but higher order do. â— No rmalised cen tral mo men t both translation and scale invariant: â—‹ Computer Science / Software Engineering Notes Network â— C hain co des - encode boundary of object, step around object and note down direction of each step, either 4 or 8 directions. â—‹ Rotating the sequence so it is the smallest integer, makes it start p o sitio n in varian t. â—‹ Ro tatio n in varian ce achieved by storing difference between consecutive numbers. â—‹ Scale in varian ce theoretically by resampling shape, usually doesnâ€™t work well in practice. â—‹ Used to computer area, p erimeter and mo men ts directly! â—‹ Not good for shape matching due to â–  Noise, resampling, problem generating good similarity/distance metrics. â— Fo urier descrip to rs - encode shape information by decomposing boundary into set of frequency components. â— Two main steps, choose carefully to make very in varian t bo un dary descrip to rs: â—‹ Define representation of curve (boundary) â—‹ Expanding representation using Fourier theory â— Regio n A djacen cy Grap hs (RA G) - (trees) describe layo ut of connected components relative to each other. â—‹ Nodes correspond to components and connected if shape border â— Invariant to:distortion (including rotation, scale, translation, non-linear transformations) â— Not invariant to â— Po in t D istributio n Mo dels (PD M) - Like eigenfaces, applies similar process to set of points representing a shape â—‹ Corresponding 2D points manually created from set of N training face images â–  Number of points fixed at M â—‹ Iterative process called Generalized Procrustes Analysis, align points â—‹ Mean shape created â—‹ Shape matrix created â–  Each column stores information on how the x or y ordinate of a point on a face can change. â—‹ PCA applied to matrix. â— A ctive Shap e Mo dels (A SM) / C o n strain ed Lo cal Mo dels (C LM) - take PDM further, incorporate what image should look like around each point. â—‹ Small pixel patch about each point - is template. â—‹ Addition of data allows model to be better fitted to unseen image â–  Each point tried to move to local optimum while PDM contains all points â— A ctive A p p earan ce Mo dels - same thing as ASMs/CLMs, but instead of local features, try to jointly optimise global appearance of face against PDM. Computer Science / Software Engineering Notes Network Lec 6 The detection of lo cal in terest p o in ts that are stable under varying imaging conditions has a huge number of ap p licatio n s in computer vision. Research in this area goes back as far as the 1960s and 70s. The Harris an d Step hen s co rn er detectio n techn iq ue developed in 1988 is a classic example of a detection technique with imp ressive ro bustn ess. A related problem to the detection of interest points is the p ro blem o f scale (the size at which an object appears in an image). Scale sp ace theo ry allows interest point techniques to be developed that are in varian t to changes in scale (i.e. the object moving further away). The D ifferen ce-o f-Gaussian blo b detecto r is an example of such a scale-space blob detection technique. â— In terest p o in ts have loads of applications in computer vision. â—‹ They need to be ro bustly detected, and invariant to rotation, lighting change etc. (sufficient but not too much texture variation in local neighbourhood) â— There are 2 types:co rn ers and blo bs â—‹ Harris & Step hen s is common co rn er detecto r â—‹ Finding extrema in a multi scale D o G p yramid provides robust blo b detecto r â— Scale sp ace theo ry allows us to find features (corners and blobs) of different sizes â— Harris & Step hen s C o rn er D etecto r - classic algorithm. â—‹ Considers the brightness of a small patch of image. â—‹ Then when you slightly shift that patch then â–  If brightness same, original patch not stable point â–  If brightness changes a lot, original patch stable â— Computing the weighted sum-squared difference of window and shifted window: â—‹ Point (x,y) and shift (âˆ†x, âˆ†y): â— This eq uatio n can be â€œp ro cessedâ€ to make: â— Square symmetric matrix M concisely describes the shape of the local weighted difference function. Computer Science / Software Engineering Notes Network â— Basically encoding of image derivatives in the x,y and xy directions. â— Called the seco n d mo men t matrix aka structure ten so r. â— You can use the abso lute values of the eigen values directly (explained somewhere else) â— But Harris & Stephens came up with a scheme that avo ids exp licitly co mp utin g the Eigen deco mp o sitio n by formulating a co rn er resp o n se fun ctio n (R(x, y) in terms of the determinant and trace of M: â— â— K usually 0.04 - 0.06 â— Value of R: â—‹ R > 0:Corner â—‹ R < 0:edge â—‹ | R | = small:flat â— Actually find corners, co mp ute R for each pixel, keep only ones over threshold. â— Filter out points not local maxima of R within small window (8 neighbouring pixels) â— Scale sp ace theo ry - formal framework for handling images at different scales by represent image as family of smo o thed images p arameterized by the size of the smo o thin g kern el used for suppressing fine detail. â—‹ Scale parameter:t, image structure of spatial size smaller than ~sqrt(t) mostly smoothed away in scale-space level at scale t. â— Gaussian Scale Sp ace - smoothing function is Gaussian kernel â—‹ Formally the Gaussian scale space of an image f(x, y) is the family of derived signals L(x, y;t) defined by the convolution of the image with the 2D Gaussian kernel: â—‹ â— Semicolon implies convolution only performed over variables x and y. â— Definition valid for all t â‰¥ 0 â— Gaussian Pyramids. â— Multiscale Harris & Step hen s - define Gaussian scale space with fixed set of scales and compute corner response function at every pixel of each scale, keep only those with response function above threshold. â— Lo cal extrema in D o G Scale Sp ace Computer Science / Software Engineering Notes Network â— Lap lacian o f Gaussian (LoG) is the 2nd differential of a Gaussian co n vo lved with an image (the kernel is shaped like a Mexican hat). â— Zero -cro ssin gs of function, get Marr-Hildreth edge detecto r. â— By finding local max and min, you get blo b detector. â— Very useful p ro p erty: if blob is detected at (x0,y0;t0) then in the scaled image (factor of s) same blob detected at (sx0,sy0;s 2t0) â— In practice, D o G ap p ro ximatio n used instead of LoG. â—‹ Build gaussian scale space, subtract adjacent scales to produce DoG scale space to then search fo r extrema. â— Every time you double scale, you can half image size, pyramid can be constructed. Lec 7 How to extract lo cal features from these in terest p o in ts. A number of techniques have been proposed in the past in order to extract robust local descriptors, starting from simp le p ixel histo grams, through to advanced features like the SIFT descrip to r, which en co de weighted lo cal image gradien ts. Once these descriptors have been extracted, they can be used for image matchin g. â— Features extracted around interest points have lots of practical uses in computer vision. â— Matching scenarios basically fall into two catego ries: â—‹ Narro w-baselin e where the difference in image is slight â–  Lo cal image temp lates are often suitable descriptors â—‹ Wide-baselin e where there are bigger differences in pose â–  Gradien t histo gram descriptors are good here â— Esp ecially SIFT! â— Lo cal feature matchin g basics - find all local features in one images that have correspondences in another image. â— Applications:image retrieval, 3D reconstruction, panoramas, trackingâ€¦. â— Stereo vision, 2 concepts: â—‹ Narro w-baselin e stereo is where 2 images very similar - local features moved by few pixels. â—‹ Wide-baselin e stereo where differences much bigger. â— Ro bust lo cal descrip tio n - type of descriptor dep en dan t on task. â—‹ Narro w-baselin e, rotation not issue, descriptiveness not too important, lighting not changed much. â—‹ Wider baselin es, local descriptors with attributes: â–  Ro bust to uniform in ten sity chan ges in pixel values around interest point. â–  In varian ce to o rien tatio n , features not change as image rotated. â–  Ro bustn ess to placing of interest point by few pixels â–  Descriptors for visually differing local regions be unique and far apart in feature space. Computer Science / Software Engineering Notes Network â— Matchin g by co rrelatio n (temp late matchin g) - rectangle region around interest point, use pixel values directly. â—‹ Sum-Sq uared-D ifferen ce perform basic matching. â—‹ Work well with small differences, but not in wide-baseline. â— Lo cal In ten sity Histo grams - histogram of pixel intensities of local region. â—‹ C ircular window, (mostly) ro tatio n in varian t. â—‹ Can use Gaussian weightin g. â—‹ Not invariant to illumination changes. â—‹ Histograms not very distinctive :( â— Lo cal Gradien t Histo grams - encode gradient directions within a window â—‹ Easy to compute in x and y directions. â—‹ â—‹ Magnitude of gradient: â—‹ â— Gaussian weighting can be applied â— Invariant to uniform intensity changes. â— No t ro tatio n in varian t! â—‹ Have to robustly compute â€œdominant orientationâ€ subtract from pixels orientation to be rotation invariant. â— The SIFT Feature - builds sp atial array of o rien tatio n -magn itude histo grams about the interest point. â—‹ Pixel contributions lin early in terp o lated across nearest spatial bins avo id disco n tin uities â—‹ Magn itudes weighted by Gaussian centred on interest point â—‹ Typical:4x4 spatial grid, 8 orientation bins;128 dimensional feature vector. â–  Very descriptive and discriminative feature â— Matchin g SIFT Features - often compared using Euclidean distance â— Take each feature from first image and find closest in second image. â—‹ Thresho ld to stop poor matches â—‹ However, tends to result in lots of incorrect matches, not robust to big viewpoint changes. â—‹ Better:take each feature from the first image, find 2 closest in second image, only form match if ratio of distances between closest and second closest is less than threshold (typically 0.8) Computer Science / Software Engineering Notes Network Lec 8 When comparing two images by matching local feature, we need to elimin ate mismatches. By applying geo metric co n strain ts to the problem of finding corresponding interest points between pairs of images, it is possible to both reduce the number of mismatches and potentially learn a geo metric map p in g between the two images. Amongst many other applications, these transforms can be used to build panoramas from multiple images. The presence of such a transform is a good indicator for a match between the two images, and is commonly used in object recognition and image retrieval applications. â— In co n sisten t lo cal feature matches can be removed by assuming some form of co n strain t holds between the two images â—‹ This is usually a geo metric map p in g â–  A ffin e tran sfo rm or Ho mo grap hy â–  Can be estimated by finding the least-squares solution of a set of simultaneous equations â—‹ Robust methods such as RA NSA C allow in liers and o utliers to be determined whilst learning the mapping â— Interest point matching is slo wâ€¦ â—‹ K-D Trees and Hashin g can help â— A p p lyin g co n strain ts to matchin g - even best local features can be mismatched â—‹ Tradeoffs in distinctiveness: â–  Can be too distinctive or not enough, not match with subtle variations or not match at all â—‹ You can estimate which correspondences are in liers or o utliers â— Geo metric map p in gs - assume object is flat, then can search for geometric mapping satisfied by correct corresponding points. â—‹ It is a tran sfo rm fun ctio n maps x,y coords from 1 image to other â–  â–  T is transform matrix â–  x,y column vectors for coordinates â— A ffin e Tran sfo rm - combination of translation, scaling, aspect ratio, rotation and skewing. â— Affine transforms always p reserve p arallel lin es â—‹ Standard 2D transform: â—‹ â—‹ A is 2x2 transform matrix encodes scale, rotation, skew, and b is translation vector. â—‹ Single matrix multiplication (extra dimension of fixed value 1): Computer Science / Software Engineering Notes Network â—‹ â—‹ 3x3 matrix: â—‹ â—‹ Said to have 6 degrees o f freedo m. â— Doesnâ€™t allow for perspective effects, as it preserves parallel lines â— 2D p ro jective tran sfo rm or (Plan ar) Ho mo grap hy: â— â—‹ Normalization by w because non-linear. â–  The vector [wxâ€™,wyâ€™,w] T is called a ho mo gen eo us co o rdin ate. â–  Deal with transform as matrix â—‹ a-f are affine parameters. â—‹ g-h define keystone distortions, make originally parallel lines come together after transform. â— Solve set of homogeneous linear equations for T â—‹ Common to compute least-sq uares estimate of transform matrix. â–  It minimises the sum-squared error in prediction. â–  Error of single point called residual;difference between predicted and observed value. â— Ro bust estimatio n - least squares problem with noise. â—‹ Mismatches can throw off estimated transform â— How to determine inliners, we need them for better transform estimate â— Number of algorithms one is RA NSA C â— Ran do m Samp le C o n sen sus â— Algorithm: â—‹ A ssume: M data items required to estimate model T, N data items in total 1. Select M data items at random 2. Estimate model T 3. Find how many of the N data items fit T within tolerance tol, call this K. points that have an absolute residual less than tol are the inliers;the other points are outliers. 4. If K is large enough, either accept T, or compute least-squares estimate using all inliers, and exit with success. 5. Repeat steps 1..4 nIterations items Computer Science / Software Engineering Notes Network 6. Fail - no good T fit of data â— RANSAC picks some pairs of points randomly, estimates transform. â— If enough inliers, algorithm stops, transform re-estimatedâ€¦â€¦ â— Imp ro vin g matchin g sp eed - biggest problem is speed. â—‹ Brute-force of 128-D SIFT takes long. â— Ways to speed it up:K-D Trees â—‹ Binary tree structure â—‹ Each node splits specific dimension of space in two â—‹ Leaf nodes store number of points corresponding to the points that have made it down the tree to that point â—‹ Fast nearest neighbour search doneâ€¦â€¦. â— Hashin g â— Locality Sensitive hashing Functions, vectors that are spatially similar, similar hash codes. â— Sketchin g - binary string encodings for features, compared more efficiently Lec 9 C o n ten t-based image retrieval (C B IR) are systems that can search for images with an image as the q uery. Research on CBIR systems started in the early 90â€™s, but it is only more recently with ubiquitous mobile computing and applications like Google Goggles that the technology has matured. Weâ€™ve seen how (lo cal) descrip to rs can be used to find matchin g o bjects within images, but weâ€™ve also seen that the matching process is rather co mp utatio n ally exp en sive. For CBIR applications we need to be able to search datasets of millio n s of images almost in stan tan eo usly. In the field of textual do cumen t search, techniques to efficiently index and efficiently search massive datasets of text documents are well understood. One of the biggest advances in CBIR has been to apply these textual indexing techniques to the image domain by extracting bags o f visual wo rds from images and in dexin g these. â— Effective and efficien t text search can be achieved with bags o f wo rds, the vecto r-sp ace mo del and in verted in dexes. â— Vecto r-q uan tisatio n can be applied to local features, making them into visual wo rds. â—‹ Then you can apply all the same techn iq ues used for text to make efficient retrieval systems â–  This is a good way of making highly scalable, effective and efficient co n ten t-based image retrieval systems â— Most text-search systems represent text as a bag o f wo rds â—‹ A bag is an unordered data structure like a set, but can have elements multiple times. â—‹ Create bag from text document: â–  Break into words (tokenisation) Computer Science / Software Engineering Notes Network â–  Process words to reduce variability (eg get rid of â€œingâ€ at end) â–  Remove common words (like â€œtheâ€) â— Vecto r-sp ace mo del - text documents represented by vectors â— Vectors contain counts of frequency of words in the lexico n (the set of all possible words) â—‹ Histo grams o f wo rd co un ts, and they are highly sp arse. â— Searching: â—‹ Query turned into vecto r form, ranked by similarity. â—‹ C o sin e similarity often used, less affected by magnitude. â—‹ Many documents have 0 similarity. â—‹ Often cosine sim can be weighted â–  Words that ap p ear a lo t in docs should have less weight â–  Common weighting scheme:Freq uen cy-in verse do cumen t freq uen cy (tf-idf) â— In practice vectors never made, BoW just indexed directly as in verted in dex. â—‹ Map o f wo rds to p o stin gs lists. â–  Po stin g is a p air containing document identifier and word count. â–  Only made if word count > 1 â— Can quickly find which docs a word occurs in, and how many. â—‹ Really efficient computation. â— Vecto r-q uan tisatio n - lossy data co mp ressio n technique â— K-Mean s clusterin g used to learn fixed size set o f rep resen tative vecto rs. â— Represent a vector by another approximate vector, draw from a p o o l o f vecto rs. Each input vector assigned to â€œclosestâ€ vector from pool. â— B ag o f Visual Wo rds (B o VW) - apply text techniques to computer vision â— A visual wo rd is a local descriptor vector (e.g. SIFT vector) that has been vector quantised. â—‹ Set of representative vectors is the visual eq uivalen t o f the lexico n (codebook) â—‹ SIFT - each visual word represents a p ro to typ ical p attern of lo cal image gradien ts â— Set of local descriptors can be transformed to fixed dimen sio n ality histo gram by counting the number of o ccurren ces of each representative vector. â— B o VW Retrieval - use same techniques for text retrieval. â—‹ Visual words indexed directly, searched by cosine â—‹ Important parameter:size of codebook. â–  Only works well efficiently if vectors sparse â–  Ensure visual words distinctive to minimize mismatching â—‹ Implies you need large codebookâ€¦ ~1 million typically â—‹ This is long. â—‹ And expensive. â— Overall process: â—‹ Find interest points + extract local features from all images Computer Science / Software Engineering Notes Network â—‹ Learn codebook from sample of features â—‹ Perform vector quantisation to assign each feature to a representative visual word â—‹ Construct an inverted index Lec 10 Weâ€™ve looked at how features can be extracted from images, and how sup ervised machin e learn in g techn iq ues like lin ear classifiers and k-n earest n eighbo urs can be used to train a computer vision system to predict a class for a particular feature input. Current research is looking at how we might make computers able to see in the human sen se, fully understanding the content of a visual scene. The choice of feature for image classification is very important. We saw how a B ag o f Visual Wo rds (B o VW) representation was a p o werful techn iq ue for image search. It turns out that B o VWs are very useful for use in high p erfo rman ce image classificatio n . â— Object reco gn itio n , scen e classificatio n and auto matic an n o tatio n are all important tasks in computer vision. â—‹ Researchers are striving to n arro w the â€œseman tic gap â€ between what computers can perceive, compared to humans, â— The B o VW approach lends itself to high-performance image classification â—‹ Performance is increased if the local features are samp led den sely â— Typical system takes in image, passes through feature extracto r and eventually feeds features to machine learning system to make decisions. â— Supervised ML algorithm uses p re-labelled train in g data for assigning class labels to vectors â— B in ary classifier - 2 classes â— Multiclass classifier - multiple classes â— Multilabel classifier can predict multiple labels or classes. â—‹ With probabilities/confidences. â—‹ Often called auto matic image an n o tatio n or auto -an n o tatio n â—‹ D o esn â€™t determine where in an image a thing is, just looks for p resen ce of the thing. â—‹ Object reco gn itio n attempts to localise an o bject and determine class. â— C urren t research challen ges â—‹ Unconstrained object recognition in natural scenes â—‹ Global classification of images into scene categories/events/topics. â—‹ Automatic annotation of large sets of imagery. â— Fundamental problem to solve:make co mp uters see images like human s. â—‹ â€œSemantic understandingâ€ - overall meaning of image â— Seman tic Gap â— Histo ry o f ap p ro aches Computer Science / Software Engineering Notes Network â—‹ Historically BoVW has been important. â—‹ The histograms created also powerful global descriptors and object detectors â—‹ A uto A n n o tatio n treated like lan guage tran slatio n with seman tic sp ace. â—‹ Raw features from SIFT, and quantised descriptors of segmen ts even co lo ur of pixels. â—‹ Codebook needs to be much smaller for ML, for performance and effectiveness â—‹ Visual words can be less distin ctive, little more variation â—‹ Number of visual words few hun dred, up to a few tho usan d. â—‹ Classification improved by samp lin g image at greater rate â–  D en se SIFT, densely sampled grid, rather at interest points â–  Pyramid D en se SIFT, Gaussian Scale Space â—‹ BoVW representations augmented with sp atial p yramid;sub-histograms of visual word occurrences for overlapping windows in image â–  Improves performance, as learns where in image objects likely appear. â–  Pyramid Histo gram o f Wo rds (PHOW) â— D evelo p in g / B en chmarkin g B o VW scen e classifier â— Use stan dardised datasets when developing/comparing results of classifiers. â—‹ Train in g and test data sets â—‹ On ly train in g set used to train classifier â—‹ Test set withheld. â— Raw features extracted from training images. â—‹ Codebook of features, use k-means. Un ifo rm ran do m samp le of all features rather than all of them for speed. â— Apply vecto r q uan tisatio n to raw features, count the number of occurrences to build histo grams of visual words for each image. â— Classifiers trained using histograms â— Might train on subset of training data to â€œvalidateâ€ and â€œo p timiseâ€ parameters. â— Re-train with all train in g data. â— Can now ap p ly classifiers to test data: â—‹ Extract features. â—‹ Quantise the features (using codebook) â—‹ Compute occurrence histograms â—‹ Use classifiers to find most likely class â— Lots of ways to evaluate the p erfo rman ce of classification on test set. â—‹ Simplest is average p recisio n â–  Proportion of n umber o f co rrect classificatio n s to to tal n umber o f p redictio n s Computer Science / Software Engineering Notes Network TL;DR By Mark Towers Mark (edited by Joshua Gregory) â€œThese notes may miss something, please say if I have. Thanksâ€ - Mark â— Lecture 1:Eye and Human vision â—‹ The human eye is a complex machine, with cameras borrowing several similar ideas like retin a, len s. The image coming into the eye is flip p ed upside down by the lens with the brain automatically flipping the image without thinking. â—‹ The retin a (the light sensitive part of the eye) with rods for low-light and black-white vision while the co n es provide co lo ur vision. However cones are most sen sitive to green then red then blue. â—‹ Mach ban ds are an optical illusion whereby the contrast between edges of slightly differing shades of grey is exaggerated when they are touching. â—‹ Neural n etwo rks are an attempt to replicate the way that the brain works for vision â— Lecture 2:Image formation â—‹ When deco mp o sin g an image into bits, the mo st sign ifican t bits have the largest in fluen ce on the image. As the bit position increases, the more information is represented. â—‹ Reso lutio n is the number of pixels for the width and the height of the image â—‹ Fo urier tran sfo rmatio n :Any periodic function can be converted to the sin e and co sin e waves of different frequencies. It is also possible to reco n struct signals from its fourier transform. The magn itude and the p hase, is calculated by Pythagoras' theorem and the angle of the hypotenuse in the complex plane. ğ¹ğ‘( ğ‘¤) = â„‘( ğ‘( ğ‘¡) ) = âˆ’ âˆ âˆ âˆ« ğ‘( ğ‘¡) ğ‘’ âˆ’ ğ‘—ğ‘¤ğ‘¡ ğ‘‘ğ‘¡ |ğ¹ğ‘( ğ‘¤) | = ğ‘…ğ‘’( ğ¹ğ‘( ğ‘¤) ) 2 + ğ¼ğ‘š( ğ¹ğ‘( ğ‘¤) ) 2 ğ‘ğ‘Ÿğ‘”( ğ¹ğ‘( ğ‘¤) ) = ğ‘¡ğ‘ğ‘› âˆ’ 1 ( ğ¼ğ‘š( ğ¹ğ‘( ğ‘¤) ) ğ‘…ğ‘’( ğ¹ğ‘( ğ‘¤) ) ) â— Lecture 3:Image sampling â—‹ A liasin g is an effect that causes different signals to become in distin guishable when sampled. It also refers to the distortion or artifacts that results when a signal reco n structed from samples is different from the original continuous signal. â—‹ Samp lin g sign als:if an original signal is a continuous function, then digitally sampling it requires a go o d samp lin g freq uen cy. So the higher the sample rate, the better rep resen tatio n of the signal can be captured. Computer Science / Software Engineering Notes Network â—‹ Wheel mo tio n is an example of where sampling rate matters in vision as if the sample rate and the wheel rotations per second are synchronous then it can make it seem like the wheel is moving in reverse. â—‹ Sampling theory known as Nyq uistâ€™s samp lin g theo rem for one-dimension says that for each point of interest (pixel or musical sample), twice the frequency should be gathered. â—‹ It is possible to reco n struct a signal from transformed components using the fourier transformation. This means the fourier transforms have a fo rward (to transform from a time space to frequency space) and a backward transformation called the inverse. Where the variables x and y are for space, u and v are for frequency and is pixel x and y in the image.ğ‘ƒ ğ‘¥, ğ‘¦ â—‹ Shiftin g an image doesnâ€™t affect the magn itude of the fourier transform of the image, but it does affect the p hase. â—‹ Ro tatin g the image does affect the fourier transform and rotates it in the same way. â—‹ Filterin g:by finding the phase of an image by the fourier transform, this gives access to the freq uen cy co mp o n en ts of the image. This can be used to find the high detail parts of the images by a high-p ass filter or all of the low level details with a lo w-p ass filter. â—‹ Additional transformation types:fo urier, discrete co sin e and hartley. â—‹ A p p licatio n s of the 2D fourier transformation:un derstan din g and an alysis of images, sp eedin g up algorithms through the use of the function, representation in varian t p ro p erties (rotation and shift), coding of images by magn itude and p hase and recognition and un derstan din g of textures. â— Lecture 4:Point operators â—‹ Image histo grams are a graph of the freq uen cy of brightn ess for an image (global feature) â—‹ An image can be brighten ed using the formula below where N is the new image, O is the old image, k is the gain, l is the level and x, y is the coordinates. â—‹ In ten sity map p in g allows for the changing of the o utp ut brightn ess (intensity) following this function where it is limited to the min and max values according to the second function. Where Nmax is the maximum limit, Nmin is the minimum limit, Omax is the maximum output and Omin is the minimum output and Ox,y being the old image coordinates and Nx, y being the new image coordinates. Computer Science / Software Engineering Notes Network â—‹ Histo gram eq ualisatio n improves contrast in images. â—‹ Equations: â—‹ Thresho ldin g is a way of turning a greyscale image into a black-white image by checking if a pixel value is greater than a thresho ld value. If true then the pixel is set to 255 otherwise set to 0. â— Lecture 5:Group operators â—‹ Temp late co n vo lutio n applies a kern el matrix turning a group of pixels to a sin gle value. Template convolution includes co o rdin ate in versio n in the x axis and y axis, however this is not needed if the template is symmetric. â—‹ It is possible to apply template convolution via the fo urier tran sfo rm such that is the fourier transform of the picture and is the fourierâ„‘( ğ‘ƒ) â„‘( ğ‘‡) transform of the template and is the point by point multiplication.. ğ‘¥ â—‹ The gaussian function is used to calculate the template values and a compromise between the varian ce and win do w size. This allows forÏƒ 2 gaussian averagin g by using the gaussian function for each element in a matrix. â—‹ There are more advanced ways of doing gaussian function like non-local means and image ray transform. â— Lecture 6:Edge detection â—‹ It is very useful to find the edge of an image. â—‹ First o rder edge detectio n :There are three equations, one for vertical edges, one for ho rizo n tal edges and vertical + horizontal edges. Where ğ‘ƒ ğ‘¥, ğ‘¦ Computer Science / Software Engineering Notes Network is a point at x and y. It is possible to rearrange the to the taylorğ¸ğ‘¥ ğ‘¥, ğ‘¦ expansion for to the first derivatives .ğ‘“( ğ‘¥ + âˆ†ğ‘¥) ğ‘“ ' ( ğ‘¥) â—‹ Edge detection in vector format where the magnitude is and theğ‘€ 2 ğ‘¥ + ğ‘€ 2 ğ‘¦ direction is . Examples are the Prewitt operator or So belğ‘¡ğ‘ğ‘› âˆ’ 1 ( ğ‘€ ğ‘¦/ğ‘€ ğ‘¥) operator. â— Lecture 7:Further edge detection â—‹ The can n y edge detection algorithm can be broken down to 5 different steps i. Apply Gaussian filter to smooth the image in order to remove the noise ii. Find the intensity gradients of the image iii. Apply non-maximum suppression to get rid of spurious response to edge detection iv. Apply double threshold to determine potential edges v. Track edge by hysteresis:Finalize the detection of edges by suppressing all the other edges that are weak and not connected to strong edges. â—‹ It is formulated with three main objectives. Computer Science / Software Engineering Notes Network i. Op timal detectio n with no spurious responses ii. Good lo calisatio n with minimal distance between detected and true edge position iii. Sin gle resp o n se to eliminate multiple responses to a single edge. â—‹ In terp o latio n in non-maximum suppression. Interpolate the gradient along the gradients (plus and minus a certain distance) and check if center is larger than neighbours. â—‹ Hysteresis thresholding transfer function. There are 2 thresho ld values. As a continuous process, when the brightness goes below the lo wer thresho ld, it is set to black, but it will only be set to white if the brightness goes above the up p er thresho ld. â—‹ The Lap lacian o p erato r is a different operator that creates a new complex gaussian like operator with an extremely complex math function. (I really hope you don't have to remember this function - Mark) â—‹ Zero -cro ssin g detectio n - average and sum the 4 corners around a pixel. If the signs change between summations, you have a detection. Computer Science / Software Engineering Notes Network â—‹ Marr-Hildreth edge detectio n - application of Laplacian of Gaussian (or Difference of Gaussian) and zero-crossing detection. â— Lecture 8:Finding shapes â—‹ It is important to be able to extract features from an image. This can be done by finding the difference between lo w and high thresho ld. While this can identify high contrast parts of the image however this doesnâ€™t help identify shapes in the image. â—‹ Temp late matchin g:This is a technique for finding small p arts of an image that match a temp late image. It uses co rrelatio n and co n vo lutio n and implementation via Fo urier. â—‹ Template matching doesnâ€™t work with n o isy images or for o ccluded images (obstructions). â—‹ Ho ugh tran sfo rm is a feature extraction technique that can find imp erfect in stan ces of o bjects within a certain class of shap es by a vo tin g p ro cedure. The voting procedure is carried out in a p arameter sp ace from which o bject can didates are obtained as a lo cal maxima in a so-called accumulato r sp ace. Hough transform has the same performance equivalent to temp late matchin g but is faster to compute. Basically, straight lines in the image are represented as points in the m,c plane and vice versa. â— Lecture 9:Finding more shapes â—‹ Ho ugh tran sfo rm can be used for not only lines but for circles or arbitrary shap es. To do so uses a gen eralised ho ugh tran sfo rmatio n where it forms a discrete look-up tables called the R-table. It vo tes via a look-up-table. The o rien tatio n is turned by ro tatin g the R-table voting and scaling by scalin g the R-table. This has inherent problems with discretisatio n (the process of transferring the continuous functions to the discrete voting table). â— Lecture 10:Applications/Deep learning â—‹ Computer vision is used in in dustry for q uality co n tro l or in academia for face reco gn itio n . Nowadays, computer vision uses mo dern hardware and mo dern cameras to achieve what we understand by â€œsightâ€. Currently there is n o techn iq ue for all p ro blems and is an active area of research in both industry and academia. Jon â— Lecture 1:Building machines that see â—‹ Key terms in designing computer vision systems as you want your system to be ro bust an d rep eatable, the system design to be in varian t an d the co n strain ts to wo rk. i. Ro bust - Changes to the en viro n men t will not affect the accuracy of the system Computer Science / Software Engineering Notes Network ii. Rep eatable - A measure of robustness, such that the system must work the same over and over regardless of the environmental changes iii. In varian t - An environmental factor that helps achieve robustness and repeatability. Hardware and software can be designed to be invariant to certain environmental changes such as invariant to illumination changes. iv. C o n strain ts - Are applied to the hardware, software and wetware (humans in the system) to make sure the vision system works in a repeatable and robust fashion. An example is putting the system in a box so there canâ€™t be any illumination changes. â—‹ For in dustry visio n , there are co n strain ts ap p lied i. So ftware co n strain ts:Simple and fast algorithm are the most popular as it means that there is a smaller chance of failure compared to deep learning approaches. Therefore Hough transform is a popular algorithm however this requires physical constraints to make it robust. So the use of colour is an important software feature as some colour-spaces don't encode the whole human vision or does not encode the luminance of a colour. ii. Physical co n strain ts - In industry environments, it is possible to constrain the physical environment e.g. lighting, enclosure, mounting, camera specs, optics and filters. â—‹ However in the wild, man y o f these co n strain ts are n o t able to be guaran teed therefore as many hardware an d wetware co n strain ts are applied with the software taking up the slack. E.g. the colour information is often less important than lumination. â— Lecture 2:Machine learning for pattern recognition â—‹ For computer vision application, they normally take the fo rm o f image â†’ feature extracto r â†’ machin e learn in g â†’ results. This is as the feature extractor converts the raw image into a feature vector which can then be learnt much easier for a machine learning algorithm (instead of learning the raw image). â—‹ Feature vecto r are just a mathematical vecto r with a fixed n umber o f elemen ts in it where the number of elements is the dimensionality of the vector. Each elemen t rep resen ts a p o in t in a featuresp ace or equally a direction in the featurespace. Therefore the dimensionality of a featurespace is equal to the dimensionality of every feature vector in it. â—‹ An aim of feature extractor is that they produce vectors that are similar to each other for similar images. This similarity is measured by fin din g the distan ce between the two feature vecto rs. There are different measures of distance i. Euclidean distan ce (L2 distan ce) - The straight line distance between two points that is computed via an extension of Pythagoras theorem to n dimensions. ğ· 2 ( ğ‘, ğ‘) = ğ‘–= 1 ğ‘› âˆ‘ ( ğ‘ ğ‘– âˆ’ ğ‘ ğ‘–) 2 = ||ğ‘ âˆ’ ğ‘|| Computer Science / Software Engineering Notes Network ii. Man hattan /Taxicab distan ce (L1 distan ce) - The distance along paths parallel to the axes of the space ğ· 1 ( ğ‘, ğ‘) = ğ‘–= 1 ğ‘› âˆ‘ |ğ‘ ğ‘– âˆ’ ğ‘ ğ‘–| = ||ğ‘ âˆ’ ğ‘|| 1 iii. C o sin e similarity - This isnâ€™t a distance but rather the angle between the two angles. This is useful if the relative length of the vectors donâ€™t matter. â—‹ Choosing a good feature vector representations for machine learning as better feature vecto rs allo ws easier distin guishin g o f o bjects o r classes o f in terest. Machine learning also becomes more difficult, the more features (dimensionally) therefore the smaller the number of features the better. â—‹ C lassificatio n is a sup ervised machin e learn in g that allo ws a co mp uter to â€œlearn â€ the assign in g o f class labels to an o bject (normally a feature vector of an image). To do this requires a pre-labelled dataset of an input and the correct output for the process. B in ary classifiers o n ly have two classes while multiclass classifier has man y classes. â—‹ Lin ear classifiers are typ e o f bin ary classifier where to tries to learn a hyp erp lan e that sep arates two classes in a featuresp ace with min imum erro r. (Do foundations of machine learning if you want to learn the specifies and cry a lo t). In order to classify a new image, the image is placed in the hyperspace then check which side of the hyperplane the image is determining the class of the image. â—‹ Some data can â€™t be lin early sep arable therefo re n o n -lin ear bin ary classifiers such as Kern el sup p o rt vecto r machin es. These machines can learn non-linear decision boundaries through the use of the kernel trick (foundations of machine learning has the mathematically details). However this has the p ro blem that the algo rithm must lo se gen erality by o verfittin g as the decisio n bo un daries is n o t limited to a straight lin e. â—‹ KNN is a multiclass classifier that assign s the class o f un kn o wn p o in ts based o n the majo rity class o f the clo sest K n eighbo urs in the featuresp ace. So for a new point then within a set radius, all of the points within the radius are found. With the new point class being determined by the majority class of the points. However this has p ro blems as it is co mp utatio n ally exp en sive if there are lo ts o f train in g examp les o r man y dimen sio n s. â—‹ K-mean clusterin g is an un sup ervised machin e learn in g algo rithm that aims to gro up data to gether in to classes witho ut an y p rio r kn o wledge (or labelled data). In terms of feature vectors, items are similar vectors should Computer Science / Software Engineering Notes Network be grouped together by a cluster operation. Some clusterin g o p eratio n s can be do n e usin g a p ro babilistic mo del in stead o f makin g a discrete cho ice. It follows the algorithm below i. The value of K (number of clusters) is chosen ii. The initial cluster centres are chosens (called the centroids) iii. Till the centroids dont move after an iteration â— Each point is assigned to its closest centroids â— The centroids are recomputed as the mean of all of the points assigned to it. If the centroid has no points assigned it to, it is randomly re-initialised to a new point. iv. The final clusters are created by assigning all of the points to their nearest centroids. â— Lecture 3:Covariance and Principal components â—‹ Varian ce is the mean sq uared differen ce fro m the mean . It is a measure of how spread-out the data is. It can be written as whereğ¸[ ( ğ‘‹ âˆ’ ğ¸[ ğ‘‹] ) 2 ] is the exception (basically the mean if the probability isğ¸[ ğ‘‹] = ğ‘¥ âˆ‘ ğ‘( ğ‘¥) ğ‘¥ equal). â—‹ C o varian ce measures ho w two variables chan ge to gether and the variance is the covariance when the two variables are the same so It can be written as .Ïƒ( ğ‘¥, ğ‘¦) = Ïƒ 2 ( ğ‘¥) . ğ¸[ ( ğ‘‹ âˆ’ ğ¸[ ğ‘‹] ) ( ğ‘Œ âˆ’ ğ¸[ ğ‘Œ] ) ] â—‹ C o varian ce matrix en co des ho w all p o ssible p airs o f dimen sio n s in an n -dimen sio n al dataset carry together square symmetric matrix. â—‹ Mean cen trin g is a p ro cess o f co mp utin g the mean (across a number of independent dimensions) as a set o f vecto rs which is then subtracted fro m the mean vecto r fro m every vecto r in the set. For each of the Computer Science / Software Engineering Notes Network dimensions then all of the vector are translated (by subtracting each point by the mean) so their mean position is the origin position. â—‹ Prin cip al axes o f variatio n - This can be computed using this algorithm: i. B asis is a set o f n lin early in dep en den t vecto rs in an n-dimensional space. This means that all of the vectors are orthogonal forming a coordinate system. ii. The first p rin cip al axis is the vecto r that describes the directio n o f greatest varian ce for a given set of n-dimensional data. iii. A ll o f the fo llo win g p rin cip al axis are o rtho go n al (p erp en dicular) to all o f the p revio us axis. E.g. second principal axis is orthogonal to the first axe while the third principal axis is orthogonal to the first and second principal axes. â—‹ Eigen vecto rs an d eigen values are fo un d by so lvin g the eq uatio n where A is a n -sq uare matrix, v is a n -dimen sio n al vecto rğ´ğ‘£ = Î»ğ‘£ (eigen vecto r) an d is the scalar value (eigen value). Eigenvectors andÎ» values have the following properties: i. A t mo st n eigen vecto r-value p airs ii. If A is symmetric, the set o f eigen vecto rs is o rtho go n al iii. If A is a co varian ce matrix, the eigen vecto r are the p rin cip al axes iv. The eigen values are p ro p o rtio n al to the varian ce o f the data alo n g each eigen vecto r. v. If all o f the eigen vecto rs are in dep en den t then the so rted list o f eigen vecto rs is eq ual to PC A axes. â—‹ Eigen deco mp o sitio n ( ) facto rizes a matrix in to eigen vecto rsğ´ = ğ‘„Î›ğ‘„ âˆ’ 1 an d a matrix where the diago n al are eigen values. If A is a real symmetric (i.e.covariance matrix) then (i.e. eigenvectors are orthogonal). Thisğ‘„ âˆ’ 1 = ^ allows finding of the principal components. â—‹ A linear transform projects data from one space into another space that can reduce the dimensions than the original. (e.g. ). This process can beğ‘‡ = ğ‘ğ‘¤ reversed if W is invertible (invertible) so this is not possible if the dimensional is changed. â—‹ Prin cip al co mp o n en t an alysis (PC A ) is an o rtho go n al lin ear tran sfo rm that map s data fro m its o rigin al sp ace to a sp ace defin ed by the p rin cip al axes o f the data. i. The tran sfo rm matrix W is just the eigen vecto r matrix Q fro m the eigen deco mp o sitio n o f the co varian ce matrix o f the data. ii. Dimensionality reduction can be achieved by removing the eigenvectors with low eigenvalues from Q (i.e. keeping the first L columns of Q assuming the eigenvectors are sorted by decreasing eigenvalue. â—‹ PC A algo rithm that allo ws fo r the reductio n o f dimen sio n s o f feature sp ace makin g it easier to learn by machin e learn in g algo rithms. It also means there are a smaller n umber o f dimen sio n s allo win g fo r greater ro bustn ess to n o ise, mis-align men t an d the do min an t features are cap tured. Computer Science / Software Engineering Notes Network i. Mean-centre the data vector ii. Form the vectors into a matrix Z, such that each row corresponds to a vector iii. Perform the eigendecomposition of the matrix to recover the eigenğ‘ ğ‘‡ ğ‘ matrix Q and diagonal matrix Î›: ğ‘ ğ‘‡ ğ‘ = ğ‘„Î›ğ‘„ ğ‘‡ iv. Sort the columns of Q and corresponding diagonal values of so thatÎ› the eigenvalues are decreasing. v. Select the L largest eigenvectors of Q (the first L columns) to create the transform matrix ğ‘„ ğ¿ vi. Project the original vectors into a lower dimensional space, .ğ‘‡ ğ¿ : ğ‘‡ ğ¿ = ğ‘ğ‘„ ğ¿ â— Lecture 4:Types of image feature and segmentation â—‹ There are fo ur main ways o f extractin g features fro m an image i. Glo bal - features are extracted from the contents of an entire image through image histo grams an d jo in t-co lo ur histo gram. ii. Grid/blo ck-based - The en tire image sp lit in to blo cks with a feature extracted fro m each blo ck. This allows for multiple feature vectors to be extracted. iii. Regio n -based - The en tire image is sp lit in to a n umber o f regio n s an d a feature is extracted fro m each allowing for multiple feature vectors to be extracted. iv. Lo cal - The in terest p o in ts o f the image are detected with a feature vecto r extracted fro m the surro un din g p ixels o f each in terest p o in ts. â—‹ Image histo grams - A simple global feature that finds the image histograms of the intensity (by averaging the colour band values). However this isnâ€™t particularly robust and canâ€™t deal well with multiple colours in the image. â—‹ Jo in t-co lo ur histo gram - The co lo ur sp ace is q uan tised in to bin s and we accumulate the number of pixels in each bin. This is technically a multidimensional histogram (due to each colour) that if flatten (unwrap) it can be used to make it a feature vector. â—‹ Image segmen tatio n - The p ro cess o f creatin g regio n -based descrip tio n s, basically creating groups (segments) of pixels that are Computer Science / Software Engineering Notes Network â€œsimilarâ€. This is normally pixels that share certain visual characteristics (colour, lighting, etc). i. Glo bal bin ary thresho ldin g - Takes a greyscale image an d assign s all p ixel with a value lo wer than a p redetermin ed thresho ld to o n e segmen t with all o ther p ixels to the o ther segmen t. This is really fast but req uires a man ually set static thresho ld making it not robust to lightning changes. Works well in applications with lots of physical constraints ii. Otsuâ€™s thresho ldin g metho d - Provides a way to auto matically fin d the thresho ld assumin g that there are o n ly two classes (i.e. foreground and background) so the histogram must have two peaks. So an exhaustive search fo r the thresho ld that maximises the in terclass varian ce iii. A dap tive/lo cal thresho ldin g - co mp utes a differen t thresho ld value fo r every p ixel in an image based o n the surro un din g p ixels. This is usually a square or rectangular window around the current pixel to define the neighbours. iv. Mean adap tive thresho ldin g - Set the curren t p ixel to zero if its value is less than the mean o f its n eighbo urs p lus a co n stan t values o therwise set to 1. This relies on the parameters:size of the window and the constant offset value. This means that it deals well with uneven lighting/contrast but it is computationally expensive (compared to other global methods) and can be difficult to choose the window size. As the scale of an object can break it. v. Segmen tatio n with K-mean s - C luster the co lo ur vecto rs [r, g, b] o f all the p ixels where each p ixel is assign ed to a segmen t based o n the clo sest cen tro id. This works best if the colour-space and sitane function are compatible (e.g. lab colour-space is designed so that Euclidean distances are proportional to perceptual colour differences. This do esn â€™t p reserve co n tin uity o f segmen ts so sin gle p ixel might en d up alo n e. Therefore also en co din g the p o sitio n is help ful but this should be normalised by the width and height of the image to remove the effect of different image sizes and scale x and y so they have more or less effect than the colour components. â—‹ Pixel co n n ectivity - A p ixel is co n n ected to an o ther if they are sp atially adjacen t to each o ther. There are two standard ways of defining this: 4-co n n ectivity an d 8-co n n ectivity rep resen tatio n the cardin al directio n s. â—‹ C o n n ected co mp o n en t labellin g - The process of detectin g co n n ected regio n s within a bin ary (segmen ted) image. There are a lot of different algorithms that have a variety of performance tradeoffs (memory vs time). An example algorithm is the two-pass algorithm: i. One the first pass, iterate through each element of the data by column then by row (raster scanning) â— If the element is not the background â—‹ Get the neighbouring elements of the current element Computer Science / Software Engineering Notes Network â—‹ If there are no neighbours, uniquely label the current element and continue â—‹ Otherwise, find the neighbour with the smallest label and assign it to the current element â—‹ Store the equivalence between neighbouring labels ii. On the second pass, iterate over the data in the same way â— If the element is not the background â—‹ Relabel the element with the lowest equivalent label â— Lecture 5:Shape description and modelling â—‹ It is possible to extract features fro m shap es by co n n ected p ixels. There are two types of p ixel bo rders: in n er an d o uter. i. The in n er bo rder is the set o f p ixels that are o utermo st p ixels within the shap e ii. The o uter bo rder is the set o f p ixels that the o utlin e o f p ixels o utside the shap e. â—‹ There are two ways of describing a shape:Region description and boundary description. i. Regio n descrip tio n : A simp le scalar shap e features â— Perimeter = Length around the outside of the component â— A rea = Number of pixels in component â— C o mp actn ess = How tightly packed the pixels are, often computed as the weighted ratio of area to perimeter squared. .ğ¶( ğ‘ ) = ( 4 Ï€ğ´( ğ‘ ) ) / ( ğ‘ƒ( ğ‘ ) 2 ) â— C en tre o f mass = The mean of x and y positions of all the pixels in the component â— Irregularity/D isp ersio n = How â€œspread-outâ€ the shape is. This is computed is two different ways:the ratio between the furthest point from the centre of mass and the area of the shape. The other is the ratio between the furthest boundary point and the closest boundary point. â— Mo men t = The distribution of pixels of shapes in a greyscale image with the order p and q and I(s) is the pixel intensity. For Computer Science / Software Engineering Notes Network the case of connected components, this is simplified to the second equation. These equations can be used to calculate the centre of mass: . But it is notğ‘¥ = ğ‘š 1 0 /ğ‘š 0 0 , ğ‘¦ = ğ‘š 0 1 /ğ‘š 0 0 invariant to translation, rotation or scaling however other moments exist. C en tral mo men ts = Translation invariant as it uses the centre of mass No rmalised cen tral mo men ts = Both scale and translation invariant â—‹ It is important to en co de the bo un dary o f a co mp o n en t, this is done in two ways:chain codes and chain code invariance. i. C hain co des:By walking around the boundary and encode the direction you take on each step as a number. They cyclically shift the code so it forms the smallest possible integer value (making it invariant to the starting point). ii. C hain co de in varian ce:This can be made rotation invariant by encoding the differences in direction rather than absolute values. It can also be made scale invariant by resampling the components to a fixed size (however this doesnâ€™t work well in practice). Computer Science / Software Engineering Notes Network â—‹ Regio n adjacen cy grap h - It builds a grap h fro m a set o f co n n ected co mp o n en ts where each n o de co rresp o n ds to a co mp o n en t an d a vertex (co n n ectio n ) between n o des exist if they share a bo rder. This can be used to easily detect patterns in the graph making it invariant to non-linear distortions but not to occlusions. â—‹ A ctive shap e mo dels an d co n strain ed lo cal mo dels (A SM/C LM) extends Point Distribution Model (PDM) by learning the local appearance around each point which is typically just an image template. Using a constrained optimisation algorithm, the shape can be optimally fitted to an image by using constraints. â— Lecture 6:Local interest points â—‹ In terest p o in ts - Points within an image that are â€œimportantâ€. Good interest points have: i. In varian ce to brightn ess chan ges ii. Sufficien t texture variatio n in the lo cal n eighbo urho o d iii. In varian ce to chan ges between the an gle / p o sitio n o f the scen e to the camera. â—‹ There are lots of different types of interest point types to choose from. i. C o rn er detectio n - By using a small window, the idea is that at a corner, there will be a significant change in all directions. Whereas if the window is at the edge then there is no change along the edge direction. And in a flat region there will be no change in all directions. This is dependant on measuring the change in intensity between a window and the shifted version. TODO maths ii. Blob detection (also called Difference-of-Gaussian Extrema) - TODO Computer Science / Software Engineering Notes Network â— Lecture 7:Local features and matching â—‹ Local features are used for:image alignment, object recognition, index and database retrieval, motion tracking, 3D reconstruction. â—‹ Lo cal features are fo un d by iden tifyin g all o f the in terest p o in ts in an image with a feature vecto r bein g extracted fro m in terest p o in ts fro m the surro un din g p ixels. â—‹ There are two distinct typ es o f matchin g p ro blem:narrow-baseline stereo and wide-baseline stereo. i. Narro w-baselin e stereo - When the local features in two images have only moved by a few pixels. Because of this the interest matcher, it doesnâ€™t need robustness to rotation and light. The search can be optimised for within the window area, so the detector doesnâ€™t need to identify a large number of points (low descriptiveness). ii. Wide-baselin e stereo - When the local features in two images have moved by large amounts (the feature has moved, rotated, etc in the image). Because of this the interest matcher, it needs to be robust to intensity change and invariant to rotation. The matcher also needs to be highly descriptive so that it identifies a large number of interest points in the image to avoid mismatches (but not so distinctive that you canâ€™t find any matches). Also robust to small localisation errors of the interest points, the descriptor should not change too much if it moves by a few pixels but to change more rapidly once it moves further way. â— There are a n umber o f p ro blems with wider baselin es: n o t ro bust to ro tatio n , sen sitive to lo calisatio n o f in terest p o in ts (small search win do w help s), wider baselin e can 't assume a search area (therefo re req uires co n siderin g all o f the in terest p o in ts between the two images). â—‹ In stead o f usin g p ixel p atches to iden tify in terest p o in ts, lo cal histo grams can be used in stead. To match each interest point in the first image to the most similar point in the second image (i.e. in terms of euclidean distance between the histograms). But this has the fo llo win g p ro blems: i. No t n ecessarily very distin ctive as man y in terest p o in ts likely to have similar distributio n o f greyscale image. ii. It is n o t ro tatio n in varian t if the samp lin g win do w is sq uare o r rectan gular but can be o verco me by usin g a circular win do w. iii. It is n o t in varian t to illumin atio n chan ges iv. It is sen sitive to in terest p o in t lo calisatio n , the point where the interest point is identified. â—‹ It order to o verco me lo calisatio n sen sitivity to i. A llo w the win do w to mo ve a few p ixels in an y directio n witho ut chan gin g the descrip to r. ii. By ap p lyin g a weightin g so that the p ixels n ear the edge o f the win do w have a smaller effect than the p o in ts in the cen tre. Commonly this is done using the gaussian weighting centred on the interest point for this. Computer Science / Software Engineering Notes Network â—‹ It order to o verco me the lack o f illumin atio n in varian ce, this is potentially achievable by n o rmalisin g o r eq ualisin g the p ixel p atches befo re co n structin g the histo gram. â—‹ Lo cal gradien t histo grams is an altern ative to p ixel histo grams by fin din g the p artial derivatives o f an image (e.g. applying convolution with Sobel). So instead of building histograms of the raw pixel values we could instead build histograms that encode the gradient magnitude and direction for each pixel in the sampling patch. As gradien t magn itudes (an d directio n s) are in varian t to brightn ess chan ge p lus gradien t magn itude an d directio n histo grams are mo re distin ctive than lo cal p ixel histo grams. This is built by quantising the direction (0 to 2 radians) into a number of binsÏ€ (usually 8). For each pixel in the sample patch, accumulate the gradient magnitude of that pixel in the respective orientation bin. i. Gradient histograms are not naturally rotation invariant however they can be made in varian t by fin din g the do min an t (bin with the greatest value) o rien tatio n an d cyclically shiftin g the histo gram so the do min an t o rien tatio n is the first bin . â—‹ SIFT (Scale Invariant Feature Transformer) feature is widely used that builds on the idea of local gradient histogram by incorporating spatial binning which create multiple gradient histograms about the interest point and appends them all together into a longer feature. Standard SIFT geometry appends a spatial 4x4 grid of histograms with 8 orientations. This leads to a 128-dimensional feature that is highly discriminative and robust. In order to match SIFT features: i. Find the SIFT feature vecto r o f the in terest p o in t o f in terest an d then fin d all o f the feature vecto rs o f the image. Using the Euclidean distan ce between the in terest p o in ts is an easy way of finding the feature vectors that are closest to the interest point. Thresho lds can be used to reject p o o r matches ho wever that do esn â€™t wo rk well an d results in lo ts o f mismatches. ii. An improved matching method is to take each feature fro m the first image an d fin d the two clo sest feature in the seco n d image. Only fo rm a match if the ratio o f distan ces between the clo sest an d seco n d clo sest matches is less than a thresho ld (usually set at 0.8 meaning that the distance to the closest feature must be at most 80% of the second closest. This leads to a more robust matching strategy. â— Lecture 8:Consistent matching â—‹ Even the most advanced local features can be prone to being mismatched as there are always tradeo ffs in feature distin ctiven ess. As if its to o distin ctive it will n o t match subtle variatio n s due to n o ise of imaging conditions but if itâ€™s n o t distin ctive en o ugh it will match everythin g. â—‹ Geo metric map p in g can be thought of as a tran sfo rm fun ctio n that map s the x, y co o rdin ates o f p o in ts in o n e image to an o ther. Computer Science / Software Engineering Notes Network i. Po in t tran sfo rms - take the form of where is theğ‘¥ ' = ğ‘‡ğ‘¥ ğ‘¥ ' transformed coordinate, is the transform matrix and is the originalğ‘‡ ğ‘¥ coordinate. ii. A ffin e tran sfo rm - take the form of that allows forğ‘¥ ' = ğ´ğ‘¥ + ğ‘ translation, rotation, scaling, changing of the aspect ratio and shearing. This means it has 6 degrees of freedom due to the number of operations iii. Pro jective tran sfo rmatio n (p lan ar ho mo grap hy) - Uses a 3x3 matrix as the transformation with the coordinator vector being [x, y, 1]. â—‹ It is possible to reco ver the tran sfo rm matrix fro m a tran sfo rmed image an d the o rigin al image by fin din g a set o f p o in t matches. For a homography is needs at least 4 points and 3 for affine transform. To solve uses the least squares difference between the proposed matrix image and the actual image. â—‹ RA Ndo m SA mp le C o n sen sus (RA NSA C ) is an iterative metho d to estimate p arameters o f a mathematical mo del fo r a set o f o bserved data that co n tain s o utliers. The algorithm works in two stages where the first stage is to generate a mathematical model representing the dataset with the second stage to find ho w far each data p o in t is fro m the lin e o f best fit. If a data p o in t is greater than a thresho ld value, the p o in t is an o utlier o therwise it is an in lier. This is repeated a set number of times keeping the inliners. â—‹ However there are problems with direct lo cal feature matchin g as the algo rithm is slo w. The typical image (800x600) has ~2000 difference of gaussian interest points / SIFT descriptors as it has 128 dimensions. For matching a query feature to an image requires operations.ğ‘› 2 i. K-D Trees - A bin ary tree structure that p artitio n s the sp ace alo n g axis-align ed hyp erp lan es. This typically takes each dimension in turn and splits on the median of the points in the enclosing partitions but stops after a certain depth or when the number of points in a leaf is less than a threshold. To search, just walk down the tree until a leaf is hit and brute-force search to find the best in the leaf. However this do esn â€™t scale well to high dimen sio n s as you tend to end up needing to search most of the tree. And there are approximate versions that wonâ€™t necessarily return the exact answer that do scale. ii. Lo cality Sen sitive Hashin g (LSH) - Creates hash codes for vectors such that similar vectors have similar hash codes â— Lecture 9:Image search and Bags of Visual words â—‹ B ag data structure - An unordered data set but allows elements to be inserted multiple times (sometimes called a multiset or counted set). â—‹ Text p ro cessin g is a way o f do in g feature extractio n o f text by sp littin g a sen ten ce in to wo rds (to ken izatio n ) with o p tio n al sto p -wo rds remo val and stemmin g/lemmatizatio n . The tokenized words are packaged into a bag called a bag o f wo rds with a co un t o f each wo rd. This conceptually makes language able to be solved with vector-space models. This allows for Computer Science / Software Engineering Notes Network documents to be modelled with a vector and queried and for evaluating how â€œsimilarâ€ two documents are. â—‹ B ag o f wo rds vecto rs uses a lexico n o r vo cabulary as the set o f all (p ro cessed) wo rds acro ss all do cumen ts. A vector is created for each document with as many dimensions as words in the lexicon. Each wo rd vecto r is a histo gram o f the wo rd o ccurren ces in the do cumen t ho wever will have a very high n umber o f dimen sio n s but it is very sp arse. â—‹ In verted in dexes are a way o f map p in g wo rds to do cumen ts. A posting is a pair formed by a do cumen t ID and the n umber o f times the sp ecific wo rd ap p eared in that do cumen t. â—‹ A more efficient way of co mp utin g the similarity o f two do cumen ts is to co mp ute the co sin e similarity usin g the in verted in dexes. Where the smaller the an gle the better. â—‹ However doing this assumes that every wo rd in the do cumen t has eq ual imp o rtan ce which is n o rmally n o t true as if a term is freq uen t in a Computer Science / Software Engineering Notes Network do cumen t but rare acro ss o ther do cumen ts, it is probably more important for that document. There are multip le weightin g schemes i. B in ary weights - Only presence (1) or absence (0) of a term recorded in vector ii. Raw freq uen cy - The frequency of occurrence of term in document included in vector iii. TF/ID F (Term freq uen cy / In verse do cumen t freq uen cy) - Provides high values for rare words and low values for common words by the term frequency being the frequency count of a term in the document and the inverse document frequency being the amount of information the word provides. â—‹ Vecto r q uan tisatio n is a lo ssy data co mp ressio n techn iq ue. Given a set of vectors, a technique like K-Means clustering can be used to learn a fixed size set of representative vectors where the set of representation vectors is called a codebook. Vector quantisation achieves this by rep resen tin g a vecto r by an o ther ap p ro ximate vecto r that is drawn fro m a p o o l o f rep resen tative vecto rs. Each input vector is assigned to the closest vector from a pool. â—‹ SIFT visual wo rds use vecto r q uan tisatio n as a way o f rep lacin g each descrip to r (in terest p o in t) by a rep resen tative vecto r kn o wn as a visual wo rd. This is a way of simplifying a patch of pixels to a feature vector of a cluster. â—‹ It is possible to create a histo gram o f bag o f visual wo rds by generating a codebook of all the visual words in the image. This is a way of aggregatin g a variable n umber o f lo cal descrip to rs in to a fixed len gth vecto r an d allo ws ap p lyin g techn iq ues fo r text retrieval to images. â—‹ A n imp o rtan t key p arameters in buildin g visual wo rd rep resen tatio n is the size o f the co debo o k as if it is to o small, all vecto rs lo o k the same mean in g that it is n o t distin ctive en o ugh. To o big an d the same visual wo rd might n ever ap p ear acro ss images mean in g it is n o t to o distin ctive.","libVersion":"0.3.2","langs":""}